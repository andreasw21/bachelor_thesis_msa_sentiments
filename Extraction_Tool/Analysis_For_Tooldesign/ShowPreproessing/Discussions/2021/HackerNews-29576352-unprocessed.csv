ID;Source;Creation Date;Content;Sentiment
29576352;HackerNews;2021-12-16;Title:Don't start with microservices – monoliths are your friend, Content: https://arnoldgalovics.com/microservices-in-production/;0

29646953;HackerNews;2021-12-22;"&gt; The exact reason why I compare microservices to OOP ;-)<p>That's just because some guru -- Uncle Bob, or even more probably Martin Fowler, I think -- some (rather long by now) time ago wrote a lot of examples along the lines of &quot;methods should be at most five, preferably three, lines long&quot; in Java.<p>If you look for later examples of the same kind of recommendation you'll probably find they're mostly written in funtional languages nowadays, so you could just as well say &quot;that's why I compare microservices to FP&quot;.";0
29614235;HackerNews;2021-12-19;Yeah, the ops side of microservices is important.<p>More granular scaling. Scaling up instances of 8, 16, 32GB or even larger instances is much more expensive than 1,2,4GB instances. In addition, monoliths tend to load slower since there's more code being loaded (so you can't scale up in sub minute times)<p>Obviously there's lazy loading, caching, and other things to speed up application boot but loading more code is still slower;0
29607442;HackerNews;2021-12-18;I feel like a lot of microservice advocates fail to price in the overhead introduced when you split stuff up into multiple independent communicating units.<p>Example:  Replacing a simple database query (effectively instant) and relaying the data as a local variable, with poking a seperately hosted microservice which ends up adding 20ms of overhead doing a HTTPS request, encoding and de-encoding the result in JSON, etc.;0
29606318;HackerNews;2021-12-18;"This is actually quite a nice sweet spot on the mono/micro spectrum. Most bigger software shops I've worked at had this architecture, though it isn't always formally specified. Different servers run different subsets of monolith code and talk to specific data stores.<p>The benefits are numerous, though the big obvious problem does need a lot of consideration: with a growing codebase and engineering staff, it's easy to introduce calls into code/data stores from unexpected places, causing various issues.<p>I'd argue that so long as you pay attention to that problem as a habit/have strong norms around &quot;think about what your code talks to, even indirectly&quot;, you can scale for a very long time with this architecture. It's not too hard to develop tooling to provide visibility into whats-called-where and test for/audit/track changes when new callers are added. If you invest  in that tooling, you can enforce internal boundaries quite handily, while sidestepping a ton of the organizational and technical problems that come with microsevices.<p>Of course, if you start from the other end of the mono/micro spectrum and have a strong culture of e.g. &quot;understand the service mesh really well and integrate with it as fully as possible&quot; you can do really well with a microservice-oriented environment. So I guess this boils down to &quot;invest in tooling and cultivate a culture of paying attention to your architectural norms and you will tend towards good engineering&quot; ... who knew?";0
29603692;HackerNews;2021-12-18;Yes, the original argument was that boundaries can't be enforced. Arguably, it is easier to enforce boundaries between microservices.;0
29603356;HackerNews;2021-12-18;"There is no single development, in either technology or management technique, which by itself promises even one order-of-magnitude improvement within a decade in productivity, in reliability, in simplicity. -- Fred Brooks<p>Sussman summed up the problem nicely: &quot;We really don't know how to compute!&quot; So we latch onto whatever semi-plausible idea some consultant cooks up, like flowcharts, structured programming, agile, object-oriented programming, test-driven development, microservices, and countless other things.<p>Microservices impose a transport layer over whatever it is you were doing before. So that's one extra point of failure that you've got to contend with. Complex problems require complex solutions. Sure, there are better and worse ways of doing things, but there are no miracles.";0
29601965;HackerNews;2021-12-18;Microservices requires lots of forethought, design, and highly qualified engineers.<p>You spent the same amount of forethought, design, and talent on monoliths - you’d might not need microservices.<p>The real issue is scale.<p>A well designed micro service mesh will scale. A well designed monolith might scale.;0
29596353;HackerNews;2021-12-17;Amazon was also doing microservices very early and it was a monolithic C++ application originally (obidos).<p>Microservices was relly more about locality and the ability to keep data in a memory cache on a thin service.  Rather than having catalog data competing with the rest of the monolithic webserver app on the front end webservers, requests went over the network, to a load balancer, they were hashed so that the same request from any of the webservers hit the same catalog server, then that catalog server usually had the right data for the response to be served out of memory.<p>Most of the catalog data was served from BDB files which had all the non-changing catalog data pushed out to the catalog server (initially this data had been pushed to the webserver).  For updates all the catalog servers had real-time updates streamed to them and they wrote to a BDB file which was a log of new updates.<p>That meant that most of the time the catalog data was served out of a redis-like cache in memory (which due to the load balancer hashing on the request could use the aggregated size of the RAM on the catalog service).  Rarely would requests need to hit the disk.  And requests never needed to hit SQL and talk to the catalog databases.<p>In the monolithic world all those catalog requests are generated uniformly across all the webservers so there's no opportunity for locality, each webserver needs to have all the top 100000 items in cache, and that is competing with the whole rest of the application (and that's even after going to the world where its all predigested BDB files with an update service so that you're not talking SQL to databases).;0
29591723;HackerNews;2021-12-17;"&gt; Overall this setup would work perfectly fine in a small/medium company and take 5-10x less time than doing everything the FAANG way.<p>The point was never comparing it to the FAANG way. The point is: it's easier (at the beginning) to maintain ONE monolith (and all the production stuff related to it) than N microservices.";0
29591341;HackerNews;2021-12-17;"I didnt't even get a quater of the way down the page before I stopped reading.<p>As soon as the author started listing things like k8s as needed for microservices it shows they havent stopped to think out side the box. there is no reason you can't run your set of microservices as 3-4 docker containers on the same host, no load balancers, no k8s, no log aggrigation, etc etc etc.<p>If your application makes sence as microservices you don't need to start with all of that, so including it all in the cost of startup makes no sense at all, as your application starts to scale out and need them, add them at that time, your going to need most of it for a monolith application as well, and some of them you may NEVER need (k8s for example, there is no reason you can't run your application on just plain old compute infrastructure, you don't even need to look at the &quot;cloud&quot; that old box in the corner of your office might be all you need for the project)<p>if you stop and remember that &quot;microservices&quot; just means small single function services, not things like k8s you will probably find that you can actully do a lot less work if you go down that road by letting other exsisting projects so a whole bunch of the work for you and save you re-inventing the wheel to get your project finished and out the door.";0
29590723;HackerNews;2021-12-17;"When the autor says<p><pre><code>  &gt; If you’re going with a microservice:
  &gt; A Kubernetes cluster
  &gt; A load balancer
  &gt; Multiple compute instances[...]
  &gt; Jaeger/Zipkin for distributed tracing
</code></pre>
To be fair, using K8s + helm I was able to install logging, grafana, prometheus very easily. If you leverage on Helm3 and Bitnami [1] helm charts, you can go fast.<p>Also, you can use pipeline (like github/bitbucket pipelines)  to deploy and remove jenkins completly: I have done it and it is a viable solution (although with some lock-in).<p>So the complexity is a bit less if you study enough well your setup, but you <i>must take time to plan your solution</i>.<p>After three years of K8s study, in my humble option K8s is far better compared to docker swarm, even for tiny projects, with k8s as a cloud managed solution (even small provider had it nowadays).<p>[1]: <a href=""https://bitnami.com/stack/postgresql/helm"" rel=""nofollow"">https://bitnami.com/stack/postgresql/helm</a>";0
29590439;HackerNews;2021-12-17;"&gt;  You're talking about something very odd here... a monorepo, with a monolithic build output, but that... transforms into any of a number of different services at runtime based on configuration?<p>I'd say that it's more uncommon than it is odd. The best example of this working out wonderfully is GitLab's Omnibus distribution - essentially one common package (e.g. in a container context) that has all of the functionality that you might want included inside of it, which is managed by feature flags: <a href=""https://docs.gitlab.com/omnibus/"" rel=""nofollow"">https://docs.gitlab.com/omnibus/</a><p>Here's an example of what's included: <a href=""https://docs.gitlab.com/ee/administration/package_information/defaults.html"" rel=""nofollow"">https://docs.gitlab.com/ee/administration/package_informatio...</a><p>Now, i wouldn't go as far as to bundle the actual DB with the apps that i develop (outside of databases for being able to test the instance more easily, like what SonarQube does, so you don't need an external DB to try out their product locally etc.), but in my experience having everything have consistent versions and testing that all of them work together makes for a really easy solution to administer.<p>Want to use the built in GitLab CI functionality for app builds? Just toggle it on! Are you using Jenkins or something else? No worries, leave it off.<p>Want to use the built in package registry for storing build artefacts? It's just another toggle! Are you using Nexus or something else? Once again, just leave it off.<p>Want SSL/TLS? There's a feature flag for that. Prefer to use external reverse proxy? Sure, go ahead.<p>Want monitoring with Prometheus? Just another feature flag. Low on resources and would prefer not to? It has got your back.<p>Now, one can argue about where to draw the line between pieces of software that make up your entire infrastructure vs the bits of functionality that should just belong within your app, but in my eyes the same approach can also work really nicely for modules in a largely monolithic codebase.<p>&gt; Is this meant to be simpler than straight separate codebase microservices?<p>Quite a lot, actually!<p>If you want to do microservices properly, you'll need them to communicate with one another and therefore have internal APIs and clearly defined service boundaries, as well as plenty of code to deal with the risks posed by an unreliable network (e.g. any networked system). Not only that, but you'll also need solutions to make sense of it all - from service meshes, to distributed tracing. Also, you'll probably want to apply lots of DDD and before long changes in the business concepts will mean having to refactor code across multiple services. Oh, and testing will be difficult in practice, if you want to do reliable integration testing, as will local development be (do you launch everything locally? do you have the run configurations for that versioned? do you have resource limits set up properly? or do you just connect to shared dev environments, that might cause difficulties in logging, debugging and consistency with what you have locally?).<p>Microservices are good for solving a particular set of problems (e.g. multiple development teams, one per domain/service, or needing lots of scalability), but adding them to a project too early is sure to slow it down and possibly make it be unsuccessful if you don't have the pre-existing expertise and tools that they require. Many don't.<p>In contrast, consider the monolithic example above:<p><pre><code>  - you have one codebase with shared code (e.g. your domain objects) not being a problem
  - if you want, you still can use multiple data stores or external integrations
  - calling into another module can be as easy as a direct procedure call in it
  - refactoring and testing both are now far more reliable and easy to do
  - ops becomes easier, since you can just run a single instance with all of the modules loaded, or split it up later as needed
</code></pre>
I'd argue that up to a certain point, this sort of architecture actually scales better than either of the alternatives, in comparison to the regular monoliths it's just a bit slower to develop in that it requires you to think about boundaries between the packages/modules in your code, which i've seen not be done too often, leading to the &quot;big ball of mud&quot; type of architecture. So i guess in a way that can also be a feature of sorts?";0
29590250;HackerNews;2021-12-17;"I agree to not start with microservices.. But its better to not wait too long after the project is growing.<p>Regarding his points:<p>- Infrastructure requirements:<p>You don't need all that stuff! You can have multiple services run on PaaS services / Cloud Run, and you dont need to deal with all the kubernetes stuff. Even if you prefer K8S, then you still dont need a service mesh from the start. Datadog and Gitlab brings you very far with hardly any work on your side.<p>- Faster Deployments<p>My point: 80 microservices? Crazy .. Why? Just have a service per business domain, and try unifying the CI/CD stack.<p>We had 1 big Monolith which would deploy 5 times per day, buy every deploy took around 1 hour.<p>Having 5 to 10 services, that all deploy within 5 minutes is so much nicer to work with.<p>- The Supporting Culture<p>This is important: Architecture follows company organization, and vice versa. Every team often owns 1 or 2 services. Teams should be organized by domain. Business boundaries should be agreed upon in a higher level.<p>Sitting in your dev corner, building services without talking and aligning with the Product owners &amp; MT is a recipe for distaster imho. The services should solve a problem that PO's understand. You should have alignment.<p>- Better Fault Isolation<p>We never said it was going to be easy... It requires a different way of building your system. You need to think distributed systems.";0
29589817;HackerNews;2021-12-17;"The problem is that orgs are not set in stone. Teams get merged and split in reorgs, buyouts and mergers happen, suddenly your microservices designed around &quot;cleanly defined boundaries&quot; no longer make any sense. Sure you can write completely new microservices but that is a distraction from delivering value to the end customer.";0
29589799;HackerNews;2021-12-17;For me it highly depends on the size of software, the time to market, the time the software is supposed to work without major rewrites.<p>If I am doing a blog application or an website for one time event, I would pick a monolith.<p>If I start on ERP, a checkout solution I would pick microservices.<p>Also if the man power is low but I expect growth in the future, I might go for a monolith broken in separate projects with minimal dependencies between modules so it wouldn't be terribly difficult to break it into microservices when the need and man power arrives.;0
29589736;HackerNews;2021-12-17;"Seems, to me, that the author is poor at designing microservices. 
Using his example: the login, session and user services should be only one service (something like Keycloak), there's no advantage to splitting this up, so why would you?
Analytics service should never be a dependency of another serivce, but rely on service discovery and a preset telemetry/analytics/rpc endpoint which each of the consumer-facing services implements.<p>Has anyone claimed that microservices will fend of poor design? There's never a silver bullet.";0
29589283;HackerNews;2021-12-17;"The archetypical &quot;microservice&quot; ecosystem I am aware of is Google's production environment. It was, at that point, primarily written in C++ and Java, neither very famous for being dynamically typed.<p>But, it was a microservice architecture built primarily on RPCs and not very much on message buses. And RPCs that, basically, are statically typed (with code generation for client libs, and code generation for server-side stubbing, as it were). The open-source equivalent is gRPC.<p>Where &quot;going microservice&quot; is a potential saving is when different parts of your system have different scaling characteristics. Maybe your login system ends up scaling as O(log n), but one data-munging part of the system scales as O(n log n) and another as just O(n). And one annoying (but important) part scales as O(n * 2). With a monolith, you get LBs in place and you have to scale you monolith out as the part that has the worst scaling characteristic.<p>But, in an ideal microservice world (where you have an RPC-based mechanism taht can be load-balanced, rather than a shared message bus that is harder to much harder to scale), you simply dial up the scaling factor of each microservice on their own.";0
29589156;HackerNews;2021-12-17;"I know, thanks.<p>My point is precisely that: if you have to handle large quantity of state (e.g.: travel agency handling diverse item bookings to sell as a complete holiday packages - note that this includes having conflicts on inventory, like &quot;cruise cabin categories&quot; or &quot;hotel rooms&quot;) microservices add latency by &quot;replacing function calls with RPC&quot;, and gain you... an unspecified advantage in terms of... deployment? The possibility to have hundreds of developers working on the system in parallel?<p>I have always worked on medium-size monoliths during most of my career, and <i>&quot;ah, if we had 137 developers all working on this everything would be magically solved, but alas, we have a monolith&quot;</i> was a sentence I uttered (or heard) exactly 0 times so far.";0
29588939;HackerNews;2021-12-17;I’ve heard before that microservices deployment scheme solves one particular task: if you get traction, you’ll be ready for scaling. If you can’t do that you are already dead, cause being unable to get 10x more users with a click (when they come) means your competition will do that instead. Is that still true?;0
29588921;HackerNews;2021-12-17;I like monoliths... which are modular inside... no need for microservices, easy deployment even without Docker and Kubernetes... just a single binary...;0
29588812;HackerNews;2021-12-17;Its really not odd at all...this is how compilers work...we have been doing it forever.<p>Microservices were a half baked solution to a non-problem, partly driven by corporate stupidity and charlotan 'security' experts - I'm sure big companies make it work at enough scale, but everything in a microservice architecture was achievable with configuration and hot-patching. Incidentally, you don't get rid of either with a MCS architecture, you just have more of it with more moving parts...absolute sphegetti mess nightmare.;0
29587824;HackerNews;2021-12-17;I mean, this is kind of how microservices should be done. Start with a MVP monolith then carve off microservices if needed (performance or large team size).<p>The problem is when the lead dev has been huffing the architecture paint too hard and starts prematurely spinning up microservices because it feels good.;0
29587781;HackerNews;2021-12-17;"<i>&gt; It's a pretty obvious conclusion to anyone who has worked in both environments, or have had to migrate from one to the other, so it's not particularly insightful.</i><p>It should be obvious, but apparently it's not. So many architecture astronauts drinking the kool-aid and making a mess. Premature microservices can easily kill a product.";0
29587721;HackerNews;2021-12-17;That can definitely happen and be painful.  I now work at a very large organization, though, and the benefits of microservice design are obvious (as were the pains of monolithic ones when I was working on an old legacy monolith in the same place).;0
29587713;HackerNews;2021-12-17;My understanding from the HTML Template days is that a monolith contained both the business and presentation layer together. I made plenty of those with Struts 1.x and Spring MVC. Micro services were pitch as a means to separate the front end and back end.<p>I'm a bit confused on microservices vs monolith in the modern SPA context. If I have an application that's front end is React and back end is Go and the two communicate over REST/HTTP+RPC, do I have microservices or a monolith?;0
29587591;HackerNews;2021-12-17;I doubt you can judge the blast radius.<p>Say your little service just changed how it parsed backticks. Now that innocuous change may affect none of the immediately connected microservices but another services three hops away relied on the old behavior of your parser through some complex business rules driven logic. Now go test and later troubleshoot that vs standing up a single monolithic jar on your laptop and seeing the exception stack trace tell you exactly what you broke.;0
29586877;HackerNews;2021-12-17;Oftentimes however with microservices it's the tail wagging the dog. Microservices architectures become such a burden that they strain the capacity of the existing team which leads to more hiring which thanks to Conway's law leads to more microservices being built which leads to more operational and architectural overhead which leads to more hiring...;0
29586831;HackerNews;2021-12-17;The individual code bases of a microservice might not involve spaghetti code, but the interconnections certain can.  I'm looking at a diagram of the service I work on, with seven components (written in three languages), five databases, 25 internal connections, two external interfaces, and three connections to outside databases, all cross-wired (via 6 connections) with a similar setup geographically elsewhere in case we need to cut over.  And that's the <i>simplified</i> diagram, not showing the number of individual instances of each component running.<p>There is clear separation of concerns among all the components, but it's the interconnections between them that take a while to pick up on.;0
29586815;HackerNews;2021-12-17;"Of course this advice doesn't <i>always</i> make sense, but it makes sense more often than people want to admit.  Not that this is an original claim, if anybody remembers the &quot;majestic monolith.&quot;<p>Simply put, the microservice path makes a lot more sense when your organization is so big that you don't really know what other teams are doing all the time and need a clear delineation of areas of responsibility, or maybe if you have very large volumes you're dealing with.  That doesn't describe most orgs, but if it describes you consider microservices.";0
29586706;HackerNews;2021-12-17;"My impression, which seems to be supported by this article? Is that when you're small, you can actually get up &amp; running with an MVP with a non-serverless monolithic approach <i>and</i> scale for a bit before you hit a wall. And <i>only</i> at that point: 1) Scaling becomes much more complex and 2) Monolithic infrastructure prevents you from easily implementing best-of-breed solutions within segments of your functionality to optimize various functionality. And, from a cloud services $$ POV, it's cheaper and perhaps require less dev time, though I'm guessing that will depend on the project<p>This seems reasonable? At least at the early &amp; early-mid stages. If you make it that far and see things like scaling issue in your future, it seems like you should also be at a stage of growth where you'll be getting reasonable funding offers and can invest the resources into migrating away from monolithic.<p>Disclaimer: My opinion here is formed mostly from following a not-completely-dissimilar process even when working with things more on a monolithic side of things: I'll use a high-powered workstation to spin up a vm's on the same host, and then if I need to I can migrate individual vm's to their own better-resourced instances on other hardware to scale things. I did this some years ago with a Hadoop cluster and the process worked out nicely.<p>Although as it turned out, that example didn't last long: Hadoop was overkill because I overestimated the bigness of my data, which turned out to only be on the bigger side of small. Or smaller side of medium. When I had a rethink on it, I wrote some python code against a the primary sql-based data source &amp; used Keras to do what I needed instead: Iterating each pieceon a nicely-spec'ed workstation took an hour or so, and a full end-to-end run maybe 3-4 hours.<p>But this is kind of my point: Starting out, it's easy to thing &quot;Oh I need to plan for ever possible eventuality &amp; level of scale.&quot; No, you don't.<p>And a final caveat to this: YMMV since circumstances differ from project to project. But these are things to consider before you automatically go for slicing each piece of functionality into grains of sand with their own microservice.";0
29586541;HackerNews;2021-12-17;"&gt; I've wondered if it's not a ploy by cloud vendors and the ecosystem around them to increase peoples' cloud bills. Not only do you end up using many times more CPU but you end up transferring a lot of data between availability zones, and many clouds bill for that.<p>Disagree. I'd argue that microservices are inherently more cost effective to scale. By breaking up your services you can deploy them in arbitrary ways, essentially bin packing N microservices onto K instances.<p>When your data volume is light you reduce K and repack your N services.<p>Because your services are broken apart they're easier to move around and you have more fine grained scaling.<p>&gt; further increasing lock-in to the managed cloud paradigm if not to individual clouds.<p>Also disagree. We use Nomad and it's not hard to imagine how we would move to another cloud.";0
29586327;HackerNews;2021-12-17;"I've been a professional programmer since the late 1990s. What I observe is that many people flocked to microservices because they had only known monoliths and they were sick of the limitations inherent to them.  Now we have a generation of engineers who have only known (or been taught) microservices and a backlash has developed because they are sick of the limitations inherent to them.<p>This is my reading of the anti-OOP / anti-inheritance movement as well.  To spice up Bjarne Stroustrup (inventor of c++), &quot;There are only two kinds of programming languages: those that don't make it and those that people bitch about.&quot;";0
29586287;HackerNews;2021-12-17;Absolutely agree. The point of microservices is to separate the concerns from an organizational point of view, not for technical reasons.<p>Most of the advantages attributed to microservices can also be achieved with a monolith architecture using a sane and rigorous design.<p>I find it frustrating at work when I see teams of 50 people having issues coordinating work because they have a monolith, while we often unnecessarily spread a team of 5 people among 10 microservices.;0
29585594;HackerNews;2021-12-17;One problem I've never seen mentioned regarding microservices is what happens when the organization that produced them has moved from growing to stagnating and finally into decline. If microservices ship the org-chart, what happens when that organization fails to attract engineers?<p>I worked for a company that microserviced themselves into a pit. We had a huge layoff which completely killed the morale of the remaining engineering staff. Over the next 3 years, more and more engineers left and the company refused to replace them. What started out as teams (~5 people) responsible for ~3 microservices ended up as teams of ~3 people responsible for ~6 microservices.<p>It was kind of cool to be responsible for more architecture and see how it all fit together, but the sad reality was that too many of the microservices that had been stood up were stitching together disparate data from other microservices to then perform what a single SQL query against a RDBMS was doing.;0
29585586;HackerNews;2021-12-17;"&gt; Independent teams (each developing independent services and acting without top-down approval) is a different way to coordinate development that values productivity (keeping everyone unblocked) and innovation (finding better ways of doing things).<p>I've had the opposite experience. In the monolith, anyone can make the necessary changes, because it's all one codebase that everyone is familiar with. At most, you might need some help/pairing/approvals from experts in particular areas, but in general any team can work independently.<p>By comparison, in the microservices world, many teams either don't want you to touch their service, or are using a tech stack so unfamiliar to you that it would take too long to be productive. And there's a rat's nest of interdependent microservices, so you end up begging other teams to adjust their roadmap to fit you in.<p>&gt; Independent teams can pursue very different patterns without needing to agree with each other.<p>I see this as more downside than benefit. If everyone is using different tech stacks, it's harder for people to move between and contribute to different teams. And you end up with situations where one team uses Java, while another uses Scala, which brings in extra complexity to satisfy what are essentially aesthetic preferences.";0
29585291;HackerNews;2021-12-17;"I disagree, microservices are an architectural concept related to the software, not to the infrastructure.<p>Whether you are using containers or VPS or serverless or bare metal for your infrastructure, that's completely unrelated to the concept of microservices: you can deploy either a monolith or microservices in any of the above.<p>As an example you can deploy a monolith on Lambda[1] or you can deploy microservices on bare metal using one of the several self managed serverless engines available[2].<p>[1] see e.g. <a href=""https://claudiajs.com/tutorials/serverless-express.html"" rel=""nofollow"">https://claudiajs.com/tutorials/serverless-express.html</a> or <a href=""https://blog.logrocket.com/zappa-and-aws-lambda-for-serverless-django/"" rel=""nofollow"">https://blog.logrocket.com/zappa-and-aws-lambda-for-serverle...</a><p>[2] see e.g. <a href=""https://fnproject.io/"" rel=""nofollow"">https://fnproject.io/</a> and <a href=""https://knative.dev/"" rel=""nofollow"">https://knative.dev/</a>";0
29585064;HackerNews;2021-12-16;Agree.  IMO modularity/coupling is the main issue. My issue w/ the microservice architecture is that it solves the modularity problem almost as a side effect of itself but introduces a whole host of new ones that people do not anticipate.<p>Yes, if you, at the outset, say we will separate things into separate services, you will get separated services. However, you do NOT need to take on the extra complexity that comes with communication between services, remote dependency management, and additional infrastructure to reduce coupling.;0
29585058;HackerNews;2021-12-16;"&quot;Microservice&quot; is inconsistently defined. One cannot accuse them of using a strawman definition if there is no realman.";0
29585054;HackerNews;2021-12-16;Managing complexity is hard, no matter the approach to it. Microservices define boundaries around component inputs and output and try to reduce complexity by black boxing functionality. Monoliths try to reduce complexity by managing code (languages, libraries, databases, etc). I'm not sure there really is a good answer between the two because over time:<p>1. Vendor (and open source) blackboxes get introduced to monoliths, and you end up with a monolith built around a bunch of microservices.<p>2. Common tooling has a huge benefit, and copy-pasta code gets the job done faster than everything being custom every time. So you end up with microservices that every task ends up importing some giant framework and uses common libraries - so the monolith gets imported to the microservice.<p>Software gets complex, faster than we all like to believe... It seems like software has gravity, attracts complexity, and ends up being a big ball of mud in the end, almost every time.;0
29584932;HackerNews;2021-12-16;I've wondered if it's not a ploy by cloud vendors and the ecosystem around them to increase peoples' cloud bills. Not only do you end up using many times more CPU but you end up transferring a lot of data between availability zones, and many clouds bill for that.<p>A microservice architecture also tends to lock you into <i>requiring</i> things like Kubernetes, further increasing lock-in to the managed cloud paradigm if not to individual clouds.;0
29584905;HackerNews;2021-12-16;"My theory is that microservices became vogue along with dynamically typed languages. Lack of static types means that code becomes unmanageable at a much lower level of complexity. So the complexity was &quot;moved to the network&quot;, which looks like a clear win if you never look outside a single component.";0
29584898;HackerNews;2021-12-16;"I'd like to challenge one part of your comment - that microservices break up data on module boundaries. Yes, they encapsulate the data. However, the issue that causes spaghettification (whether internal to some mega monolith, across modules, or between microservices), is the semantic coupling related to needing to understand data <i>models</i>. Dependency hell arises when we need to share an agreed understanding about something across boundaries. When that agreed understanding has to change - microservices won't necessarily make your life easier.<p>This is not a screed against microservices. Just calling out that within a &quot;domain of understanding&quot;, semantic coupling is pretty a fact of life.";0
29584806;HackerNews;2021-12-16;Yeah, I don't think there's really a best approach here. I know where I work right now, we have this giant java app that is just a nightmare to even get running. I've been working on the microservices, and they do have all the downsides you're talking about, but I can get that stack up and flying super fast, whereas this giant java app takes 3 minutes to even be (barely) functional, and has so many configuration options that out of this 1000 line config file it's hard to find the 3 things you might actually care about.;0
29584587;HackerNews;2021-12-16;The idea of microservices sometimes reminds me of visual programming looking at infrastructure diagrams. Like instead of writing the code and executing it. You implement variables (databases), conditions (lambda), loops (pubsub/sqs) etc as separate entities and then put them together.;0
29584157;HackerNews;2021-12-16;You're talking about something very odd here... a monorepo, with a monolithic build output, but that... transforms into any of a number of different services at runtime based on configuration?<p>Is this meant to be <i>simpler</i> than straight separate codebase microservices?;0
29583989;HackerNews;2021-12-16;"I think this leads to silos. 
MicroServices written in different lang's mean Java Dev X can't maintain Python service Y.. Not in an efficient way.
What's worse, Java Dev can't move to Team Y without upskilling not only on the service, but also the lang, so they get pidgin holed. She also can't move because she's the only Java dev left.";0
29583929;HackerNews;2021-12-16;"Here's my take on microservices: It's a form of modularization. Like all modularization it can be useful to uncouple larger parts of a system - to enforce an architecture, avoid unintended cross-dependencies, make explicit the dependencies that are intended, to allow teams to work more independently, and to allow the different parts of the system to be be developed, compiled, built, deployed (and sometimes scaled) independently.<p>But all this comes at a cost - you have to know where the module boundaries are, because moving these is much harder. The flexibility that you've gained within each module comes at the cost of making it harder to move the border between modules. It's very easy to put down a module border in the right spot, and you make a refactor (needed for cleanup or performance improvement) go from tricky to impossible. E.g. that piece of info needed in this module now canonically lives in the other one. Or we accidentally add a couple N's to the time complexity of an algorithm that works on data in multiple modules.<p>But getting the borders right on the first try is hard, unlikely even. Where those borders should be depends on a lot of things - the user domain, the performance characteristics, the development team(s) (Conway's law and all that) and the ways in which the business requirements change. For that reason, I think most successful modularizations are either done by breaking down an existing monolith (providing these insights) or by basing it on a known running system that is &quot;close enough&quot; in its needs and environment.";0
29583902;HackerNews;2021-12-16;"I have worked in companies which had monolith software, but they have been large and established businesses.<p>I will not name names, but I can say this...<p>All but one had constant, repetitive and quite painful failures, not fixable, it seemed.
One company was an exemption, with some failures but a clear culture of pushing issues while implementing new features.<p>And one company I have been with from startup days till 150 employees had a microservice infrastructure, I have never seen so little downtime, such a smooth back office, front end, database, reporting system.
The cto was owner and main developer, if something went awry, he would start working on the fix within minutes, no matter the day or time. The fastest issue response time ever.
2 lifetime downtimes of a handful minutes for the full service, and a couple components which didn't work for maybe an hour or sometimes overnight.
I have to say though, when one microservice broke, it took down more tangential services than one would think, but other than that, hands down the best software I ever worked with.";0
29583826;HackerNews;2021-12-16;"Microservices doesn't necessarily mean K8S running on self-managed compute instances as in the example given in the article.<p>The main mistake of the article is that the premise is &quot;microservices&quot; but then the examples are about infrastructure (K8S etc), but the 2 things are not tied, you can have that same architecture cited in the article running monolith instances for example, and it would be every bit as complex as managing the micorservices example, without any of the advantages.<p>Fully managed serverless solutions like Lambda/Cloudflare Workers/etc, managed through SAM or Serverless Framework solves most of the problems cited in the article.<p>The whole infra in our case is API Gateway + Lambda + DynamoDB, this infra is not more complex to manage than our old monolith which was on ALB + EC2 + RDS.<p>Deployment is one &quot;sam deploy&quot; away (done in Github Actions), this is actually even simpler than our previous monolith.";0
29583628;HackerNews;2021-12-16;How many more articles do we need to drive this point home? First it was a fashion to develop using microservices. Now writing about not using them is in vogue :-).;0
29583430;HackerNews;2021-12-16;"Another point of confusion among many folks is monorepos. Some consider &quot;monorepos = monoliths&quot;, which is more like comparing apples to oranges imo.<p>Monorepos provide some of the benefits of monoliths in terms of
1) making it easier to refactor your entire codebase at once
2) sharing some common dependencies like framework versions etc., which makes it much easier to keep those up to date and more secure, as a result
3) sharing some common utilities like unit testing harnesses etc.
4) other things<p>At the same time, monorepos don't force one into a monolith architecture at all, an in fact can provide most of the same benefits of microservices in terms of separate deployments etc.<p>The most important lesson in my mind is, there's no &quot;panacea&quot; or &quot;perfect structure&quot; as a prescribed way to do things. Every system has its own demands (which also change over time), and the only &quot;should&quot; is to tailor things on a case-by-case basis, adjusting to a better structure when needed.";0
29583156;HackerNews;2021-12-16;Modular design is decades old though.  Rails devs argue about it because it's against the rails way, and that makes things harder for them.  But I've been modularizing my Java webapps since I graduated from college, 18 years ago.<p>But also, microservices weren't pioneered by rails devs.  They were pioneered by huge companies, and they definitely have a role to play there, as you point out.;0
29582529;HackerNews;2021-12-16;Not, it's adding a single microservice to a monolith.;0
29582493;HackerNews;2021-12-16;"Ok, maybe a better way to say it is that having teams independently develop services is a good way to reduce the coordination tax <i>if you have high coordination costs.</i>  If your environment doesn't have that problem I guess this doesn't apply.<p>Coordination between engineers was a frequent activity everywhere I've been regardless of how well built the systems were.  For example:  a new requirement for the customers signing up in a given country to have features X, Y, and Z enabled.  In a large organization there are probably a few teams that will be involved that make that happen.  The question is how to coordinate them.<p>Many companies try to solve it with top-down decision making, prioritizing certainty but hampering productivity (some teams have to wait) and strictly limiting risky innovation (nothing can be done without approval).<p>Independent teams (each developing independent services and acting without top-down approval) is a different way to coordinate development that values productivity (keeping everyone unblocked) and innovation (finding better ways of doing things).<p>&gt; You have to invest a bit in tooling and recommended patterns to keep things from going crazy, but you kind of need to do that either way.<p>Aha, here's a difference.  If we're talking about the same things, <i>common</i> tools and patterns don't need to be enforced.  Independent teams can pursue very different patterns without needing to agree with each other.  This is a big advantage if you don't like being told what to do by people who are pretty remote to the problem you're solving.  Different people react differently to that.  Netflix teams tended to be staffed with very experienced and skilled people (no junior or mid level positions) so there wasn't much value in one engineer dictating architecture, patterns, or tooling to another.  Nearly all tooling was opt-in, and the good tools were the de facto standards.  But if you came up with a better tool or pattern, you had the freedom to try it out.  This is how independence fostered innovation, and why microservices were useful in such an environment.";0
29582367;HackerNews;2021-12-16;"You would only deploy 10 microservices if all 10 changed at once. Why are all 10 changing at once?<p>&gt; especially if you find yourself in the model where your microservices share code<p>Good architecture decisions trump the monolith vs microservices argument. I'm not saying cross-service shared code is inherently bad, but it does have a bad smell.";0
29582292;HackerNews;2021-12-16;Maybe because breaking up a monolith has proved to be as hard as combining microservices, once you get past the obvious and trivial steps.;0
29582286;HackerNews;2021-12-16;"&gt; If you’re going with a microservice:
A Kubernetes cluster
A load balancer
Multiple compute instances for running the app and hosting the K8S cluster<p>Try running a high-traffic high-functionality website without something equivalent to this. What's this magical single computer you'll be running it all on? You'll need a cluster of servers on a rack somewhere, managed the old fashioned way. You'll need equivalent tools to deploy to them and monitor them.<p>I think what this article should be getting at is that low-traffic sites shouldn't start with microservices, and maybe that's true. But if you're trying to build a business that scales rapidly, you want to be ready to scale your website from day one.";0
29582256;HackerNews;2021-12-16;"&gt; There is a lot of talk about monoliths vs microservices lately.<p>Actually it's been going on for years, and it's always the same argument. People think they're thought leaders for saying &quot;Start with monoliths and only move to microservices if you absolutely need to!&quot;<p>It's a pretty obvious conclusion to anyone who has worked in both environments, or have had to migrate from one to the other, so it's not particularly insightful. And yet here were are, 5+ years later saying the same thing over and over again.";0
29581948;HackerNews;2021-12-16;"This article sounds like someone who's never successfully implemented either solution. Things that are wrong so far:<p>Monolithic apps need monitoring (e.g. Prometheus) just as much as microservices.<p>Monolithic apps can have scaling issues if you only have one database instance (especially if you're write-heavy), so you may need to shard anyway.<p>Monolithic apps will probably want a messaging system or queue to handle asynchronous tasks (e.g. sending e-mail, exporting data).<p>Microservices do not require kubernetes. You can run them fine on other systems; at my last job, we just ran uwsgi processes on bare metal.<p>Microservices do not require a realtime messaging system. You can just use API calls over HTTP, or gRPC, or whatever else. You'll probably want some kind of message queue as above, but not as an integral part of your architecture, meaning it can just be Redis instead of Kafka.<p>Microservices do not require separate DB instances. If your microservices are all using separate schemas (which they should), you can migrate a service's schema to a new DB primary/replica set whenever you want. In fact, if you have one e.g. MySQL primary you can have multiple secondaries, each only replicating one schema, to handle read load from individual services (e.g. a primary write node and then separate read nodes for user data, product information, and everything else). When it's time to break out the user data into a separate database, just make the read replica the new primary for that DB and add a new read replica off that.<p>This dude just straight up doesn't know what he's talking about, and it sounds like his experience with microservices is following a bad 'microservices with golang, kafka, cassandra, prometheus, and grafana on k8s' tutorial.<p>Here's how you write baby's first microservices architecture (in whatever language you use):<p>1. Decide where the boundaries are in your given application; e.g. user data, product data, frontend rendering, payment systems<p>2. Write those services separately with HTTP/gRPC/whatever APIs as your interface.<p>3. For each API, also write a lightweight native interface library, e.g. user_interface, product_interface, payment_interface. Your services use this to call each other, and the method by which they communicate is an implementation detail left up to the interface library itself.<p>4. Each service gets its own database schema; e.g. user, product, payment, which all live on the same MySQL (or RDS or whatever) instance and read replica.<p>5. Everything has either its own port or its own hostname, so that your nginx instances can route requests correctly.<p>There, now you have a working system which behaves like a monolith (working via what seems like internal APIs) but is actually a microservice architecture whose individual components can be scaled, refactored, or rewritten without any changes to the rest of the system. When you swap out your django protobuf-over-HTTP payment processing backend for a Rust process taking gRPC calls over Redis queues, you change your interface file accordingly and literally no one else has to know.<p>It also means that your deployment times are faster, your unit testing is faster, your CI/CD is faster, and your application startup time is faster when you do have to do a service restart.<p>I'm not sure why this is so hard for people to understand.";0
29581868;HackerNews;2021-12-16;"&gt; it’s really nice to be able to add a column and deploy in minutes without having to rebuild and deploy a giant monolith<p>This is orthogonal to monolith vs microservices. I've worked on monoliths that could easily be (and were) deployed very frequently.";0
29581844;HackerNews;2021-12-16;"&gt; Refactor to separate microservices when either: 1) the team is growing and needs to split into multiple teams<p>I've heard this before, and I just don't get it. I've worked on multiple monoliths where hundreds of engineers contribute, and it's fine. You have to invest a bit in tooling and recommended patterns to keep things from going crazy, but you kind of need to do that either way.<p>&gt; At 35-50 people a common limiting factor is coordination between engineers<p>So don't coordinate? If engineers working on different aspects of the codebase need to coordinate, that feels like something is wrong architecturally.";0
29581680;HackerNews;2021-12-16;You bring up an excellent point. As of now, it is impossible for me to run my company's backend ecosystem on my machine locally. What we do could easily be done by one simple monolith, but our eng. lead is obsessed with overengineered microservice architecture, so nobody can actually develop locally. Everything on the backend is done by a bunch of lambdas behind an API gateway.<p>I got so burnt out developing in that environment that I asked to become a pure frontend dev so that I wouldn't have to deal with it anymore.;0
29581373;HackerNews;2021-12-16;This is exactly how I feel. Great to hear it from someone with Netflix experience. So, so many organizations jump headfirst into microservices before they even realize what that entails just because they heard it's the trendy new thing Netflix is doing.<p>If you make your code and architecture simple from the get-go, then you can refactor to microservices when you know you really need it.;0
29581189;HackerNews;2021-12-16;"I explicitly claim to use a &quot;citadel&quot; archicture [0] when talking about breaking off services for very similar reasons. Having a single microservice split out from a monolith is a totally valid application of microservices, but I've had better results in conversation when I market it properly.<p>I've found this to go so far as to have &quot;monolith&quot; understood to mean &quot;single instance of a system with state held in memory&quot;.<p>[0] <a href=""https://m.signalvnoise.com/the-majestic-monolith-can-become-the-citadel/"" rel=""nofollow"">https://m.signalvnoise.com/the-majestic-monolith-can-become-...</a>";0
29581131;HackerNews;2021-12-16;"To be fair, modular monoliths weren't talked about much before microservices. I'm thinking about the heyday of Rails, where there were bitter debates over whether you should even move critical business logic out of Rails. I got the impression that most devs didn't even care. (That was sort of the point where I saw that I was a bit of an odd duck as far as developers go if I cared about this stuff, and pivoted my career.) I really enjoy contexts where I can make modular monoliths, however. I'm thinking of mobile/desktop apps here, mostly.<p>Software design is something of a lost art currently. This is partially due to the current zeitgeist believing that anything that automated tools cannot create/enforce can't be that important in the first place. Of course, there's a whole swath of concerns that cannot be addressed with static or dynamic analysis, or even different languages.<p>Microservices is a response to the fact that a monolith without any real design moves quickly but can easily converge on spaghetti due to the fact that everything is within reach. It enables teams to create an island of code that talks to other islands only via narrowly defined channels. Additionally, they support larger dev teams naturally due to their isolationist take, and reinforce the belief that &quot;more developers = always better&quot; in the process. In other words, they mesh perfectly with the business context that a lot of software is being built in.<p>Factor in the fact that $COOL_COMPANY uses them, and you have a winner.";0
29581104;HackerNews;2021-12-16;It's usually not obvious where to put the seams ahead of time so you can cut them when you need to split into microservices.<p>Plus keeping the API boundaries clean costs time and resources, and it's tempting to violate them just to launch this one feature. This extra discipline doesn't have any payoff in the short term, and it has unknown payoff in the long term because you're not sure you drew the boundaries ahead of time anyway.<p>So I think in practice what happens is you create a monolith and just eat the cost of untangling it when the team gets too big or whatever.;0
29581003;HackerNews;2021-12-16;"&quot;But I've worked on teams that advocated for microservices needlessly, and it was 100% cargo cult behavior: &quot;Netflix does it, so we should too.&quot;<p>I've seen this too, and in fact I got laid off because I pushed back against this silliness for what should have been a simple lift and shift.";0
29580929;HackerNews;2021-12-16;"It's a big tradeoff for maintenance complexity and cognitive load though, and people often don't realize how big that tradeoff is. Chasing bugs and maintaining the spiderweb of network connections between all your services can quickly become a nightmare. A distributed ball of mud instead of a monolithic ball of mud, but a ball of mud nonetheless.<p>Personally I lean towards building a monolith first, then breaking out features into separate services if you need it. But I've worked on teams that advocated for microservices needlessly, and it was 100% cargo cult behavior: &quot;Netflix does it, so we should too.&quot;<p>Another anecdote: my current company, a startup, could've launched its product a year earlier than it did, but our engineering lead insisted on pre-engineering a Rube Goldberg machine of a microservices backend before we even had any prospective customers. Took months of engineering time and headaches to grok all of it, when in reality, one monolith and a basic database could've done the job for years before we'd ever have to scale past that.<p>But microservices architecture looks great on a resume, so /shrug";0
29580906;HackerNews;2021-12-16;"The interesting thing about microservices is not that it lets you split up your code on module boundaries. Obviously you can (and should!) do that inside any codebase.<p>The thing about microservices is that it breaks up your <i>data</i> and <i>deployment</i> on module boundaries.<p>Monoliths are monoliths not because they lack separation of concerns in code (something which lacks that is not a ‘monolith’, it is what’s called a ‘big ball of mud’)<p>Monoliths are monoliths because they have<p>- one set of shared dependencies<p>- one shared database<p>- one shared build pipeline<p>- one shared deployment process<p>- one shared test suite<p>- one shared entrypoint<p>As organizations and applications get larger these start to become liabilities.<p>Microservices are part of one solution to that (not a whole solution; not the only one).";0
29580870;HackerNews;2021-12-16;"These days DevOps is used as a bludgeon to fire sysadmins and make devs pretend to manage infrastructure. The idea from the corporate viewpoint is to save money by not having anyone to handle systems stuff, just shove into &quot;The CloudTM&quot; as serverless microservices, then just restart it when it falls over instead of troubleshooting.<p>Yes, I'm cynical. Having to use javalike stuff to do what should be a simple shell script will do that to you.";0
29580841;HackerNews;2021-12-16;It seems to me that the ratio of hate-for-microservices/companies-actually-using-microservices is way out of wack. I know that there are some high-profile cases like Uber, but in nearly every conversation I see online or in person, we're all in agreement that going all in on microservices is hype-driven nonsense. Maybe I'm wrong though.;0
29580753;HackerNews;2021-12-16;IMO, the big advantage of microservices over monoliths (if they're done right) is reducing iteration times. It's just a lot quicker to build/run/etc. I think monoliths are a fine starting point but once your iteration times get slow, that's when it's time to break it up.;0
29580682;HackerNews;2021-12-16;"Someone please write an article, &quot;Don't start with architecture some dude suggested because of their ego&quot;. 
There are cases when monolith is bad, and when microservice is the must.
Take the healthy approach.<p>The article is garbage with no real understanding how real microservices works:<p>The deployment:
Modern microservices are working using templates. You deploy it using that directly from you gitlab/github. You copy paste your artifact and it's there. The builds are taking 2 minutes to build, sometimes less, means you can quickly react on some issue as opposed to 30 minutes old school java monolith. Deployments are build in the same cluster you use for everything else. CI job runner is just another application in your cluster. So if your cluster is down, everything is down.<p>The culture part:<p>We use templates, where you have all the libraries , tracing in place. In fact when this request is coming we have some similar functionality written, so we reply to product, oh, this feature is very similar to feature X, we'll copy it, while we discuss some schedule thing our developer renamed similar project did commit and it's already deployed automatically to the dev cluster, the rest of the team joined to development. There is a bad pattern when you need to update your templates. This is tradeoff of approach, you don't libraries as a concept. Hence that you can have half services migrated, half services don't, that's a bonus. The cons is that you need scripts to push everything immediately.<p>Better Fault Isolation:<p>Yes, you might have settings down and core functionality working, means you have less SLA breaking events. Saves you money and customers.
Same thing with error handling. If it's just tooling you copy paste a different set of tooling. If the error logging is not implemented in a proper way in the code... It's no different from monolith, it's just errors in code. But things like tracing are already part of the template so for basic evens  handlers are traced from deploy #1.";0
29580638;HackerNews;2021-12-16;Well, there are distributed transaction systems you could use, but usually it's a good idea to ensure things happen inside one transaction in one microservice (also leads to better code in general, IMO - keep your transaction scope as small as possible);0
29580484;HackerNews;2021-12-16;"Well, it quickly can become a mess because of people having different ideas about what microservice is, and also decrying things as &quot;for microservices only&quot; when for example I just want to offload auth and monitoring to a specialized service.<p>It's also a common trope when I'm dealing with k8s decriers - yes, you might have one application that you can easily deploy, but suddenly there are 15 other medium-weight applications that solve different important problems and you want them all ;)<p>P.S. Recently a common thing in my own architectures is separate keycloak deployment that all services either know how to use, or have it handled at separate request router (service mesh or ingress or loadbalancer)";0
29580465;HackerNews;2021-12-16;The problem with monoliths is that they're written fast and chock full of business logic that isn't reusable anywhere else.  With microservices, if done correctly a la hasura, your business logic is abstracted to config, or to the highest level of abstraction your business allows.;0
29580380;HackerNews;2021-12-16;"We've been following this modular monolith approach as well (for 3 years now), bounded contexts and all, but our architectural end goal is still &quot;mostly microservices&quot;. Maybe it's specific to PHP and the frameworks we use (what the modulith is written in) but the startup time of the monolith is just unacceptable for our processing rates (for every request the framework has to be reinitialized again, with all the 9000 services via DI and what not). Microservices tend to be much more lightweight (you don't even need DI frameworks), and monolith also encourages synchronous calls (calling a bounded context's API in memory is so simple) which has been detrimental to our performance and stability, because microservices, at least in our architecture, encourage event-based communication which is more scalable, and allows clean retries on failure etc. But again, your mileage may vary, maybe it's specific to our tools.";0
29580260;HackerNews;2021-12-16;"Former Netflix engineer and manager here.  My advice:<p>Start a greenfield project using what you know (unless your main goal is learning) keeping things as simple as possible.<p>Microservices is more often an organization hack than a scaling hack.<p>Refactor to separate microservices when either:
1) the team is growing and needs to split into multiple teams, or
2) high traffic forces you to scale horizontally.<p>#1 is more likely to happen first.  At 35-50 people a common limiting factor is coordination between engineers.  A set of teams with each team developing 1 or more services is a great way to keep all teams unblocked because each team can deploy separately.  You can also partition the business complexity into those separate teams to further reduce the coordination burden.";0
29580248;HackerNews;2021-12-16;i always think of microservices as a product an organization offers to itself. If you don't have the team including managers and even marketing to run an internal product then you probably shouldn't be doing the microservice thing.;0
29580118;HackerNews;2021-12-16;The point here for me is team dynamics, maturity, and skill level.<p>If you have a team that can't competently write and maintain a monolith that is modular and built/tested via automation, then that team has no business trying to build and maintain microservices.;0
29579910;HackerNews;2021-12-16;I tend to build monoliths, because I've got a sub-two-pizza team to work with.  If I could throw dozens of people at the problem, and moreover, needed to invent things to keep that many people busy, then I think microservices would be more interesting.;0
29579900;HackerNews;2021-12-16;Microservices are just a way to implement a distributed system.<p>The problem seems to be that quite a number of teams don't have any formation about system design (mono/distributed/mixed).<p>Most teams go for microservices because of hype and because they see an spagheti monolith and believe the problem is the monolith and not the rotten badly modularized code.;0
29579876;HackerNews;2021-12-16;"What I think nobody talks about is:  (1) the legitimate reason for breaking up services into multiple address spaces,  and (2) that using different versions of the runtime, different build systems, and different tools for logging, ORM, etc. in different microservices is slavery,  not freedom.<p>(1) is that some parts of a system have radically different performance requirements than other parts of the system.  For instance 98% of a web backend might be perfectly fine written in Ruby or PHP but 2% of it really wants everything in RAM with packed data structures and is better off done in Java, Go or Rust.<p>(2) The run of the mill engineering manager seems to get absolutely ecstatic when they find microservices means they can run JDK 7 in one VM,  run JDK 8 in another VM,  run JDK 13 in another VM.  Even more so when they realize they are 'free' to use a different build system in different areas of the code,  when they are 'free' to use Log4J in one place,  use Slf4J someplace etc,  use Guava 13 here,  Guava 17 there,  etc.<p>The rank and file person who has to actually do the work is going to be driven batty by all the important-but-not-fashionable things being different each and every time they do some 'simple' task such as compiling the software and deploying it.<p>If you standardize all of the little things across a set of microservices you probably get better development velocity than with a monolith because developers can build (e.g. &quot;make&quot;,  &quot;mvn install&quot;) smaller services more quickly.<p>If on the other hand the devs need to learn a new way to do everything for each microservice,  they are going to pay back everything they gained and then some with having to figure out different practices used in different areas.<p>(Throw docker into the mix,  where you might need to wrangle 2G of files to deploy 2k worth of changes in development 100 times to fix a ticket you can really wreck your productivity,  yet people really account for &quot;where does the time go&quot; when they are building and rebuilding their software over and over and over and over again.)";0
29579845;HackerNews;2021-12-16;"Good points, but I want to point out some caveats to a few of them. I overall disagree with your conclusion of &quot;Once you have business running, you need to dismantle it ASAP&quot;. I think it's a case-by-case thing, and you're ignoring the significant complexity costs of a microservices approach.<p>&gt; Deployment times will be longer<p>Not necessarily. I'd say that when you have multiple changes across boundaries that need to go out together, monolith deployments can actually be faster as you only need to do a rolling update of 1 container instead of N. But if by &quot;deployment time&quot; you mean the time between deploys, I agree. But also...so what? As long as your deployment times are good enough, it doesn't really need to be faster.<p>&gt; requirements for hosts where you deploy it will be bigger<p>True<p>&gt; you will end up with _huge_ instances<p>Not necessarily. Depends on the tech stack, framework, etc. I've seen .NET (and even Rails) monoliths that are huge and only use hundreds of MB/a GB or two of RAM. But I've also seen Java monoliths using 12GB+ to handle small amounts of traffic, so YMMV.<p>&gt; You'll have to scale more than you really need to because some parts of your monolith are more scalable than another<p>A problem often easily fixed with throwing a bit of $$$ at the problem, depending on your scale and per-request profitability.";0
29579660;HackerNews;2021-12-16;At the same time, you have to be at a pretty huge scale before resource over-provisioning really hurts the bottom line. You can buy a lot of compute for the price of a single engineer's salary, and it usually takes more than one engineer to support a microservice architecture. Most applications hit problems scaling the database vertically long before they exhaust resources at the application level.;0
29579543;HackerNews;2021-12-16;This is a pretty critical point. The drum I tend to beat is that the positive read of microservices is that <i>they make your code reflect your org chart</i>.<p>(If they don't do this, and that usually resolves into developers each owning a bunch of microservices that are related concepts but distinct and are context-switching all the live-long day, you've created the reverse of a big ball of mud: you've created a tar pit. Companies used to brag to me when they had 250 microservices with 100 developers, and I don't think any of those companies are going concerns.);0
29579386;HackerNews;2021-12-16;"Actually monoliths maybe faster than microservices.<p>Consider monolith -&gt; cpu l1 cache -&gt; end user<p>Microservice -&gt; account servive -&gt; foo service -&gt; l1 cache -&gt; end user<p>Ie monolith goes directly to cou cache<p>Microservice goes to network calls which are mich slower than cpu cache.<p>Also with microservices you will need distributed application performance metric for call tracing. Distributed central logging. Container orchestration platform to run all the services.";0
29579313;HackerNews;2021-12-16;Microservices mean that different parts of your code are now communicating via (e.g.) HTTP instead of function calls.<p>It changes the situation regarding deployment, but it does not miraculously absolve you from changing code when data formats change. How could it?;0
29579219;HackerNews;2021-12-16;"No one had to invent the monolith or define how far it should go; it was the default.<p>Microservices came about because companies kept falling into the same trap - that because the code base was shared, and because organizational pressures mean features &gt; tech debt, always, there was always pressure to write spaghetti code rather than to refactor and properly encapsulate. That doesn't mean it couldn't be done, it just meant it was always a matter of time before the business needs meant spaghetti.<p>Microservices, on the other hand, promises enforced separation, which sounds like a good idea once you've been bitten by the pressures of the former. You -can't- fall into spaghetti. What it fails to account for, of course, is the increased operational overhead of deploying all those services and keeping them playing nicely with each other. That's not to say there aren't some actual benefits to them, too (language agnostic, faults can sometimes be isolated), but the purported benefits tend to be exaggerated, especially compared to &quot;a monolith if we just had proper business controls to prevent engineers from feeling like they had to write kluges to deliver features in time&quot;.";0
29579078;HackerNews;2021-12-16;"Why not...<p>&gt; branch from whatever branch is in prod<p>&gt; implement change on that branch (in this case adding a column), test<p>&gt; deploy to prod<p>I realize the build and deployment process may be more complex than that making it hard... but it doesn't have to be.<p>I agree that a microservice OR even another system (a collection of services) is a good solution if you need to make quick iterative changes, and you can't do so with your current system.";0
29579063;HackerNews;2021-12-16;"&gt; monolith was always supposed to be MODULAR from the start<p>Well, that certainly <i>is</i> sensible, but I wasn't aware that someone had to invent the monolith and define how far it should go.<p>Alas, my impression is that the term &quot;monolith&quot; doesn't really refer to a pattern or format someone is deliberately aiming for in most cases, but instead refers to one big execution of a lot of code that is doing far more than it should have the responsibility to handle or is reasonable for one repository to manage.<p>I wish these sorts of battles would just go away, though, because it's not like micro services are actually bad, or even monoliths depending on the situation.  They're just different sides of the same coin.  A monolith results from not a lot of care for the future of the code and how it's going to be scaled or reused, and micro services are often written because of too much premature optimization.<p>Most things should be a &quot;modular monolith&quot;.  In fact I think most things should start out as modular monoliths inside <i>monorepos</i>, and then anything that needs to be split out into its own separate library or microservice can be made so later on.";0
29578944;HackerNews;2021-12-16;"&gt; deploy in minutes without having to rebuild and deploy a giant monolith<p>that's a tooling issue not a monolith vs microservice issue.<p>tooling can allow the exact same speed of deployment for tiny background processing without the need to completely segment your code base.";0
29578738;HackerNews;2021-12-16;"On the TDD argument, you should know what a return for a function f should be when given an argument b. Ideally, you have a set of arguments B to which b belongs, and a set of results C to which the return belongs. Your tests codifies mapping examples from B to C so that you can discover an f that produces the mapping. Take another step and generate random valid inputs and you can have property-based testing. Add a sufficient type system, and a lot of your properties can be covered by the type system itself,  and input generators can be derived from the types automatically. So your tests can be very strong evidence, but not proof, that your function under test works as expected. There's always a risk that you modeled the input or output incorrectly, especially if you are not the intended user of the function. But that's why you need user validation prior to final deployment to production.<p>Likewise, with a microservice architecture, you have requirements  that define a set of data C that must be available for an application that provides get/post/put/delete/events in a set B to your service over a transport protocol. You need to provide access to this data via the same transport protocol, transform the input protocol to a specified output protocol.<p>You also have operational concerns, like logging that takes messages in a set C and stores them. And monitoring, and authorization, etc. These are present in every request/response cycle.<p>So, you now split the application into request/response services across the routing boundary-&gt; 1 route = 1 backing model. That service can call other apis as needed. And that's it. It's not hard. It's not even module-level split depolyment. It's function-level deployment in most serverless architectures that is recommended because it offers the most isolation, while combining services makes deployment easier, that's mostly a case of splitting deployment across several deployment templates that are all alike and can be managed as sets by deployment technologies like Cloudformation and Terraform [1].<p>You can also think of boundaries like this: services in any SOA are just like software modules in any program - they should obey open/closed and have strong cohesion [2] to belong in a singular deployment service.<p>Then you measure and monitor. If two services always scale together, and mutually call each other, it's likely that they are actually one module and you won't effect cohesion by deploying them as a single service to replace the two existing ones. easing ops overhead.<p>Not deploying and running as a monolith doesn't mean not putting the code to be run into the same scm/multiproject build as a monorepo for easy shared cross-service message schema refactoring, dependency management, and version ingredient. That comes with its own set of problems -- service projects within the repo that do not change or use new portions of the comm message message schema shouldn't redeploy with new shared artifact dependencies; basically everything should still deploy incrementally and independently, scaling it is hard (see Alphabet/Google's or Twitter's monorepo management practices, for example); but there seems to be an extra scale rank beyond Enterprise size that applies to, it's very unlikely you are in that category, and if you are you'll know it immediately.<p>We like to market microservice architecture as an engineering concern. But it's really about ops costs in the end. Lambdas for services that aren't constantly active tend to cost less than containers/vps/compute instances.<p>1: <a href=""http://serverless.com//blog/serverless-architecture-code-patterns"" rel=""nofollow"">http://serverless.com//blog/serverless-architecture-code-pat...</a><p>2: <a href=""http://www.cs.sjsu.edu/faculty/pearce/modules/lectures/ood/metrics/lcom.htm"" rel=""nofollow"">http://www.cs.sjsu.edu/faculty/pearce/modules/lectures/ood/m...</a>";0
29578697;HackerNews;2021-12-16;"Many of these points you're mentioning is exactly why k8s was developed. Yes it makes deploying simple applications unnecessary hard, but it make deploying more complicated applications WAY more manageable.<p>So in the k8s world:<p>- auth: service meshes, network policies, ...<p>- monitoring: tons of tooling there to streamline that<p>- deploy: this at scale is trickier than you'd think, many seem to assume k8s on it's own here is the magic dust they need. But GitOps with ArgoCD + helm has worked pretty well at scale in my experience.<p>- Security is a CI problem, and you have that with every single language, not just Go. See Log4j.<p>Kubernetes is my bread &amp; butter, but I do realise this has way too much overhead for small applications. However, once you reach a certain scale, it solves many of the really really hard problems by streamlining how you look at applications from an infrastructure and deployment side of things. But yes - you need dedicated people who understand k8s and know what the hell they're doing - and that's in my experience a challenge on it's own.<p>Let's also dispel a myth that k8s is only suitable for microservices. I have clients that are running completely separate monolith applications on k8s, but enough of those that managing them 'the old way' became very challenging, and moving these to k8s in the end simplified thing. But getting there was a very painful process.";0
29578681;HackerNews;2021-12-16;On the point about technology lock I've thought that the move to microservices would mean allowing different programming languages, but whenever I've asked existing companies if they would be open to other languages it's usually a no. Part of it seems to be decent enough reasons but I usually think they could be overcome with a reasonable amount of work where the tradeoff may be worth it. I usually suspect it's more social reasons. My company also made the move to microservices and I was able to use another programming language but there was strong social pressure to conform.;0
29578564;HackerNews;2021-12-16;"one strong operational reason I have seen recently is resource management.<p>The monolith where most API endpoints are instant and use constant memory, but some use much more memory and can be slower... is tough.Like if you just give a bunch of memory to each process now you're overprovisioning and if you try to be strict you run into quality of service issues.<p>If you split out homogenous API endpoints into various groups you now have well behaved processes that are each acting similarly. One process could be very small, another could be much larger (but handle only one kind of request), etc...<p>of course the problem with standard microservice-y stuff is now you gotta have N different applications be able to speak your stack. The idea of a monolith with feature sets is tempting... but also can negate chunks of microservice advantages.<p>Ultimately the microservice-y &quot;everything is an API&quot; can work well even as a monolith, and you would then have the flexibility to improve things operationally later.";0
29578533;HackerNews;2021-12-16;First you publish the next version of your service with the new data, then <i>the disparate teams in charge of 12 clients</i> (not you) update the rest of the application, then the old version is retired and every team has coordinated properly. Microservices allow lean coordination, basically just asking every team when they think they'll be ready without anyone messing with someone else's code.;0
29578524;HackerNews;2021-12-16;"&gt; If your developers are writing crap code in a monolith they're going to continue writing crap code in microservices but now you have new problems of deployment, observability, performance, debugging, etc etc.<p>Anecdotally I witnessed this once. There was this huge ball of mud we had that worked okay-ish. Then the architects decided &quot;hey microservices could solve this&quot;, so we started building out microservices that became a distributed ball of mud. Every microservice shared and passed a singular data model across ~30 microservices, which made things interesting when we needed to change that model. Also, we took mediocre developers and asked them to apply rigor they didn't have in developing these services to that they were &quot;prepared&quot; for the failures that happen with distributed systems.<p>The big upside to management though was that we could farm out parts of the system to different teams on different sides of the planet and have each of them build out the microservices, with each team having different standards as to what is acceptable (what response messages look like, what coding standards should be, ect). All of this was less of a technical problem and more of a managment one, but we felt the pain of it as it was made into a technical problem.";0
29578433;HackerNews;2021-12-16;"This feels very FUDy. It gives a bunch of examples of ways in which microservices can go wrong, without empirically examining those claims. It also excludes the middle: you can have &quot;milliservices&quot; (really, just service-oriented architecture) which do more than route a single endpoint, but still give flexibility and scaling.<p>We are a young startup in the ML-service space, with about a dozen engineers + data scientists. Our stack is based on all docker containerized python. We have 6 &quot;micro&quot;-services (not sure at which point they become micro) but each plays their own role, with ~4-30 REST endpoints each. It's been fantastic, and none of us are particularly experienced with microservices. We run on AWS east but you can spin most of the stack up locally with docker-compose. I don't even need k8s, if we wanted, we could probably deploy a local cluster with docker swarm.<p>- Fault isolation<p>I can't talk in depth but we were able to handle the recent east-1 outage with only parts of the stack degraded, others stayed functional.<p>Also, rollout is most likely when something will fail. Rolling back a services is way easier than disrupting the whole stack.<p>- Eliminating the technology lock<p>The ML container is a massive beast with all the usual heavy ML dependencies. None of the other containers need any of that.<p>- Easier understanding<p>Yep, definitely true in my experience. The API surface area is much smaller than the code under the hood, so it's easier to reason about the data flow in/out.<p>- Faster deployment<p>We can easily patch one service and roll it out, rolling out hotfixes to prod with no disruption in the time to run through the CI pipeline, or roll back a task spec, which is near-instant.<p>- Scalability<p>Emphatically so. The difference in scale between our least and most used service is over 100x.<p>We could probably get away with making the user-facing backend a monolith (and in fact it's the most monolithic, has the most endpoints) but for data pipelining, micro/&quot;milliservices&quot; has been a dream. I don't even know <i>how</i> it would work as a monolith.<p>As with everything in this field, it all depends on use-case and tradeoffs. If your services each handle roughly the same load, the independent scaling argument weakens. If your business logic is complex, tightly coupled, and fits on a single box, you'll waste a ton of cycles just communicating.";0
29578424;HackerNews;2021-12-16;"Your microservices pains are legitimate. That's why we built the Control Plane (<a href=""https://controlplane.com"" rel=""nofollow"">https://controlplane.com</a>) platform. Our customers deploy microservices in seconds and get unbreakable endpoints, even when AWS, GCP or Azure go completely down.<p>They get free logging, metrics, secrets management, load balancing, auto-scaling, MTLS between services, service discovery, TLS, intelligent DNS routing to the nearest healthy cluster and much more.<p>Multi region and multi cloud used to be hard. Now they are as natural as clicking a button.<p>Before you write part two of your article, give the <a href=""https://controlplane.com"" rel=""nofollow"">https://controlplane.com</a> platform a try. One you've tried it - I'm 100% convinced you'll make a 180. I'm happy to personally demo it for you.";0
29578385;HackerNews;2021-12-16;Yeah, I think this is right.<p>Microservices is an <i>organizational</i> optimization. It can allow one team to manage and deploy their own subsystem with minimal coordination with other teams. This is a useful thing, but be aware what it's useful for.<p>If each of your developers manages a microservice, that probably reflects that you do no actual teamwork.;0
29578383;HackerNews;2021-12-16;"Microservices solve pretty much one problem: you have a larger organization (&gt; 10  devs, certainly &gt; 100) and as a result the coordination overhead between those devs and their respective managers and stakeholders is significantly limiting overall forward progress. This will manifest in various concrete ways such as &quot;microservices allow independent component release and deployment cycles&quot; and &quot;microservices allow fine grain scaling&quot;, and &quot;microservices allow components written in different languages&quot;, but really it's all Conway.";0
29578367;HackerNews;2021-12-16;"I agree. Started with a simple React for frontend and NestJS for backend. Now I am running microservices for distributing third-party widgets, my search engine, and the analytics.<p>Works well and it actually simplifies things a lot, each service has its repository, pipeline and permissions, developers don't need to understand the whole application to code.<p>You also don't have to start with Kubernetes to make microservices work, many tools can act as in-betweens. I am using app engine from gcloud, yes it's a lot of abstraction over kubernetes and it is overpriced, but I don't care. It works perfectly for these use cases and even if overpriced, it stays a low absolute value.<p>The caveat is that you really need to start off with a &quot;stateless mindset&quot;.";0
29578265;HackerNews;2021-12-16;I've started doing microservices 5-6 years ago, and I don't want to go ever writing monoliths. I don't understand why these articles keep popping up.;0
29578254;HackerNews;2021-12-16;"I agree with your position, I'm a big fan of the modular monolith approach. I took a look at your post. This is one thing that jumped out to me:<p>&gt; Because the people who design programming languages have decided that implementing logic to deal with distributed systems at the language construct level... isn't worth it<p>I'm not sure if this is just a dead end or something really interesting. The only language I really know that [does this is Erlang](<a href=""https://www.erlang.org/doc/reference_manual/distributed.html"" rel=""nofollow"">https://www.erlang.org/doc/reference_manual/distributed.html</a>), though it's done at the VM / library level and not technically at the language level (meaning no special syntax for it). What goes into a language is tricky, because languages tend to hide many operational characteristics.<p>Threads are a good example of that, not many languages have a ton of syntax related to threads. Often it's just a library. Or, even if there is syntax, it's only related to a subset of threading functionality (i.e. Java's `synchronized`).<p>So there might not be much devotion of language to architectural concerns because that is changing so much over time. No one was talking about microservices in the 90s. Plus, the ideal case is a compiler that's smart enough to abstract that stuff from you.";0
29578220;HackerNews;2021-12-16;"&gt;&gt;&gt; If I’ve got a microservice collecting events off a queue and writing a csv out to S3 on a schedule<p>I call this a worker.";0
29578102;HackerNews;2021-12-16;In my experience with enterprise software one of the things that cause most trouble is premature modularization (sibling to the famous premature optimization).<p>Just like I can't understand how people can come up with the right tests before the code in TDD, I can't understand how people can come up with the right microservices before they start developing the solution.;0
29578047;HackerNews;2021-12-16;Yes, microservices are a bad idea. But modern OSes are built using them. E.g. Linux runs as a bunch of services.;0
29577988;HackerNews;2021-12-16;Bad idea: 10 teams jointly maintaining a monolith.<p>Good idea: 10 teams each with responsibility for a separate microservice.<p>It always goes back to Conways Law.;0
29577974;HackerNews;2021-12-16;"I agree more or less with 1 and 4 mostly. But for monitoring either you would have to monitor the service calling this microservice or need to have a way to detect error.<p>&gt;  if it does have memory leaks anyways, just basic cpu/mem usage monitoring on your hosts<p>Who keeps on monitoring like this? How frequently would you do it? In a startup there are somewhere in the range of 5 microservice of that scale per programmer and daily monitoring of each service by doing top is not feasible.<p>&gt; 3. deployment? if its a go service, literally a bash script to scp over the binary and an upstart daemon to monitor/restart the binary.<p>Your solution literally is more complex than simple jenkins or ansible script for build then kubectl rollout restart yet is lot more fragile. Anyways the point stands that you need to have a way for deployment";0
29577889;HackerNews;2021-12-16;"Choose your architecture.<p>Worked with a huge monoliths, business critical, predictable usage, easy debugging, deployment simple, onboarding quick even with less documentation.<p>Worked with microservices, business critical, predictable usage, less documentation here onboarding took long time spent on how it works, Hard to debug, never needed to scale up. (Why microservices ? )<p>Lessons learnt:
Problem is never with the architecture.  Why you choose one over the other is the question to ask when you start on.";0
29577869;HackerNews;2021-12-16;Microservices are supposed to be autonomous. Independent services with an own lifecycle. What this article is describing sounds more like what is called a distributed monolith. Too many horizontal dependencies will create problems no matter where they are located.<p>I worked on monoliths where the shortest theoretical amount of time from a commit to running in production is several hours. The way you in a controlled way can change small parts of system with microservices is incredibly useful.<p>What I do see though is people making microservices too small. Like one table in a monolith database becomes one microservice. Microservices is not about having everything loosely coupled. Cohesion rules for modules still applies.;0
29577844;HackerNews;2021-12-16;Yup. This is why I think that microservices require a stronger operational plattform, but then it enables new and more effective ways of developing new tunctionality.<p>Our internal software plattform is getting to a point so it can answer most of these things - auth via the central OIDC providers, basic monitoring via annotations of the job's services, deployments via the orchestration and some infrastructure around it, including optional checks and automated rollbacks and automated vulnerability scanning on build-servers and for the running systems. It wouldn't be 15 lines of go, more like 15 lines, plus about 100-200 lines of terraform and/or yaml to get everything configured, and a ticket do register the service in the platform. It's pretty nice and our solution consultants like it very much.<p>The thing is - this took a team about a year to build and it'll take another half a year to get everything we currently want to do right. And it takes a non-trivial time to maintain and support all of this. This kind of infrastructure only makes business sense, because we have enough developers and consultants moving a lot faster with this.<p>Back when we were a lot smaller, it made a lot more sense to just push a single java monolith on VMs with chef or ansible, because that was a lot easier and quicker to get working correctly (for one thing).;0
29577792;HackerNews;2021-12-16;The cost of starting with microservice is higher than starting witht monothlics.<p>But the cost of splitting a monothlic into a microservice is x10, in some case, impossible.;0
29577782;HackerNews;2021-12-16;"For the life of me, I never understood, nor will ever understand, why people think making RPCs is easier or leads to better design than making normal function calls (&quot;split your code into microservices, it will make your code modular, smaller and easier to understand!&quot;).<p>There are legitimate reasons to put a network between one piece of code and another, but &quot;modularity&quot; is not one of them.";0
29577761;HackerNews;2021-12-16;"Managed Microservices &gt; Monoliths &gt;  Microservices";0
29577753;HackerNews;2021-12-16;If a team is not disciplined/skilled enough to build a well structured monolith the chances they can build and support a microservices solution which is a distributed system with orders of magnitude more failure modes and requires an order of magnitude more tooling and testing is pretty much 0.;0
29577727;HackerNews;2021-12-16;It is not a binary flip between monolith and modular monolith, it is on a gradual scale, and I saw teams moving toward modularity with a various degree of success. They may not even use the term of distributed monolith to name their approach. Sometimes, they do it to keep the monolith maintainable, sometimes as the first steps towards  microservices - defining boundaries, simplifying dependencies certainly help against the antipattern of distributed monolith.<p>Ideally, the decision to build modular monolith should be made and implemented from the very start of the project. Some frameworks like Django help with keeping separation.<p>I found that fitness functions help with policies and contracts. You run then in your CI/CD and they raise alarm when they detect contract / access violation across the boundaries.;0
29577722;HackerNews;2021-12-16;Well that's the thing isn't it. As soon as you move away from the atomicity of a relational database you can't guarantee anything. And then we, like you do to, resort to cleanup jobs everywhere trying to rectify problems.<p>I think that's one of the things people rarely think of when moving to microservices. Just how much effort needs to be made to rectify errors.;0
29577707;HackerNews;2021-12-16;"&gt; Splitting code into modules has the same downsides as splitting it into microservices.<p>Not to be pedantic, but it has <i>some</i> of the same downsides. Microservices have other major downsides in that they bring in all the fallacies of network computing. Even if you manage to stabilize these in the end they just waste so much time in development and debugging.";0
29577706;HackerNews;2021-12-16;"&gt;  Over 15 years though,...<p>Correct. That is why the advice not to start with Microservices. Perhaps later may make sense; but not in the beginning.";0
29577692;HackerNews;2021-12-16;But the problem with this is it's a technical solution to a social problem.<p>If your developers are writing crap code in a monolith they're going to continue writing crap code in microservices but now you have new problems of deployment, observability, performance, debugging, etc etc.<p>As an aside I have a sneaking probably ahistorical suspicion microservices hype happened because people realised Ruby (or similar trendy dynamic languages) often ended up being a write only framework and rather than try and recover some codebase sanity people would rather abandon the code entirely and chase the new codebase high.;0
29577683;HackerNews;2021-12-16;"&gt; A microservice runs as some (somewhat) privileged user, you may want some auth.<p>Auth can be a very broad term in the case of services, especially in the cloud. IAM could only allow access from certain other machines, etc...";0
29577670;HackerNews;2021-12-16;"Did someone say Microservices?<p><a href=""https://www.youtube.com/watch?v=y8OnoxKotPQ"" rel=""nofollow"">https://www.youtube.com/watch?v=y8OnoxKotPQ</a>";0
29577647;HackerNews;2021-12-16;I've argued for a long time that microservices should grow out of a monolith like an amoeba. The monolith grows until there is a clear section that can be carved off. Often the first section is security/auth, but from there it's going to be application specific. A modulith could be just another step in the carve up process.<p>But, there is no right answer here. Application domain, team size, team experience, etc... all matter and mean a solution for one team may not work for another and vice versa.;0
29577644;HackerNews;2021-12-16;"Splitting code into modules has the same downsides as splitting it into microservices. You can still end up making the wrong splits and needing to back track on things you once thought were modular but no longer are.<p>The logistics of microservices are rarely the hard part. It's the long term maintenance. Everyone who's ever maintained a &quot;core&quot; library knows the same pain, at some point you just end up making sacrifices just to get things to work.";0
29577584;HackerNews;2021-12-16;IME it's the drive for microservices that encourage the modular monolith. That is to say, the monolith is typically loosely modular (but still with many rule-breakages) until the push for microservices starts _then_ the big refactor to modularity begins.;0
29577556;HackerNews;2021-12-16;Exactly, it doesn't have to be one or the other. So far, i've been using a monolith for core functionality and microservices for independent processing tasks.<p>That way, i can attach as much functionality as i want without bloating the main app and processing scales with demand.;0
29577535;HackerNews;2021-12-16;"While it is easy to start with monolith, it's not easy to just go from monolith to Microservices in case it is determined that's the best path forward. Often organizations don't have the luxury to &quot;learn&quot; by doing a monolith first and then determining the boundaries. In most cases, market pressures, limited budgets keep monoliths monoliths. OTOH, my experience is that it is easier to combine overly granular Microservices into just right size Microservices on an ongoing basis. And yes, infra requirements are bigger for Microservices, but often it is highly automate-able.<p>I think, one of the reasons, besides all the usual reasons, that keeps Microservices in vogue is the ability to align with organizational structure and enabling independent-ish teams.";0
29577505;HackerNews;2021-12-16;"Well in my eyes equating microservices with Kubernetes is a problem in of itself. I actually wrote about Docker Swarm as a simpler and more usable alternative to it (for smaller/simpler deployments), though some other folks also enjoy Hashicorp Nomad which is also nice (another article on my blog, won't link here not to be spammy myself).<p>If you evaluate your circumstances and find that microservices could be good for you, then there are certainly options to do them more easily. In my eyes some of the ideas that have popped up, like 12 Factor Apps <a href=""https://12factor.net/"" rel=""nofollow"">https://12factor.net/</a> can be immensely useful for both microservices and even monoliths.<p>So i guess it's all very situational and a lot of the effort is finding out what's suitable for your particular circumstances. For example, i made the page over at <a href=""https://apturicovid.lv/#en"" rel=""nofollow"">https://apturicovid.lv/#en</a> When the app was released and the page was getting hundreds of thousands of views due to all of the news coverage, scaling out to something like 8 instances was a really simple and adequate fix to not break under the load.";0
29577503;HackerNews;2021-12-16;"I would say this is another problem. If an external call to a web service is involved, then you can NEVER have an atomic call in the first place. One always needs to just have a state machine to navigate these cases.<p>Even with a monolith, what if you have a power-off at the wrong moment?<p>What you are describing here is to me pretty much the job description of a backend programmer to me -- think through and prepare for what happens if power disappears between code line N and code line N+1 in all situations.<p>In your specific example one would probably use a reserve/capture flow with the payment services provider; first get a reservation for the amount, then do the external webservice call, then finally to a capture call.<p>In our code we pretty much always write &quot;I am about to call external webservice&quot; to our database in one DB transaction (as an event), then call the external webservice, and finally if we get a response, write &quot;I am done calling external webservice&quot; as an event. And then there's a background worker that sits and monitors for cases of &quot;about-to-call events without matching completed-events within 5 minutes&quot;, and does according required actions to clean up.<p>If a monolith &quot;solves&quot; this problem then I would say the monolith is buggy. A monolith should also be able to always have a sudden power-off without misbehaving.<p>A power-off between line N and N+1 in a monolith is pretty much the same as a call between two microservices failing at the wrong moment. Not a qualitative difference only a quantitive one (in that power-off MAY be more rare than network errors).<p>Where the difference is is in the things that an ACID database allows you to commit atomically (changes to your internal data either all happening or none happening).";0
29577434;HackerNews;2021-12-16;"Thank you for sharing, and I agree with you-<p>In my humble opinion microservices are &quot;hot&quot; because in theory you can scale a lot with them if you are able to do cloud provisioning.
Microservices needs DevOps+Orchestration Service.<p>A good example of microservices architecture is how K8s is designed: I think it is an overkill for most average needs, so think twice before entering in microservice trip tunnel.";0
29577430;HackerNews;2021-12-16;Don't forget serdes costs for microservices. They kill runtime speeds even with nice toys like flatbuffers.;0
29577424;HackerNews;2021-12-16;"Yes, a view would be exactly how I would address the problem I described in my other answer above (&quot;What happens if I need to change zip code representation in a data source that is read by multiple microservices?&quot;).<p>But this also means that we are now back into &quot;YesSQL&quot; territory, and specifically that we have to use a RDBMS which allows us to create Views. 
Goodbye NoSQL, goodbye Key+Valus datastore. (Or maybe you will just create an extra &quot;newZipCode&quot; and mantain it in parallel with &quot;ZipCode&quot; allowing every other consumer to adapt at their leisure...?).<p>So it is another step back to &quot;more traditional ways&quot; to design a system... or a recipe for a disaster as soon as you start dealing with significant quantities of &quot;state&quot;.";0
29577415;HackerNews;2021-12-16;Don’t end with microservices either. Monoliths are your friend.;0
29577404;HackerNews;2021-12-16;I agree.<p>The only addition: Often it is useful to design with microservices because it means you don't have to create and maintain them yourself. Even a database is a microservice. PhPMyAdmin (and similar) is a microservice, so are other Open Source projects I like: Celery, Sentry, Mailhog, Jupyter, Nextcloud even Caddy or Nginx. All could be considered microservices. Integrating them into your platform often makes a lot of sense.;0
29577383;HackerNews;2021-12-16;"Caveat: I am really not qualified to discuss the nuances (because I have never used microservices so the little I know is based on reading a bit on those here and on other online forums).<p>&quot;Single writer multiple readers&quot;, yes, this is what I would probably use, but yet again, wasn't the &quot;promise&quot; of Microservices being able to work in total isolation?<p>If I have one table (e.g. &quot;Customer&quot;) which is written by one specific microservice and read by a dozen or more... what happens when I decide that I have to change the schema because the current representation of &quot;Zip Code&quot; is not adequate anymore because, I dunno, we started dealing with UK customers now?<p>Lo and behold, I have to change code in 13 Microservices - the one actually writing to it, and the 12 more that only need to get the data to show or print or convert to JSON or whatever... ¯\_(ツ)_/¯";0
29577366;HackerNews;2021-12-16;"&gt; It's a lot of code, but it's not a lot of logic.<p>The exact reason why I compare microservices to OOP ;-)";0
29577355;HackerNews;2021-12-16;That's a fallicy. You've optimized for one use case, but you've made everything else more complicated as a consequence.<p>Deploying a single monolith is faster than deploying 10 microservices, especially if you find yourself in the model where your microservices share code, you've ended up with a distributed monolith instead of microservices.;0
29577350;HackerNews;2021-12-16;"&gt; However, I still wouldn’t recommend microservices to a new team / org starting from scratch. IMHO microservices only make sense when the system grows so vast it cannot be understood in its entirety by a single person.<p>I wouldn't go that far. The problem is prescribing a stock design solution to every problem without even considering the problem domain or what benefits it will bring.<p>There are domains where this style of programming is an absolute benefit, even at smaller scales, and it's really nothing new either. A lot of the patterns in microservice design rhyme rather well with what Erlang has done for decades.";0
29577336;HackerNews;2021-12-16;"I wonder why no one ever talks about architectures in the middle between those two - modular monoliths.<p>The point in time where you're splitting your codebase up in modules (or maybe are a proponent of hexagonal architecture and have designed it that way from the beginning), leading to being able to put functionality behind feature flags. That way, you can still run it either as a single instance monolith, or a set of horizontally scaled instances with a few particular feature flags enabled (e.g. multiple web API instances) and maybe some others as vertically scaled monoliths (e.g. scheduled report instance).<p>I wrote more about that approach on my blog, as one of the first articles, &quot;Moduliths: because we need to scale, but we also cannot afford microservices&quot;: <a href=""https://blog.kronis.dev/articles/modulith-because-we-need-to-scale-but-we-also-cannot-afford-micro-services"" rel=""nofollow"">https://blog.kronis.dev/articles/modulith-because-we-need-to...</a><p>In my eyes, the good part is that you can work with one codebase and do refactoring easily across all of it, have better scalability than just a monolith without all of the ops complexity from the outset, while also not having to worry as much about shared code, or perhaps approach the issue gently, by being able to extract code packages at first.<p>The only serious negatives is that this approach is still more limited than microservices, for example, compilation times in static languages would suffer and depending on how big your project is, there will just be a bit of overhead everywhere, and not every framework supports that approach easily.";0
29577335;HackerNews;2021-12-16;A microservice runs as some (somewhat) privileged user, you may want some auth. Can everyone internally create sales tickets? Or can everyone just query them? If a team provides a library to run, and you run it, you still only run as whatever user you have access to.<p>Monitoring: it's easier to look at a stack trace, including some other team's external library, than a HTTP error code 500.<p>Deployment is certainly <i>easier</i> when you're just shipping code and a build. You don't have to faff around with the previous instance running, maybe having some active connections/transactions/whatever, needing to launch a new one. Maybe it's not hard overall, but less fun.;0
29577323;HackerNews;2021-12-16;"I've felt SOA is the easiest to grow because it encourages you to swap out concrete implementations as requirements change. For example, IUserService can start off with a &quot;local&quot; UserService implementation that makes direct calls to a database. Once you signup with an IdP this might become UserServiceAzureAD/Okta/Auth0. Unlike microservices, I keep my compile-time guarantees that IUser continues to have the properties I require without any tooling.<p>Given the rhetoric here I worry that I'm the only person who's genuinely swapped out their implementation. The ol' &quot;N-tier is stupid - you're never going to change the database&quot; comment couldn't be more wrong.";0
29577303;HackerNews;2021-12-16;Going for microservices without a central director role is indeed madness and leads to inefficiency.<p>My employer has a landscape like that, hundreds of microservices each managed by a different team (some teams manage multiple). However, we have an enterprise architecture group whose job it is to keep an overview and make sure every microservice is meaningful and fulfills a clear role for the organization. Every project presents their architecture to this group as well as a group of peers and this often results in changes that increase cohesion and avoid redundant work. We had a few semi-interconnected monoliths before, and from what I’m told (I joined after the microservice transition) the new way is better.<p>However, I still wouldn’t recommend microservices to a new team / org starting from scratch. IMHO microservices only make sense when the system grows so vast it cannot be understood in its entirety by a single person.;0
29577260;HackerNews;2021-12-16;"microservices are such a bad idea for most companies out there.
Specially for companies doing onprem SW that suddenly want to &quot;go to the cloud with microservices&quot; without realising that micorservices is the end destination of a long journey involving serious DevOps and a mentality radically different from onprem.<p>It is just so easy to get it or implement it plainly wrong that it is a disservice to most company to suggest ms without a huge warning sign.<p>But...it the CV zeitgeist as Design Patterns and SOLID were a decade ago. These days if you don't do ms and don't deploy to k8s you're not worth your salt as a dev. And if you're a company doing monolith you're not worth the time and money of worthy investors. Our field is pop culture. It's all about belonging and 
 plain averse for history. Which is why we go in endless cycles of dogma/disillusionment.<p>I'm sorry if I sound cynic but you get some cynicism when you see the same movie about the great silver bullet the 3rd or 4th time around.";0
29577248;HackerNews;2021-12-16;"&gt; no good reason<p>How about deployment speed? If I’ve got a microservice collecting events off a queue and writing a csv out to S3 on a schedule, it’s really nice to be able to add a column and deploy in minutes without having to rebuild and deploy a giant monolith. It also allows for fine grained permissions: that service can only read from that specific queue and write to that specific bucket.<p>People throw around “distributed monolith” like it’s a dirty phrase but I’ve found it actually a very pleasant environment to work in.";0
29577230;HackerNews;2021-12-16;This sounds dreadful. I don’t think microservices are with talking about with these basics figured out.;0
29577211;HackerNews;2021-12-16;"Most people give up and do orchestration instead at some point.<p>Fortunately there are not that many things in the world that need to be 100% atomic so you can get away with a lot.<p>For your own microservices you generally have at least the <i>option</i> of &quot;fixing&quot; the problem properly even if it's at great expense.<p>But then you hit external systems and the problem resurfaces.<p>You can go crazy thinking about this stuff, at a certain point most business logic starts to look like connectors keeping different weird databases in sync, often poorly.<p>Pure crud api? Oh that's a database where client is responsible for orchestration (create folder, upload document...) and <i>some</i> operations might be atomic but there are no transactions for you. Also, the atomicity of any particular operation is not actually <i>guaranteed</i> so it could change next week.<p>Sending an email or an SMS? You're  committing to a far away &quot;database&quot; but actually you never know if the commit was successful or not.<p>Payments are a weird one. You can do this perfectly good distributed transaction and then it fails months after it succeeded!<p>Travel booking? <i>runs away screaming</i> so many databases.<p>etc.";0
29577207;HackerNews;2021-12-16;The need to do a cross-service atomic operation indicates that you chose the wrong service boundaries in your architecture.<p>And, since it's microservices, it's near impossible to refactor it, while it could have been a simple thing to reorganize some code in a monolith (where it is also a good idea to make sure that DB transactions don't span wildly different parts of the source code, but the refactor to make that happen is easier).<p>This is the big downside of microservices -- not the difficulty of doing atomic operations, but the difficulty of changing the architecture once you realize you drew the wrong service boundaries.<p>Microservices is great as long as you choose the perfect service boundaries when you start. To me, that's like saying you always write bug-free code the first time -- it's not doable in practice for large complex projects -- hence I'm not a fan of microservices...;0
29577124;HackerNews;2021-12-16;"In some companies teams are not divided in a way that follow technical faultlines, but rather after &quot;product owners&quot; so that the only valid divison lines are exteral facing superficial aspects.<p>E.g. think a car company where  you are not organized as &quot;engine team&quot; and &quot;transmission team&quot;, but rather &quot;sedan team&quot; and &quot;SUV team&quot;, and the engine and transmission just need to happen somehow.<p>The microservices fad and &quot;every team own their own services&quot; fad combined can really get performance to a halt in such a setting.<p>Product owners are suddenly the unwilling and unwitting chief architects.<p>At least with a monolith, everything is reasonably standardized and people from different teams can be expected to contribute to larger parts of it..";0
29577096;HackerNews;2021-12-16;"Except when the team division in the company does not map to any natural service API boundaries, yet it is insisted that each &quot;team&quot; own their services.<p>Then microservices increase organizational complexity too.<p>Suddenly product owners and middle management are chief architects without even knowing it.";0
29577087;HackerNews;2021-12-16;"&gt; &quot;monorepo/monolith/microservices/etc&quot; is -just- the way you organize your code<p>I don't think this is true.<p>I think — at least as far as I've observed — microservices in practice means replacing various functions calls with slow and error-prone network requests.<p>This is a big and meaningful difference.";0
29577082;HackerNews;2021-12-16;"Example of decoupling things in goblins framework (nodejs):<p>one domain give one package/module, domains are now dependencies<p>actors models can act as services, one actor in a domain is a service with an API communicating with other trought event loops or tcp/ip (microservice?)<p>we can develop and debug the whole system in a mono-repo &lt;- the monolith is the repository of code.";0
29577066;HackerNews;2021-12-16;I've seen organizations that have hundreds of developers organized in 5-10 man teams, each managing their microservice. I think it tends to happen when a large organization decides to get down with the kids and start to do microservices.<p>Conway's law enters into it in a lot of ways. Because of the way the people are organized into tiny isolated teams, the code shares that shape too. There is an event horizon one team/service away, beyond which nobody knows what happens or who wrote the code.<p>What you get is that the services actually don't do that much, except take a request from one service, translate it to an internal format, perform some trivial operation, then translate it to another external format and pass it on to another service. It's a lot of code, but it's not a lot of logic. Add to that maintaining the test and prod environments as code, and suddenly it looks like this is a lot of work, but you've essentially gotten a hundred people to do work that three people could probably accomplish if it wasn't for this pathological misapplication of an architectural pattern.;0
29577057;HackerNews;2021-12-16;"The only issue I have with microservices is when you're dealing with atomic things. Like in a monolith you'd probably just stick it all in a database transaction. But I can't find any good reads about how to deal with this in a distributed fashion. There's always caveats and ultimately the advice &quot;just try not to do it&quot; but at some point you will probably have an atomic action and you don't want to break your existing architecture.<p>Distributed transactions? Two stage commits? Just do it and rely on the fact you have 99.9% uptime and it's _probably_ not going to fail?<p>Anyone else dealt with this headache?";0
29577033;HackerNews;2021-12-16;"no why?<p>The task of the Microservice is to convert the pdf to stardust and to return it to its sender. so no auth.
Furthermore its most likely only reachable through the local network, or at least should be if you want some stranger not to be able to also make stardust from pdfs.<p>Monitoring: are you trying to say that its a lot esaier to pick up one logfile thant lets say 15? because they should be aggregated somewhere anyway no?<p>Deployment: Depending on anything you listed how do i do anything? Of course if have to define it but if you want a fancy example: k8s argocd canary deployments done. I literally set it up once.<p>security? Really?
Please dont get this wrong but this feels to me like whataboutism but well here i go:<p>i implement security just the same way as i would in the monorepo. The thing/person/entity just has to look into more repositories ;)<p>It comes down do one sentence i think:
State is not shared, state is communicated.";0
29577023;HackerNews;2021-12-16;"&gt;nice parts of microservice decoupling<p>If you require microservices to enforce decoupling you're &quot;doing it rong&quot;";0
29577019;HackerNews;2021-12-16;When microservices were new they brought a whole slew of new technologies and architecture patterns with them: SPAs, JWT authentication, micro frameworks, REST/Graphql, containerization. Things that solved problems people were having with the previous monolithic approach, all above the lack of maintainability and composability. So I see the term microservice today not as something that's measured in lines of code, but above all by embracing the new technology landscape that came from it.;0
29577013;HackerNews;2021-12-16;Microservices is kind of a superpower that allows you to choose where you want your complexity to be.  You are supposed to start with something simple, so the microservices decision needs to come in later.<p>Not starting full in with microservices is a good pattern.;0
29577007;HackerNews;2021-12-16;If there’s no organizational barrier (e.g. microservices architecture, separate repos with strict permissions) that will prevent devs from leaking abstractions across technical boundaries, those well-defined modules and interfaces will devolve into a big ball of mud.<p>I say this with the assumption that the team is large and members regularly come and go.;0
29577005;HackerNews;2021-12-16;"&gt; What if the product (department) doesn’t give a damn about the underlying system architecture? I mean shall they?<p>They should not. Either it works correctly, or it doesn't. Even buildings need to be useful, even though people might admire the architecture. No non-technical person will admire your microservice or whatever architecture.";0
29577002;HackerNews;2021-12-16;"I find with microservices, they are a fake separation of concerns. When I have to work on something, and it covers two or three services, I'm actually working on a mono app.
I've found smaller &quot;service&quot; classes that do one job meet the same need. One or two public methods &quot;perform&quot; and &quot;valid&quot; seem to work perfectly.";0
29576992;HackerNews;2021-12-16;"All ideas that are good in principle, become absurd the moment they are elevated to a kind of dogma, applied to every problem, no matter if it makes sense to do so or no.<p>Microservices are no exception from that rule, and often repeat the same mistake as OOP did with its promise of &quot;reusable code&quot;.<p>Does it sometimes make sense to break some larger services up in smaller ones? Yes.<p>Does it make sense to &quot;factor out&quot; every minor thing of the implementation into something that can individually be manhandled into a docker container because at some point in the far future, someone may save a few minutes of typing by talking to that service? No.<p>Why not? Because on the 1:1000 chance that what the service does is actually exactly what that other thing requires, it will probably take more time to implement an interface than it would to simply implement a clone of the functionality.";0
29576990;HackerNews;2021-12-16;"&gt; One or more (relational) databases, depending on whether you’re gonna go with single database per service or not<p>This imho is where serious complications can come in. A single database for all services is a good trade off if you want the nice parts of microservice decoupling but not the headaches of a distributed system. Just perhaps don’t call it “microservices” to avoid having to deal with arguments from purists who want to explain why this is not true microservices, etc.";0
29576954;HackerNews;2021-12-16;"I'd sum up with: simplicity is your friend...<p>recently I am getting more and more thoughtful about &quot;accidental&quot; complexity we add to our solutions in form of dependencies on external libs/module, frameworks such as IoC, log services anyone ;) and on the architectural side microservices etc.";0
29576953;HackerNews;2021-12-16;More and more I think of OOP and services as the same thing at different scales. Objects are services, dependency injection is your service discovery/orchestration layer. Your monolith is already a microservice architecture.<p>In the end, extracting a microservice from a monolith built this way is just a matter of moving the implementation of the object to a separate application, and making the object a frontend for talking to that application.<p>The single biggest reason OOP gets a bad reputation is because lots of languages insist on treating OOP as the be-all end-all of code structure (Java and Ruby are particularly bad examples) and insist on trying to shoehorn this sort of logic into tiny dumb pieces of pure data.;0
29576918;HackerNews;2021-12-16;Back when I was studying CS in the early 90s, it wasn't obvious at all that I am going to work with a DB anytime in my career. I loved the subject, I passed with A*. But I thought I am not going to see it later, because I didn't plan to work for a bank or some large enterprise.<p>Then, in about two years, everything changed. Suddenly, every new web project (and web was also novel) included a MySQL DB. That's when the idea about the three tier architecture was born. And since then, a few generations of engineers have been raised that can't think of a computer system without a central DB.<p>I'm telling this because in microservices I see the opportunity to rethink that concept. I've built and run some microservices based systems and the biggest benefit wasn't technical, but organizational. Once, the system was split into small services, each with its own permanent storage (when needed) of any kind, that freed the teams to develop and publish code on their own. As long as they respected communication interfaces between teams, everything worked.<p>Of course, you have to drop, or at least weaken, some of ACID requirements. Sometimes, that means modifying a business rule. For example, you can rely on eventual consistency instead of hard consistency, or replenishing the data from external sources instead of durability.<p>Otherwise, I agree with the author that if you are starting alone or in a small team, it's best to start with a monolith. With time, as the team gets bigger and the system becomes more complex, your initial monolith will become just another microservice.;0
29576911;HackerNews;2021-12-16;IMHO Microservices done well should actually cut a whole vertical through your applications feature space. So not only should it be responsible completely for its own storage of data but it should be responsible for how that data is shown on the front end (or as close to that as you can realistically achieve). A microservice should ideally be <i>reviews</i> or <i>left navigation</i> not <i>customer authentication</i> or <i>order processing</i>.;0
29576901;HackerNews;2021-12-16;"&gt; Developing a montolith for years but now you have written a 15 line golang http api that converts pdfs to stardust and put it into on a dedicted server in your office? welp thats a microservice.<p>But the 15 lines of Golang are not just 15 lines of Golang in production. You need:<p>- auth? Who can talk to your service? Perhaps ip whitelisting?<p>- monitoring? How do you know if you service is up and running? If it's down, you need alerts as well. What if there is a memory problem (because code is not optimal)?<p>- how do you deploy the service? Plain ansible or perhaps k8s? Just scp? Depending on your solution, how do you implement rollbacks?<p>- what about security regarding outdated packages the Go app is using? You need to monitor it as well.<p>And so on. The moment you need to store data that somehow needs to be in sycn with the monolith's data, everything gets more complicated.<p>Production stuff is not just about lines of code.";0
29576885;HackerNews;2021-12-16;"Well, it's all about the data responsibility: who is the owner of the data, how others can access the data. Once you have defined these, you see that you can &quot;share the access&quot; with other microservices (for example read only mode on a view), as long as the ownership and the access rules are preserved.";0
29576874;HackerNews;2021-12-16;Except that you do not need microservices to solve organisational problems. You need, as has always been done, to have well-defined modules with well-defined interfaces.;0
29576873;HackerNews;2021-12-16;I don't even think the deployment pipelines are the problem. If you have 1000 microservices you have 2^1000 possible states of your application based on any of those services being up or down (reality is much more complex). It is genuinely hard to keep that number of services up and running so you then need to be extraordinarily diligent about how you handle failures.;0
29576871;HackerNews;2021-12-16;"Clarify the microservice architecture concept with &quot;how you are going to deploy your system&quot;, as per your example, is exactly what I'm trying to explain to my teams since the microservice architecture inception. There are too many concepts conflating into the &quot;microservice&quot; term: code architecture(separation of concerns), source code organization, deployment, etc. This is very confusing, which is the reason why it's now common to say that microservices are &quot;hard&quot;.";0
29576857;HackerNews;2021-12-16;"Maybe it is just me, but I always understood that properly designed microservices have <i>their own specific datastore</i>, which is <i>not</i> shared with other microservices even if these all collaborate to the same process.<p>If this is actually (still) true, that means that &quot;the way you organize your code&quot; is a bit simplistic. Your example of an &quot;http api that converts pdfs to ...&quot; is surely a valid example of a microservice, but most business products have to handle much more &quot;state&quot; than those, and this will create further complications which go far beyond &quot;how to organize your code&quot; (and make monoliths more appealing).";0
29576854;HackerNews;2021-12-16;"Unless you have a strong technical or organizational reason to use microservices, using microservices is just more work to achieve the same results.<p>Organizational reason would be multiple people/teams who don't want or can't talk much to each other, so they develop pieces of a larger system as relatively independent projects, with clear API and responsibility boundaries. Frontend/backend style web development is an example of such approach, even though we don't typically call these parts &quot;microservices&quot;.<p>A technical reason I can see is some component of a system actually having to be written in a different stack, or to be run in a separate location for business reasons (separate physical computer, separate VMs or containers don't count). Like a firmware running on an IoT system. Or most of the system uses python, but there's a really good library in java for solving some very specific problem, so let's use it.<p>If neither of these reasons stands, you don't have a microservice architecture, you have a distributed monolith. You just replaced some function calls with RPC. RPC call which takes a much a longer time than a local one, and can randomly fail. Most of your microservices are written in a single stack, so you refactor common parts into a library, but then different services are stuck to use different versions of this library. You end up with a much slower and a much more fragile system which is harder to work on for no good reason.";0
29576849;HackerNews;2021-12-16;No wonder, you have a bad time, if you associate microservices with k8s.;0
29576842;HackerNews;2021-12-16;"&gt;monorepo/monolith/microservices/etc&quot; is -just- the way you organize your code.<p>It’s also about how you deploy your code. If you have 1000 micro services do you have 1000 deployment pipelines? If so how do you manage those pipelines? If not, you sacrifice independent deployment of each micro service.";0
29576841;HackerNews;2021-12-16;I feel like microservices are a solution for magpie developers. It's hard to keep a hundred engineers excited about working in an aging Java stack when there's all these shiny new tools out there. But maybe that's just my perspective, coming from a consultancy firm whose devs wanted to stay on the cutting edge.;0
29576836;HackerNews;2021-12-16;"Maybe I'm too old, but I don't even want to have to worry about all that. I think in terms of functions and I don't care if they are being called remotely or local.<p>That was the promise back in the day of J2EE, and it seems to me Microservices are just a rehash of that same promise.<p>Which never really worked out with J2EE - it was mostly invented to sell big servers and expensive consultants, which is how Sun made money?<p>These days I sometimes don't even bother to make extra interface classes for everything. If I need it, I can still insert that, but no need to do it up front.<p>And &quot;DevOps&quot; is just a ploy to make developers do more work, that was formerly done by admins.<p>Writing every Microservice in a new language also seems like a huge headache. The only bonus is that you can attract developers by being able to promise them that they can work with some shiny new technology. Maybe that is actually the main reason for doing that?<p>Otherwise, again, perhaps it is my age, but I prefer to minimize the dependencies. Do I really want to have to learn a whole new programming language just so that I can fix a bug in some Microservice. I personally don't want to.";0
29576833;HackerNews;2021-12-16;"Well, not git, but modularity was invented for that.<p>You can have a modular monolith that works just as well with 100 people as something service-oriented would. The difference lies in the level of discipline needed. It's much easier to &quot;just go in and make that field public because it makes my implementation easier&quot; when you have a modular monolith. With microservices, you are more explicitly changing an external API by doing that.<p>Yes, it's the same thing. But somehow, psychologically, people feel worse about changing a networked API than making an identifier public instead of private.<p>Edit: I forgot, there's one more thing: with service orientation, you can deploy in finer grains. You shouldn't have heavily stateful services, but if you do (and you always do!), it can be cumbersome to redeploy them. At that point, it's nice to be able to deploy only the parts that changed, and avoid touching the stateful stuff.";0
29576832;HackerNews;2021-12-16;I originally had my search engine running on a kubernetes-style setup off mk8s.<p>The code is a microservice-esque architecture. Some of the services are a bit chonky, but overall it's roughly along those lines, besides the search engine I've got a lot of random small services doing a lot of things for personal use, scraping weather forecasts and aggregating podcasts and running a reddit frontend I built.<p>I'd gone for kubernetes mostly because I wanted to dick around with the technology. I'm exposed to it at work and couldn't get along with it, so I figured we may get on better terms if I got to set it up myself. Turns out, no, I still don't get along with it.<p>Long story short, it's such a resource hog I ended up getting rid of it. Now I run everything on bare metal debian, no containers no nothing. Systemd for service management, logrotate+grep instead of kibana, I do run prometheus but I've gotten rid of grafana which was just eating resources and not doing anything useful. Git hooks instead of jenkins.<p>I think I got something like 30 Gb of additional free RAM doing this. Not that any of these things use a lot of resources, but all of them combined do. Everything works a lot more reliably. No more mysterious container-restarts, nothing ever stuck in weird docker sync limbo, no waiting 2 minutes for an idle kubernetes to decide to create a container. It's great. It's 100 times easier to figure out what goes wrong, when things go wrong.<p>I do think monoliths are underrated in a lot of cases, but sometimes it's nice to be able to restart <i>parts</i> of your application. A search engine is a great example of this. If I restart the index, it takes some 5 minutes to boot up because it needs to chew through hundreds of gigabytes of data to do so. But the way it's built, I can for example just restart the query parser, that takes just a few seconds. If my entire application was like the query parser, it would probably make much more sense as a monolith.;0
29576828;HackerNews;2021-12-16;"Org complexity is a valid point. Sure you can solve it  using microservices. But in this particular case (solving org complexity) such &quot;microservice&quot; is akin to a library with some RPC. You might as well have each team developing their &quot;microservice&quot; as a shared / statically linkable lib. Same thing in this context.";0
29576815;HackerNews;2021-12-16;This is the right take on this. All tech people here that rave on that microservices really make their life easier even though they are working in a small team for an entire product are looking through rose colored glasses while chucking down the koolaid and also not the intended audience. The tech complexity is hardly ever worth it unless you are a large Corp.;0
29576809;HackerNews;2021-12-16;Monoliths and microservices are two bad ways to develop software. Monoliths are rife with hidden dependencies and microservices tend to collapse from even simple faults<p>Need to rightsize the modules. More than one, but just a few with boundaries chosen around recovering from faults;0
29576805;HackerNews;2021-12-16;"Wasn't git invented for that?<p>In what way do Microservices even help? It seems to me you still have to synchronize to be sure that the Microservice from team B does exactly the things that are specified in the new version?<p>Is it not easier to have a pull request that says &quot;this will do thing x&quot;, you merge it into your monolith, and then you can see in the git log that this version will indeed to x?<p>How do Microservice organizations even manage that? Is it the reason that Atlassian has a billion dollar evaluation, because people need it to keep track?";0
29576762;HackerNews;2021-12-16;"&gt;&quot;Jenkins is not necessary, you can still deploy stuff with a local bash script and you need containerization whether you are on Microservices or Monolyth architecture&quot;<p>This is what I do. Single bash script when ran on bare OS can install and configure all dependencies, create database from backup, build and start said monolith. All steps are optional and depend on command line parameters.<p>Since I deploy on dedicated servers I have no real need for containers. So my maintenance tasks are - ssh to dedicated server and run that script when needed. Every once in a while run the same thing on fresh local VM to make sure everything installs, configures, builds and works from the scratch.";0
29576759;HackerNews;2021-12-16;"DB isn't needed. Our microservices pipeline either uses MQ, EFS or S3 for the destination for another pipeline to pick up. Unless you count those 3 as DBs ;)";0
29576747;HackerNews;2021-12-16;Microservices aren't implemented to solve technical problems,<p>rather they are used to solve organizational problems. Having 10 developers working on a single monolith? Probably fine. 100? Good luck managing that.<p>Yes they add technical complexity. But they reduce organizational complexity.;0
29576743;HackerNews;2021-12-16;"There is a lot of talk about monoliths vs microservices lately.. I just want to throw into the ring that you can do both at the same time. easily. And noone is going to kill you for it either.<p>maybe we are getting caught up in sematics because its christmas, but &quot;monorepo/monolith/microservices/etc&quot; is -just- the way you organize your code.<p>Developing a montolith for years but now you have written a 15 line golang http api that converts pdfs to stardust and put it into on a dedicted server in your office? welp thats a microservice.<p>Did you write a 150 repo application that can not be deployed seperatly anyway? welp thats a monolith.<p>You can also build a microservice ecosystem without kubernetes on your local network. We have done it for years with virtual machines. Software defined networking just makes things more elegant.<p>So dont stop using microservices because its &quot;hard&quot; or start writing monoliths because its &quot;easy&quot;, because none of that is true in the long run.<p>What is true is that you have a group of people trying to code for a common goal. The way you reach that goal together defines how you organize your code.";0
29576736;HackerNews;2021-12-16;You misunderstand their point. Not all microservices need persistence at any level. Very often, microservices are just processing things.;0
29576643;HackerNews;2021-12-16;"&gt; - DB is not an obligatory part of microservices.<p>If the microservices don't have their own store, but are all mucking around in a shared data store, everything will be much harder. I wouldn't even call that a microservice, it's a distributed something. It can work, sure.";0
29576476;HackerNews;2021-12-16;Nice article! Although I think you are overdramatizing microservices complexity a little.<p>- Kubernetes is rather a harder way to build microservices.<p>- DB is not an obligatory part of microservices.<p>- Kafka isn't as well. It's a specific solution for specific cases when you need part of your system to be based on an events stream.<p>- Jenkins is not necessary, you can still deploy stuff with a local bash script and you need containerization whether you are on Microservices or Monolyth architecture<p>- Kibana, Prometheus, Zipkin are not required. But I think you need both logs aggregation and monitoring even if you have just a Monolith with horizontal scalability.<p>Also, all this is assuming you are not using out of the box Cloud solutions.;0
