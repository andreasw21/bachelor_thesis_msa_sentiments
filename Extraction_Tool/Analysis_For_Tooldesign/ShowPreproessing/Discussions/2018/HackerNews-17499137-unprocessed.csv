ID;Source;Creation Date;Content;Sentiment
17499137;HackerNews;2018-07-10;Title:Goodbye Microservices: From 100s of problem children to 1 superstar, Content: https://segment.com/blog/goodbye-microservices/;0

17523513;HackerNews;2018-07-13;"You took the words right out of my mouth:<p>&gt;&gt;To summarize, it seems like they made some mistakes, microed their services in a knee-jerk attempt to alleviate the symptoms of the mistakes, realized microservices didn't fix their mistakes, finally addressed the mistakes, then wrote a blog post about microservices.<p>I read the article a few days ago and was struck by what a poor idea it was to take a hundred or so functions that do about the same thing and to break them up into a hundred or so compilation and deployment units.<p>If that's not a micro-service anti-pattern, I don't know what is!";0
17518584;HackerNews;2018-07-12;The failure her does seems like someone who have made microservices with the wrong bounded context (Domain Driven design). When one splits a monolith up into microservices it is essential to not split a bounded context into several micoservices (or 100+ in this case).;0
17512129;HackerNews;2018-07-12;"The issue is that it is rare and difficult to be able to synthesize all the changes happening in computing and to go deep.  So a certain “Pop culture” of computing develops that is superficial and cliche’d.  We see this in many serious subjects:  pop psychology, pop history, pop science, pop economics, pop nutrition.   Some of these are better quality than others if they have a strong academic backing, but even in areas such as economics we can’t get to basic consensus on fundamentals due to the politicization, difficulty of reproducible experiment, and widespread “popular” concepts out there that may be wrong.<p>Concepts like microservices synthesize a bunch of tradeoffs and patterns that have been worked on for decades.  They’re boiled down to an architecture fad, but have applicability in many contexts if you understand them.<p>Similarly with Agile, it synthesizes a lot of what we know about planning under uncertainty, continuous learning, feedback, flow, etc.  But it’s often repackaged into cliche tepid forms by charlatans to sell consulting deals or Scrum black belts.<p>Alan Kay called this one out in an old interview:
<a href=""https://queue.acm.org/detail.cfm?id=1039523"" rel=""nofollow"">https://queue.acm.org/detail.cfm?id=1039523</a><p>“computing spread out much, much faster than educating unsophisticated people can happen. In the last 25 years or so, we actually got something like a pop culture, similar to what happened when television came on the scene and some of its inventors thought it would be a way of getting Shakespeare to the masses. But they forgot that you have to be more sophisticated and have more perspective to understand Shakespeare. What television was able to do was to capture people as they were.<p>So I think the lack of a real computer science today, and the lack of real software engineering today, is partly due to this pop culture.”";0
17512051;HackerNews;2018-07-12;No, I’m assuming they must be deployed on an IP network of some sort, not necessarily the Internet.  A microservice by definition is a networked service.  The whole REASON the term was coined was to differentiate from services which often were developed and deployed  monolithically, as opposed to truly autonomous runtime processes (aka. bounded contexts).<p>Services, whether SOA, web services, messaging services, or network services etc, as in SOA, describe a client/server  architecture with an API.  Usually these APIs are designed by the principles of Domain Driven Design, where different teams map to bounded contexts that have their own published API.<p>Microservices are a form of SOA where each API also runs in its own process and thus has independent deployment lifecycle.  Many in the SOA world advocated for this 10-15 years ago (and often ignored), and now the industry has gone back and coined a term for this practice.<p>In an organization that does microservices properly at scale, like Amazon, you have teams that build and run and upgrade their service autonomously from others.  Read the Steve Yegge rant about Amazon and Platforms to understand this.  It allows tremendous parallelisation of effort and allows for thousands of deploys to production daily without breakage.  This is hard to pull off though, and   a new initiative doesn’t generally won’t more than a small handful of services.<p>If it’s a shared library, then call it a shared library.  It has a completely different lifecycle from a microservice.  In your Android example, shared libraries by definition are controlled by whomever can dictate OS updates, not the app developer.;0
17510041;HackerNews;2018-07-11;"<i>Verification on the system boundaries means running the app, not running a unit test</i><p>What is an app at the system boundaries if not a piece of code with dependencies?<p>If you have a microservice - FooService that calls  BarService. The &quot;system boundary&quot; you are trying to test is FooService using a fake BarService. I'm assuming that you're calling FooService via HTTP using a test runner like Newman and test results.<p>In a monolithic application you have class FooModule that depends on BarModule that implements IBarModule. In your production application you use create FooModule:<p>var x = FooModule(new BarModule)<p>y = x.Baz(5);<p>In your Unit tests, you create your FooModuleL<p>var x = FooModule(new FakeBarModule)
actual= x.Baz(5)
Assert.AreEqual(10,actual)<p>And run your tests with a runner like NUnit.<p>There is no functional difference.<p>Of course FooModule can be at whatever level of the stack you are trying to test - even the Controller.";0
17508513;HackerNews;2018-07-11;"&gt;&gt; A huge point of frustration was that a single broken test caused tests to fail across all destinations. When we wanted to deploy a change, we had to spend time fixing the broken test even if the changes had nothing to do with the initial change. In response to this problem, it was decided to break out the code for each destination into their own repos<p>We went through this painful period. Kept at it devoting a rotating pair to proactively address issues. Eventually it stabilized, but the real solution was to better decouple services and have them perform with more 9s of reliable latency. Microservices are hard when done improperly and there doesn't seem to be a short path to learning how to make them with good boundaries and low coupling.";0
17508286;HackerNews;2018-07-11;How much of the code was shared among these services?  It sounds like you essentially had mostly the same code running in 140 different configurations with only some translation logic and glue varying between each.  I'm not surprised you found this untenable.  This is akin to running a microservice per web page.;0
17507991;HackerNews;2018-07-11;Microservices are a great tool for the toolbox. Not everybody needs them. It has definitely seemed like a fad over the past few years to push everybody to use them (even projects or orgs that don't need them). I suspect that is what happened here;0
17507741;HackerNews;2018-07-11;"First of all, I'm 54 and started coding at 15 on punch cards, so I've seen pretty much every paradigm in the last 40 years.<p>It's certainly a truism that technology can be cyclical, but that's not relevant in this case.<p>The OP's statement and article &quot;Goodbye Microservices&quot; is anecdotal and incorrect.<p>I have been on teams developing microservice architectures for about six years and this particular paradigm shift has proven to be a dramatic leap forward in efficiency, especially between the business, technical architecture, and change control management.<p>When you develop a domain model, the business can ask questions about the model. The architects can answer them by modifying the model. The developers can improve services by adopting model changes in code. This is the fundamental benefit of domain driven design and works fluidly with a microservice architecture.<p>There's still a pervasive belief in technology circles that software should be developed from a purely technical perspective. This is like saying the plumber, electrician, and drywaller should design a house while they're building it. They certainly have the expertise to build a house, and they may actually succeed for a time, but eventually the homeowner will want to change something and the ad hoc design of the house just won't allow for it. This is why we have architects. They plan for change within the structure of a house. They enable modification and addition.<p>Software development is no different. The Segment developers have good intentions, but they needed to work with the business to properly model everything, then build it. Granted, it sounds like they're a fast moving and successful business, so there are trade-offs. But once the business &quot;settles&quot;, they really should go back to the drawing board, model the business, then build Segment 3.0.";0
17507668;HackerNews;2018-07-11;Lots of people judging/bashing the author (and team) that they made bad choices jumpinng on the microservice bandwagon when it clearly wasn't the correct thing to do.<p>Sure, but at the same time the team learned some valuable lessons, gained experience and hopefully matured. I applaud Alexandra for opening up and sharing that with us.;0
17507615;HackerNews;2018-07-11;"Thanks for sharing this video.<p>When she's discussing Compensations she mentions that the Transaction (T_i) can't have an input dependency on T_i-1. What are some things I should be thinking about when I have hard, ordered dependencies between microservice tasks? For example, microservice 2 (M2) requires output from M1, so the final ordering would be something like: M1 -&gt; M2 -&gt; M1.<p>Currently, I'm using a high-level, coordinating service to accomplish these long-running async tasks, with each M just sending messages to the top-level coordinator. I'd like to switch to a better pattern though, as I scale out services.";0
17507223;HackerNews;2018-07-11;I kind of get what systemizer is saying. People may think of evolution of technologies as cycles but it is never that. A new technology 'Y' is always developed because the incumbent 'X' has some shortcomings. And even after a period of disillusionment when we revert back to 'X', it is not always the same. We synthesize the good points of 'Y' back to 'X'.<p>Coming to this topic, I see Microservices as a solution to the problem of Continuous Delivery which is necessary in some business models. I can't see those use cases reverting back to Monolith architecture. For such scenarios, the problems associated with Microservices are engineering challenges and not avoidable architecture choices.;0
17507201;HackerNews;2018-07-11;Ugh. Sounds like a severe case of resume-driven architecture.<p>In my experience, you won't know whether you need microservices until you're on at least v2.0 of your application. By then, you have a better understanding of what your real problems are.;0
17507180;HackerNews;2018-07-11;"That's basically how microservices are operated on orchestrators like Kubernetes—just substitute &quot;container&quot; for &quot;VM&quot;, which is a mostly-academic difference from the perspective of your application. Operations tooling—distributed tracing, monitoring, logging...—is essential.";0
17507176;HackerNews;2018-07-11;"I think you are exactly right. Microservice architectures are definitely not automatically good, and there is nothing wrong with a well architected &quot;monolith&quot;.";0
17507112;HackerNews;2018-07-11;"&gt; I’ve seen the hype swing from micro services to soa and back to micro services a few times already.<p>SOA and microservices are the same thing; microservices is just a new name coined when the principles were repopularized so that it didn't sound like a crusty old thing.";0
17507084;HackerNews;2018-07-11;"&gt; it's much easier to process the test output and debug the microservices than monolithic applications.<p>You easier to debug end-to-end tests of a microservice architecture that monolith? That's not my experience. How do you manage to put side by side all the events when they are in dozen of files?";0
17507027;HackerNews;2018-07-11;"&gt; <i>I am a strong believer that microservices is over hyped.</i><p>In general, I fully concur: there are very few services that actually warrant a microservice architecture.";0
17507011;HackerNews;2018-07-11;"A model checker is a software program you use to validate a given mathematical model of a system. If the proposed properties of the model hold the model checker will let you know. More interestingly if the properties fail to hold you'll get a trace back into where your assumptions fell apart.<p>TLA+ is one such system that includes a language for writing models and a model checker to verify them for you. In the context of microservices you would write a model of your services and the checker would help you to verify that certain properties of your model will hold for all possible executions of the model. Properties people seem to be interested in are consistency and transaction isolation. You can develop a model of your proposed microservices architecture and work out the errors in your design before you even write a lick of code.<p>Or if you already have a microservice system you could write a model of it and find if there are flaws in its design causing those annoying error reports.<p>Amazon wrote a paper about how they use it within the AWS team [0]. Highly worth the read. And if any of this sounds interesting I suggest checking out Hillel Wayne's course he's building [1].<p>[0] <a href=""https://lamport.azurewebsites.net/tla/formal-methods-amazon.pdf"" rel=""nofollow"">https://lamport.azurewebsites.net/tla/formal-methods-amazon....</a>
[1] <a href=""https://learntla.com/"" rel=""nofollow"">https://learntla.com/</a>";0
17506945;HackerNews;2018-07-11;I have tests which verify DNA analysis. The test data vectors are large -- a few hundred MB here, a couple GB there. The hundreds of tests that use these test vectors still run in a few seconds.<p>If you're using a tape drive or SD cards, sure. But even a 10 year old 5400RPM on an IDE connection should be able to satisfy your tests' requirements in a few seconds or less.<p>I suspect your tests are just as monolithic as you think microservices shouldn't be. Break them down into smaller pieces. If it's hard to do that, then redesign your software to be more easily testable. Learn when and how to provide static data with abstractions that don't let your software know that the data is static. Or, if you're too busy, then hire a dedicated test engineer. No, not the manual testing kind of engineer. The kind of engineer who actually writes tests all day, has written thousands (or hundreds of thousands) of individual tests during their career. And listen to them about any sort of design decisions.;0
17506451;HackerNews;2018-07-11;Sounds like a typical case of hype driven development. When faced with a problem they threw buzzwords at it, hoping it would go away.<p>Had they taken the time to explore the root causes of these problems and how to best approach them, they probably wouldn‘t have taken the microservices approach to begin with.<p>I see this far too often. Instead of looking for solutions to problems, people look for problems to try the new hot solution they read about. Happened with ML/AI, NoSQL, Microservices, Blockchain, …;0
17506376;HackerNews;2018-07-11;All of these things you mentioned are either also required for a monolith (like DNS+DHCP) or are required for neither (isolated environments).<p>Does that mean that because of all the work required to properly set those up for a monolith, we better stay with microservices?;0
17506246;HackerNews;2018-07-11;"I am a strong believer that microservices is over hyped. I usually resist when senior management asks for us to use it (that proves the point of hype).<p>But by reading the first paragraphs of the article you see that the guys from Segment made a series of grave mistakes on their &quot;microservices architecture&quot;, the most important one being the use of a shared library on many services. The goal of microservices is to achieve isolation, and sharing components with specific business rules between them not only defeats the purpose, but results in increased headaches.<p>Without deep knowledge of the solution, it's hard to judge, but it seems this was never the case for microservices. They needed infrastructure isolation when the first delay issues surfaced, but there wasn't anything driving splitting the code up.<p>Sam Newman discusses on his book how to find the proper seams to split services (DDD aggregates being the most common answer) and it seems to people are making rather arbitrary decisions.";0
17506213;HackerNews;2018-07-11;I remember using BEA Tuxedo 15 years ago and I can't help wondering if theoretically one could do microservices using it or some OLTP to achieve consistency when needed. Has it progressed  over the years? Is there any free alternative to it? Or is it dead tech? When I used it it was hard and too much extra work, but it did the job successfully.;0
17506161;HackerNews;2018-07-11;"&quot;I've worked with microservices a lot. It's a never-ending nightmare. You push data consistency concerns out of the database and between service boundaries.&quot;<p>This exactly. Me too. Data consistency concerns in sufficiently large real world projects can be practically dealt with only 2 ways IMO: transactions or spec changes.";0
17506117;HackerNews;2018-07-11;I was too, of course a distributed system could also be built with DCOM and MTC (later COM+), and the DTC (Distributed Transaction Coordinator) could be used when you needed a transaction across services (or DBs, or MQs). Obviously the DTC was developed in recognition of the fact that distributed transactional service calls were a real requirement - something that current microservice architectures over HTTP REST don't seem to support.;0
17506075;HackerNews;2018-07-11;"It will be interesting to read your post in a few months time, when you hit the issues caused by using a monolith 
which could be resolved by splitting that service apart.<p>Introducing components to match on various forms of value seems odd.  Ideally standardise on those names; but if that's not possible (and you're using microservices), why not have a service to correct the name, rather than a library deployed to every endpoint?  Then you call that service with data containing `first_name` or `givenname` and it returns that message with the standardised form.  Or have a &quot;global key&quot; service; where you send the source system name and value, and have that translated for the destination via a lookup, allowing any field names or defined values to be translated by a generic reusable component:<p><pre><code>  Entity    | System Name | System Value | GlobalKey
  -------------------------------------------------------------------------------------------
  Boolean   | HR          | TRUE         | 52582622-4322-445b-bb7a-8ca118d0ca2b
  Boolean   | HR          | FALSE        | 688c2298-6b99-4c31-a356-1ab9b1caacde
  Boolean   | Finance     | 1            | 52582622-4322-445b-bb7a-8ca118d0ca2b
  Boolean   | Finance     | 0            | 688c2298-6b99-4c31-a356-1ab9b1caacde
  Boolean   | Sales       | Yes          | 52582622-4322-445b-bb7a-8ca118d0ca2b
  Boolean   | Sales       | No           | 688c2298-6b99-4c31-a356-1ab9b1caacde
  FieldName | HR          | GivenName    | 3de31cff-e4bb-4d4a-a819-fd96c7c5032e
  FieldName | HR          | Surname      | e799b891-1eeb-4598-85a5-a9534d3a3a4c
  FieldName | Finance     | First_Name   | 3de31cff-e4bb-4d4a-a819-fd96c7c5032e
  FieldName | Finance     | Last_Name    | e799b891-1eeb-4598-85a5-a9534d3a3a4c
  FieldName | Sales       | FirstName    | 3de31cff-e4bb-4d4a-a819-fd96c7c5032e
  FieldName | Sales       | Surname      | e799b891-1eeb-4598-85a5-a9534d3a3a4c
</code></pre>
As for &quot;handcrafted XML&quot;... the alternative looks like handcrafted code... For translation a language like XSLT which was designed for translating data from one format to another seems like a good choice.  You can also use this to get around your translation issue by defining a central &quot;universal&quot; format; so you can have XSLTs to translate messages from source systems to the universal format, then other XSLTs to translate from universal to the destination; so that adding or removing a system (/service) only impacts that system / you don't need to rewrite every point-to-point interaction of that service with another (OK, not point to point since we have microservices; but if they're not adding value by abstracting you away from point to point then essentially you've just got complex point-to-point interactions rather than simple point-to-point).";0
17505922;HackerNews;2018-07-11;The whole thing is released and deployed together, which is the usual distinction between microservices and not. I don't think anyone's advocating not using modules and folders to structure your code in your repository (certainly I hope not), but a single deployment artifact makes life a lot easier.;0
17505910;HackerNews;2018-07-11;The big problems with microservices come from distributed transactions being difficult and clocks getting out of sync. Multiple services on a single machine don't have that problem.;0
17505882;HackerNews;2018-07-11;you avoid them. you design your system (a set of microservices) in such a way that you don't need a transaction across service boundaries ( or rather database boundaries );0
17505807;HackerNews;2018-07-11;"Microservices are damn hard to implement correctly. Think how much math and algorithms must there be to correctly, test, version your model and API, load balance, gather logs, collect logs, archive logs, grep in logs, setup VMs/Dockers, DNS+DHCP and firewalls, load balance databases (master/slaves?) and your services, horizontal scaling maybe?, manage privileges and access, manage disk space, store and collect user-uploaded/generated files, isolate environments...<p>If you don't understand these concepts and how much work must be don't to correctly implement microservice architecture - you SHOULD STAY with monoliths, they are much easier.<p><a href=""http://principlesofchaos.org/"" rel=""nofollow"">http://principlesofchaos.org/</a>";0
17505651;HackerNews;2018-07-11;Oh man, this is just the tip of the iceberg with Microservices. There is nothing Micro about them, they are so difficult to deal with that it becomes impossible to actually iterate or build user value and introduces loads of difficult to debug problems.<p>We have architects here that dictate the design of the system but IMO they have not done the simplest implementation of anything. We have Kafka to provide ways of making each service eventually consistent so we delete something out of our domain service, but it requires absolutely huge amounts of code in various different other services to delete things in each place listening for events. Every feature is split across N different services which means N times more work + N times more difficult to debug + N times more difficult to deploy.<p>The system has been designed with buzzwords in mind - Go and  GRPC have been a disaster in terms of how quickly people have developed software (as has concourse - so many man hours wasted trying to run our own CI infrastructure it's unreal), loads of small services that are individually difficult to deploy and configure (and come with scary defaults like shared secret keys for auth - use a dev JWT on prod for example). The difficulty in dealing with debugging the system - there simply aren't the tools to understand what is going wrong or why - you have to build dashboards yourself and make your application resilient to services not existing.<p>Never ever try to build Microservices before you know what your customers <i>really</i> want - we've spent the last 6 months building a really buggy CRUD app that doesn't even have C and D fully yet. Love your Monolith.;0
17505610;HackerNews;2018-07-11;"You may also want to look into the Saga pattern - I found <a href=""https://www.youtube.com/watch?v=xDuwrtwYHu8"" rel=""nofollow"">https://www.youtube.com/watch?v=xDuwrtwYHu8</a> to be a handy high level overview for applying it to microservices.<p>Although personally, I've never felt the need to try and apply it specifically, but the idea is interesting.";0
17505538;HackerNews;2018-07-11;"I wonder if microservices will have a similar evolution as microkernels. While some microkernels almost closed the performance gap to monolith kernels, the communication overhead killed them. It turned out that the kernel's complexity could be reduced by moving some functions to the applications (&quot;library OS&quot;). Linux kernel drivers are only accepted when the functionality can't be done in user space. The extreme version is running single purpose, highly specialized unikernels on a hypervisor.";0
17505459;HackerNews;2018-07-11;"&gt; This isn’t an article about how microservices architecture is somehow bad, it’s an article about how bad Segment’s engineering team is.<p>Wow.  This is the level of discourse we've reached here now.<p>Why the ad hominem?  If you have an actual point to make on technical merit, by all means, make it.  Were you passed on in a job interview at Segment?  Why the hostility?  (honest question)";0
17505440;HackerNews;2018-07-11;Can't wait to write sane monolith with libraries instead of microservices.;0
17505262;HackerNews;2018-07-11;"Interesting perspective. I think that seeking and naming patterns &quot;microservices&quot;, &quot;agile&quot;, etc. is useful. It provides something like a domain specific language that allows a higher level conversation to take place.<p>The problem, as your identify, is that once a pattern has been identified people too easily line up behind it and denigrate the &quot;contrasting&quot; pattern. The abstraction becomes opaque. We're used to simplistic narratives of good vs evil, my team vs your team, etc. and our tendency to embrace these narratives leads to dumb pointless conversations driven more be ideology than any desire to find truth.";0
17505238;HackerNews;2018-07-11;"&gt; What exactly are you trying to accomplish?
Good test must verify the contract on the system boundaries: in case of the API, it's verification done by calling the API. We are discussing two options here: integrated application, hosting multiple APIs, and microservice architecture. Verification on the system boundaries means running the app, not running a unit test (unit tests are good, but serve different purpose). Feature flags make it only worse, because testing with them covers only non-production branches of your code.<p>&gt; Your initial assumptions were incorrect.
With nearly 20 years of engineering and management experience, I know very well how modern testing is done. :)";0
17505205;HackerNews;2018-07-11;It is weird that they took on some problems so easy. Shared libraries is one point. To get them right for hundreds slightly different services is something I don't even want to think about. The only strategy I can come up with is to maintain them as if they're a 3rd party lib and cannot contain business logic. So you're forced to build solutions around them and not with them.<p>Then there have been quite a few warnings to not use shared code in microservices.;0
17505131;HackerNews;2018-07-11;"As a microservice agnostic, i wonder how you can deal elegantly with transactions across services, concurrent access &amp; locks, etc. [Disclaimer: i have not read the article yet]";0
17505111;HackerNews;2018-07-11;"Another article with the TL;DR similar to &quot;we jumped blindly on the bandwagon, did obvious mistakes along the way, and microservices are the root of all evil&quot;.<p>Thanks, but no thanks.";0
17505040;HackerNews;2018-07-11;"&gt; It's hard to do right.<p>This is correct; I'd argue doing microservices right is even harder than doing a monolith right (like, keeping the code base clean).";0
17505000;HackerNews;2018-07-11;I’ve been tracking the comments and my sense is that almost no one here believes the business domain drives the technical solution.<p>Microservices, when constructed from a well-designed model, provides a level of agility I’ve never seen in 33 years of software development. It also walls off change control between domains.<p>My take from the Segment article is that they never modeled their business and just put services together using their best judgment on the fly.<p>That’s the core reason for doing domain driven design. When you have a highly complex system, you should be focused on properly modeling your business. Then test this against UX, reporting, and throughput and build after you’ve identified the proper model.<p>As for databases, there are complexities. Some microservices can be backed by a key-value store at a significantly lower cost, but some high-throughput services require a 12-cylinder relational database engine. The data store should match the needs of the service.<p>One complexity of microservices I’ve seen is when real-time reporting is a requirement. This is the one thing that would make me balk at how I construct a service oriented architecture.<p>See Eric Evans book and Vaughn Vernon’s follow up.;0
17504993;HackerNews;2018-07-11;Going back from 100+ microservice to 1 monolith is another extreme to me...Perhaps you can have 10 macroservices.<p>And shared library shouldn't contain business logic...;0
17504980;HackerNews;2018-07-11;"&gt; <i>To summarize, it seems like they made some mistakes, microed their services in a knee-jerk attempt to alleviate the symptoms of the mistakes, realized microservices didn't fix their mistakes, finally addressed the mistakes, then wrote a blog post about microservices.</i><p>You try to remove the critique from microservices, but for me these issues are actually good arguments against microservices. It's hard to do right.";0
17504950;HackerNews;2018-07-11;"It looks to me that the shared library issue got solved by the monorepo approach. They could have gone the monorepo way and still have microservices. 
Managing a lot of repos and keeping them consistent with regards do dependencies is not easy. In reality you do not want everyone to use a different version of a dependency. You might allow deviations but ultimately you wand to minimize them.<p>They also just might have had too many repos.";0
17504811;HackerNews;2018-07-11;"This is a common pattern, when it come to semi-idealistic memes like microservices or agile. I think it's a bad idea to have such hairy, abstract ideas travel too far and wide.<p>They become a bucket of clichés and abstract terms. Clichéd descriptions of problems you're encountering, like deployments being hard. Clichéd descriptions of the solutions. This let's everyone in on the debate, whether they actually understand anything real to a useful degree or not. It's a lot easier to have opinions about something using agile or microservice standard terms, than using your own words. I've seen heated debates between people who would not be able to articulate any part of the debate without these clichés, they have no idea what they are actually debating.<p>For a case in point, if this article described architecture A, B &amp; C without mentioning microservices, monoliths and their associated terms... (1) <i>Far</i> fewer people would have read it or had an opinion about it. (2) The people who do, will be the ones that actually had similar experiences and can relate or disagree in their own words/thoughts.<p>What makes these quasi-<i>ideological</i> in my view is how things are contrasted, generally dichotomously. Agile Vs Waterfall. Microservices Vs Monolithic Architecture. This mentally limits the field of possibilities, of thought.<p>So sure, it's very possible that architecture style is/was totally besides the point. Dropping the labels of microservices architecture frees you up to (1) think in your own terms and (2) focus on the problems themselves, not the clichéd abstract version of the problem.<p>Basically, microservice architecture can be great. Agile HR policies can be fine. Just... don't call them that, and don't read past the first few paragraphs.";0
17504729;HackerNews;2018-07-11;You are assuming that microservices must be deployed on the internet, however they can be deployed on another type of network or on a single OS installation, which would prevent them from being completely transparently updated.<p>Shared libraries can be an implementation of micro services. On a platform (e.g. Android) it can be that a shared library is updated and then all consumers are forced to update.;0
17504682;HackerNews;2018-07-11;"&gt; The disadvantage is obviously that creating such's a 'perfect architecture' is hard to do because of different concerns by different parties within the company/organisation.<p>I think you get at two very good points. One is that realistically you will never have enough time to actually get it really right. The other is that once you take real-world tradeoffs into account, you'll have to make compromises that make things messier.<p>But I'd respond that most organizations I see leave a lot of room for improvement on the table before time/tradeoff limitations really become the limiting factor. I've seen architects unable to resolve arguments, engineers getting distracted by sexy technologies/methodologies (microservices), bad requirements gathering, business team originated feature thrashing, technical decisions with obvious anticipated problems...";0
17504561;HackerNews;2018-07-11;"<i>it seems like they made some mistakes, microed their services in a knee-jerk attempt to alleviate the symptoms of the mistakes, realized microservices didn't fix their mistakes, finally addressed the mistakes, then wrote a blog post about microservices.</i><p>That seems... appropriate?<p>This is the general problem with the microservices bandwagon: Most of the people touting it have no idea when or why it's appropriate. I once had a newly hired director of engineering, two weeks into a very complicated codebase (which he spent nearly zero time looking at), ask me &quot;Hey there's products here! How about a products microservice?&quot; He was an idiot that didn't last another two months, but not before I (and the rest of the senior eng staff) quit.<p>I'm fully prepared to upvote more stories with the outline of <i>Microservices were sold as the answer! But they weren't.</i>";0
17504506;HackerNews;2018-07-11;I imagine that your development velocity with this monolith is enabled by the modularity of your code that was enforced as microservices. This benefit will wane and if you had started with a monolith would be much worse.;0
17504453;HackerNews;2018-07-11;"What do you mean by model checking? Usually anything with the keyword &quot;Design&quot; like design patterns for microservices have no science or mathematics to back it up.";0
17504357;HackerNews;2018-07-11;"It seems like splitting into separate repos was a rash response to low-value automated tests. If tests don't actually increase confidence in the correctness of the code they're negative value. Maybe they should have deleted or rewritten a bunch of tests instead. Which is what they did in the end anyway.<p>&gt;&gt; A huge point of frustration was that a single broken test caused tests to fail across all destinations. When we wanted to deploy a change, we had to spend time fixing the broken test even if the changes had nothing to do with the initial change. In response to this problem, it was decided to break out the code for each destination into their own repos<p>They also introduced tech debt and did not responsibly address it. The result was entirely predictable, and they ended up paying back this debt anyway when they switched back to a monorepo.<p>&gt;&gt; When pressed for time, engineers would only include the updated versions of these libraries on a single destination’s codebase... Eventually, all of them were using different versions of these shared libraries.<p>To summarize, it seems like they made some mistakes, microed their services in a knee-jerk attempt to alleviate the symptoms of the mistakes, realized microservices didn't fix their mistakes, finally addressed the mistakes, then wrote a blog post about microservices.";0
17504319;HackerNews;2018-07-11;You’re oversimplifying.<p>Architecturally speaking, there are some similarities between micro services and  libraries because they’re both forms of modularization and usually have an API, but there are some stark differences beyond the “dispatch mechanism”.<p>The main difference is that a service’s deployment lifecycle is completely up to the service admin.  Microservices are like websites - they can continuously evolve (within their API’s contract) without asking permission from consumers.  This is their main superpower and why they’re a way of scaling a development organization without slowing it down too much.<p>In the case of a shared library, it’s completely up to the host admin as to when to upgrade.  In the case of a static library, it’s up to the consuming software to determine when to upgrade.  A service can upgrade when it feels like it.<p>Issues of API backwards compatibility, forwards compatibility, extensibility, self descriptiveness, versioning, etc. are old issues but usually have different answers when upgrades are truly happening all the time and not just in theory.  It tends towards much fewer hard versions and more evolutionary backwards compatibility.<p>IOW, microservices aren’t a cure all, but they do encourage a set of behaviors.  Many articles detracting from them seem to have not wanted those behaviors in their org in the first place.;0
17504289;HackerNews;2018-07-11;"It appears to me that this isn't so much a case of &quot;microservices don't work&quot; as it is the case of several poor architectural choices. It is always really easy to be part of the crowd in the peanut gallery and say &quot;hah, you are doing it wrong&quot;, but when actually at the coalface the pressures and perspectives are different. Having said that, the microservices story isn't half as interesting as the Centrifuge story they link to in the article. Their approach (queues for everything) didn't work for them at their scale, so they invented something different (Centrifuge) that does. Poorly thought-out choices around microservices has little to do with the issue they solved.";0
17504220;HackerNews;2018-07-11;"It seems like their initial problems were:<p>1. Tests hitting 3rd party APIs are flakey &amp; slow
2. The job queuing mechanism can cause all jobs to be slowed by a single 3rd party API outage/slowdown<p>Eventually they arrived at:<p>1. Replay responses to speed up HTTP based tests
2. Create a smarter queuing mechanism in house<p>I'm not sure what microservices has to do with any of this. Anyway, kudos to them for having the courage openness to share their learning from mistakes!";0
17504211;HackerNews;2018-07-11;"It took me longer than it probably should have to realize that the term <i>microservices</i> was made by analogy to the term <i>microkernel</i>.  I think that part of my main issue with the &quot;microservices&quot; is that it conflates highly technical semantics with word forms that are a bit more wishy washy in meaning (i.e., a service is something formed more of human perception, not rooted directly in operating system abstractions).";0
17503970;HackerNews;2018-07-11;Glad you think so too.<p>The title and opening paragraphs gave me the impression they felt they were moving away from microservices, but maybe I didn't those bits correctly.;0
17503604;HackerNews;2018-07-11;3 full time engineers. No wonder they crashed and burned with microservices. Even a slightly distributed system, what I call miniservices, with an api and login server and half a dozen regular apps on top is way too much for such a small team. In a similar, but much smaller setup, I estimate at one point our two person team was spending as much as 25% of their time on cross-service (between the app and api / login server) concerns that would simply not exist with a monolith or something not distributed. That's 3 months out of every year of development time wasted on concerns that shouldn't even exist.;0
17503522;HackerNews;2018-07-11;The practices of Doctors are based off of science and theory. Design patterns and microservices while technical are not based off of science. They are ideas without quantitiative basis or science.;0
17503500;HackerNews;2018-07-11;"&quot;From an architectural perspective, there is absolutely no difference between a micro service and a library. The only real difference is in the dispatch mechanism.&quot;<p>I always tell people if you can't write and maintain a library then don't do microservices.";0
17503441;HackerNews;2018-07-11;Microservices do have various organizational tradeoffs. Individual teams can now own deployment, operation, language, and tooling choices for better or worse. This is probably advantageous for large companies where the number of teams scales beyond what a hierarchical control structure can support. In other words, a single devops team can't make good decisions and prescriptions about languages, tooling, deployment, etc nor effectively react to feedback from the dev teams. Control (power and responsibility) becomes distributed instead of centralized, with all the tradeoffs that entails.;0
17503317;HackerNews;2018-07-11;<i>With separate microservices and databases, you could just pause that microservice. With one application and one database, all teams need to be aware of the highest SLA requirements when doing their respective deployments, and design for it. It is certainly doable, but requires a higher level of alignment between the development teams.</i><p>That’s easily accomplished with a Blue-Green deployment. As far as the database, you’re going to usually have a replication set up anyway. So your data is going to live in multiple databases anyway.<p>Once you are comfortable that your “blue” environment is good, you can slowly start moving traffic over. I know you can gradually move x% of traffic every y hours with AWS. I am assuming on prem load balancers can do something similar.;0
17503295;HackerNews;2018-07-11;I saw this at my last company. Even worse breaking it into microservices allowed teams of 2 or 3 to start building entire realms where only they could commit and used it for internal politics.<p>I witnessed someone that wanted to leverage their service into a promotion so they started pushing for an architecture where everything flowed through their service.<p>It was the slowest part of our stack and capped at 10tps.;0
17503189;HackerNews;2018-07-11;Segment is very much built around microservices. They just consolidated 100+ of these destination services into a single one. That's it.;0
17503045;HackerNews;2018-07-11;"What this actually looks like is a case of very imperfect knowledge transfer within the technology industry. What's interesting about the entire craze of microservices and containers and the cloud is that they are not a new technology. This entire architecture -- the so-called lambda architecture -- has been the standard approach to developing high-performance trading systems on Wall St. since the early 90s. The architecture is literally 30 years old and certain technologies (event sourcing, event driven architecture) are even older.<p>The problem is that the all the best knowledge has clearly not made it out. For example, this design introduces a &quot;Centrifuge&quot; process that redirects requests to destination specific queues... congratulations you've just reinvented a message bus, a technology that goes back to the 80s. There is absolutely nothing new about virtual queues as described here but unfortunately the authors are likely not at all aware of the capabilities of real enterprise messaging systems (even free, open-source ones like Apache Artemis) and certainly not aware of the architecture and technologies and algorithms that underlie them and the (admittedly much more expensive) best-of-breed commercial systems.<p>(I won't even go into the craziness of 50+ repos. That's just pure cargo cult madness.)<p>Watching the web/javascript reinvent these 30 year old technologies is a little disheartening but who knows they may come up with something new. (Then again, recently the javascript guys have discovered the enormous value of repeatable builds. Unfortunately the implementations here all pretty much suck.) Still, we ought to perhaps ask ourselves why this situation has come about...";0
17503032;HackerNews;2018-07-11;Microservices benefits large teams. There’s just too much operational overhead for a small team to manage microservices. Large teams with monolith applications benefit from microservices as the communication issue of large teams is reduced by splitting the team into cross functional teams.;0
17502950;HackerNews;2018-07-11;I must agree. Seems like every step could have been solved without jumping to the conclusion that it was because we didn't use a monolith.<p>I've never understood the false dichotomy of microservices vs monolith... just split things when it makes sense. ¯\_(ツ)_/¯;0
17502882;HackerNews;2018-07-11;"&gt; It’s easier to just press a button and have your application go to all of your servers based on a deployment group.<p>It's not so much about the deployment process itself (I agree with you that this can be easily automated), but rather about the deployment granularity. In a large system, your features (provided by either components or by independent microservices) usually have very different SLAs. For example, credit card transactions need to work 24x7, but generating the monthly account statement for these credit cards is not time-critical. Now suppose one of the changes in a less critical component requires a database migration which will take a minute. With separate microservices and databases, you could just pause that microservice. With one application and one database, all teams need to be aware of the highest SLA requirements when doing their respective deployments, and design for it. It is certainly doable, but requires a higher level of alignment between the development teams.<p>I agree with your remark about refactoring. In addition, when doing a refactoring in a microservice, you always need a migration strategy, because you can't switch all your microservices to the refactored version at once.";0
17502843;HackerNews;2018-07-11;I don't think you reverted to a monolith. I think you built a singular service with more cohesion than when it was split into 140 repos. And it sounds like the split wasn't because of the microservices architecture but because you didn't have a good solution for the queue problem.;0
17502840;HackerNews;2018-07-11;It seems like splitting off small repos for everything is a solution looking for a problem. Some of the most successful software companies out there have monolithic repos, but not monolithic services.<p>The real solution is microservices in monorepos.;0
17502829;HackerNews;2018-07-11;"I've heard of even Unicorns throwing consistency out the window. Apparently Netflix has a bunch of &quot;cleanup jobs&quot; that comb the database for various inconsistencies that inevitably show up.<p>You can't have consistent microservices without distributed transactions. If a service gets called, and inside that call, it calls 3 others, you need to have a roll back mechanism that handles any of them failing in any order.<p>If you write to the first service and the second two fail, you need to write a second &quot;undo&quot; call to keep consistent.<p>Worse, this &quot;undo state&quot; needs to be kept transactionally consistent in case it's your service that dies after the first call.<p>In reality, nobody does this, so they're always one service crash away from the whole system corrupting the hell out of itself. Since the state is distributed, good luck making everything right again.<p>Microservices are insane. Nobody that knows database concepts well should go near them";0
17502767;HackerNews;2018-07-11;"My problem with <i>microservices</i> is the word 'micro'. It should just be <i>services</i>.<p>Problem domains (along with organizational structures) inherently create natural architectural boundaries... certain bits of data, computation, transactional logic, and programming skill just naturally &quot;clump&quot; together. <i>Micro</i>services ignore this natural order. The main driving architectural principle seems to be &quot;I'm having trouble with my event-driven dynamically-typed metaprogrammed ball-of-mud, so we need more services!&quot;.";0
17502762;HackerNews;2018-07-11;"&gt; The shared libraries made building new destinations quick. The familiarity brought by a uniform set of shared functionality made maintenance less of a headache.<p>&gt; However, a new problem began to arise. Testing and deploying changes to these shared libraries impacted all of our destinations. It began to require considerable time and effort to maintain. Making changes to improve our libraries, knowing we’d have to test and deploy dozens of services, was a risky proposition. When pressed for time, engineers would only include the updated versions of these libraries on a single destination’s codebase.<p><i>When pressed for time, engineers would only include the updated versions of these libraries on a single destination’s codebase.</i><p>I think I see the problem and it wasn't with microservices.";0
17502682;HackerNews;2018-07-11;This might come off as being snide, but I'm genuinely curious: was their solution really just having all the services in one repository? That doesn't seem like a problem with microservices at all but more of a devops problem. To be clear, I'm not arguing for microservices, I'm just trying to understand if this was really a problem with splitting off multiple repos. Maybe I'm just really dense and someone can set me straight.<p>I've actually had experiences with seemingly this same problem at a previous startup. Once we started spinning off individual repositories for small pieces of business logic stuff started to go downhill as the logistics of communicating and sharing one another's code became more and more complex.;0
17502648;HackerNews;2018-07-11;Been there. OTOH, my last job was at a startup that used a LAMP stack but made enough money to be self-sufficient and not depend on VC money to keep running.<p>When the legacy systems started to hurt us (because they were written by the founder in a couple of weeks in the most hacky way), we decided against microservices and went to improve the actual code into something more performing and more maintenable,also moving from PHP5 to PHP7.<p>As much as we all wanted to go microservices and follow the buzz, we were rational enough to see that it didn't make any sense in our case.;0
17502644;HackerNews;2018-07-11;“Recall that the original motivation for separating each destination codebase into its own repo was to isolate test failures. However, it turned out this was a false advantage. ”<p>At least they finally got to the right conclusion, they were on the wrong path to begin with.<p>People seem to be blaming Microservices when they weren’t even close to understanding what they were doing and why. I’d be much more interested in an article about issues faced with Microservices where they actually tried to slice their functionality based on their domain.;0
17502633;HackerNews;2018-07-11;If you're using microservices why are you using shared libraries? Wouldn't it make sense to break the common portion into a separate service and define an API for it?<p>I swear more people writing microservices need to read about flow-based programming and perhaps the actor model.;0
17502549;HackerNews;2018-07-10;"See the below response for more details. But that’s not how modern testing works (neither is my feature flag suggestion).<p><a href=""https://www.developerhandbook.com/unit-testing/writing-unit-tests-with-nunit-and-moq/"" rel=""nofollow"">https://www.developerhandbook.com/unit-testing/writing-unit-...</a><p>It works similarly in almost every language.<p><i>For a certain class of applications and organizational constraints, I also would prefer it. But it requires a much tighter alignment of implementation than microservices (e.g., you can't just release a new version of a component, you always have to release the whole application).</i><p>Why is that an issue with modern CI/CD tools? It’s easier to just press a button and have your application go to all of your servers based on a deployment group.<p>With a monolith, with a statically typed language, refactoring becomes a whole lot easier. You can easily tell which classes are being used, do globally guaranteed safe renames, and when your refactoring breaks something, you know st compile time or with the correct tooling even before you compile.";0
17502494;HackerNews;2018-07-10;I found the frustrations with microservices too setting up a pretty substantial ETL system using Amazon Lambda among other services.<p>Originally you couldn't trigger Lambda functions from SQS (seemingly the most obvious integration). You could use Kinesis but small print says Lambda concurrency is restricted to the Kinesis streams which gets very expensive.<p>Visibility/monitoring into most microservices is not good (Iron.io is quite nice but any concurrency is really expensive). I don't like the workflow for deployment and testing either.<p>So I shifted to single EC2 instance with my application and Beanstalkd w/ my own configurable workers. Way cheaper, easier to manage, normal programming workflow, etc.<p>For some use-cases Lambda and other services are really nice and efficient but there's usually a lot of hidden limitations so be sure to spend a lot of time evaluating before committing. You often spend way more time fighting the microservice drawbacks than the benefits are worth.;0
17502420;HackerNews;2018-07-10;Getting the feeling Segment didn't really stop using microservices.<p>Sounds like they just redrew their service boundary, from Integration APIs to Business Function.<p>Centrifuge sounds like a new service deals with connecting to integration APIs, so they've replaced 140+ services with one.<p>Another service they've spun up is Traffic Recorder, and its responsibility is to eliminate the need for http requests when testing integrations.<p>Feels like the biggest change is going from so many repos to a monorepo.;0
17502415;HackerNews;2018-07-10;"What exactly are you trying to accomplish?<p>If you are testing a single microservice and don’t want to test the dependent microservice - if you are trying to do a unit test and not an integration test, you are going to run against mock services.<p>If you are testing a monolith you are going to create separate test assemblies/modules that call your subject under test with mock dependencies.<p>They are both going to be part of your CI process then and either way you aren’t going to publish the artifacts until the tests pass.<p>Your deployment pipeline either way would be some type of deployment pipeline with some combination of manual and automated approvals with the same artifacts.<p>The whole discussion about which is easier is moot.<p>Edit: I just realized why this conversation is going sideways. Your initial assumptions were incorrect.<p><i>you may want to test only A: with monolithic architecture you'll have to produce another build of the application, that contains mock of B (or you need something like OSGi for runtime module discovery).</i><p>That’s not how modern testing is done.<p><a href=""https://www.developerhandbook.com/unit-testing/writing-unit-tests-with-nunit-and-moq/"" rel=""nofollow"">https://www.developerhandbook.com/unit-testing/writing-unit-...</a>";0
17502401;HackerNews;2018-07-10;"I have been doing some tech advice jobs on the side to see what's going on in the world and it's really scary what I found. Only yesterday I was talking with the cto of a niche social networking company that has a handful of users and probably won't get much more who was telling me the tech they use; Node, Go, Rust, Mongo, Kafka, some graph db I forgot, Redis, Python, React, Graphql, Cassandra, Blockchain (for their voting mechanism...), some document database I had never heard of and a lot more. A massive, brittle, SLOW (!) bag of microservices and technologies tied together where in 'every micro part' they used best practices as dictated by the big winners (Facebook, Google, whatever) in Medium blogs. It was a freak show for a company of 10 engineers. But this is not the first time I encounter it; 3 weeks ago, on the other side of the world, I found a company with about the same 'stack'.<p>People really drink the koolaid that is written on these sites and it is extremely detrimental to their companies. PostgreSQL with a nice boring Java/.NET layer would blow this stuff out of the water performance wise (for their <i>actual real life usecase</i>), would be far easier to manage, deploy, find people for etc. I mean; using these stacks is good for my wallet as advisor, but I have no clue why people do it when they are not even close to 1/100000th of Facebook.";0
17502400;HackerNews;2018-07-10;I like that they highlight the tradeoffs between isolating faults and optimizing resource utilization. However, you don't need microservices to achieve isolation. You can have different worker pools of the monolith configured to handle different destinations. The monolith actually gives you more flexibility in how you approach the tradeoff. With microservices you're forced into one pool of workers per destination, but with the monolith you can choose any mapping of pools to destinations that makes sense.;0
17502379;HackerNews;2018-07-10;The thing people failed to realize is that microservices are a way of structuring people in teams in an organization, not a way of structuring a product architecture.<p>To use words that are mine, microservices are a hack on Conway's law. One team should be responsible for 2-3 microservices and should have a lot of autonomy.;0
17502325;HackerNews;2018-07-10;"The problem is it's not clear when to use what. Some get confused and use it in the wrong place.  Here are some questions to ask before you use microservices.<p>Does the service need an independent and dedicated team to manage its complexities, or is it a &quot;part time&quot; job? Try a Stored Procedure first if its the second.<p>Is the existing organization structure (command hierarchy) prepared and ready for a dedicated service? (Conway's law) Remember, sharing a service introduces a dependency between all service users. Sharing ain't free.<p>Do you really have a scalability problem, or have you just not bothered to tune existing processes and queries?  Don't scrap a car just because it has a flat tire.";0
17502271;HackerNews;2018-07-10;Every microservice on its own little cluster of VMs for HA and performance...<p>A couple of hundred VMs is nothing in a scenario like that. Good luck trying to debug anything.;0
17502258;HackerNews;2018-07-10;You will end up with something like OSGi. That can be the right choice, but is also a quite 'heavyweight' architecture.<p>For a certain class of applications and organizational constraints, I also would prefer it. But it requires a much tighter alignment of implementation than microservices (e.g., you can't just release a new version of a component, you always have to release the whole application).;0
17502221;HackerNews;2018-07-10;Thesis: monoliths in one repo<p>Antithesis: microservices in separate repos<p>Synthesis: microservices in a monorepo;0
17502174;HackerNews;2018-07-10;I agree completely.<p>I think that regardless of whether microservices works for anyone or not, they came about to address a real issue that we still have, but that I’m not sure anyone has fully solved.<p>I think that microservices are an expression of us trying to get to a solution that enables loose coupling, hard isolation of compute based on categorical functions. We wanted a way to keep Bob from the other team from messing with our components.<p>I think most organizations really need a mixture of monolithic and microservices. If anyone jumps off the cliff with the attitude that one methodology is right or wrong, they deserve the outcome that they get. A lot of the blogs at the time espoused the benefits without bothering to explain that Microservices were perhaps a crescent wrench and really most of the time we needed a pair of pliers.;0
17502137;HackerNews;2018-07-10;"<i>&gt; Recall that the original motivation for separating each destination codebase into its own repo was to isolate test failures.</i><p>This seems like a weird reason to adopt microservices. Can't you isolate tests within the same repo using folders?";0
17502130;HackerNews;2018-07-10;I once scoffed at microservices.<p>Then I was tasked with building a scalable platform around a message broker system.<p>Then the switch clicked. I get it now.;0
17502121;HackerNews;2018-07-10;This misses the mark completely.  A separate repo and service simply for different serialization and routing?  Each performing the same basic function.  That’s not a legitimate use-case for microservice separation and was doomed from the start.;0
17502120;HackerNews;2018-07-10;It's exactly the other way around:<p>If you can afford all your components sharing the same database without creating a big dependency hell, then your problem is _too small_ for microservices.<p>If your problem is so large that you have to split it up to manage its complexity, start considering microservices (it might still not be the right option for you).;0
17501902;HackerNews;2018-07-10;"<i>I'd wager that microservices, a lot of the time, are basically used as a management structure</i><p>Conway’s Law in action, or in reverse <a href=""https://en.wikipedia.org/wiki/Conway's_law"" rel=""nofollow"">https://en.wikipedia.org/wiki/Conway's_law</a>";0
17501896;HackerNews;2018-07-10;Yeah. We're currently in a second push to introduce microservices and this time I'm on board. The first time was more about enforcing code ownership and such, and that was going to be a mess.<p>Now, people are  pondering things like: Ok, management of elasticsearch has performance issues and it's a general pain if the elasticsearch documents change. So let's try to move the schema of elasticsearch documents into a strictly semver'd artifact. And let's move management of our search indexes into a service depending on that artifact, so the schema changes in a controlled way. And ops and the search team can scale and optimize the service as they need to minimize search outages.<p>Creating smaller services based on problems is a good thing.  Creating smaller services because of ... reasons... not so much.;0
17501730;HackerNews;2018-07-10;"Didn't take long for &quot;we used shared libraries, and then found out we couldn't deploy independently&quot;. Sounds like you weren't quite doing microservices?";0
17501685;HackerNews;2018-07-10;They're both bad.  It feels like a reverse Sophie's Choice to have to pick one.<p>The real friction in the system is always in the boundaries between systems. With microservices it's all boundaries.  Instead of a Ball of Mud you have Trees and No Forest.   Refactoring is a bloody nightmare.  Perf Analysis is a game of finger pointing that you can't defuse.;0
17501557;HackerNews;2018-07-10;We should bring Categorical concepts into microservices. Maintain compose-ability between entities rather than forming a graph of intercommunicating objects similar to OOP.<p>I don't know if adhering to these principles will make microservices better.;0
17501549;HackerNews;2018-07-10;There's actually no reason you can't architect microservices like this. You can put RabbitMQ or some AMQP service as a comm layer between services. But then you have to architect your system to be event-driven. It's not a bad approach.;0
17501542;HackerNews;2018-07-10;well, we can think of the synthesis as returning to monolithic services with the understanding the sufferings of microservices.;0
17501467;HackerNews;2018-07-10;This is really missing the point. Microservices are not about code organisation - they are about runtime separation.<p>And even kernels have kernel threads, which are basically local microservices. Anything which needs to scale beyond a single system is more deserving of microservices than a kernel.;0
17501462;HackerNews;2018-07-10;But does this timeline apply to microservices or monoliths? Methinks both...;0
17501451;HackerNews;2018-07-10;"I can't help but think that this is an experience report covering &quot;microservices-done-wrong considered harmful&quot;. There are definitely pitfalls to the microservice approach, but there are canonical answers to all of these issues. That said, the right approach is the one that fits with your team's disposition, skill-set, and experience level, so ditching microservices might well have been the correct choice in this case.<p>&gt; Eventually, all of them were using different versions of these shared libraries. We could’ve built tools to automate rolling out changes, but at this point, not only was developer productivity suffering but we began to encounter other issues with the microservice architecture.<p>In the extreme case where you have 150 microservices and 3 devs, I think that spending a few days to build a tool that auto-updates your common deps and re-runs your tests would be a good investment. Or you could pay someone else to do this with a service like <a href=""https://www.dependencies.io/"" rel=""nofollow"">https://www.dependencies.io/</a>. Handling common code is one of the known pain points in microservices, so it's worth tackling head-on. (Last I saw Netflix handles this by the rule &quot;no services can share code unless it's by an open source library&quot;, which encourages common code to be thoughtfully packaged and released.)<p>&gt; The additional problem is that each service had a distinct load pattern. Some services would handle a handful of events per day while others handled thousands of events per second. For destinations that handled a small number of events, an operator would have to manually scale the service up to meet demand whenever there was an unexpected spike in load.<p>I can see this being tricky to tune, and am hesitant to opine without knowing the details, but if you can fix the problem by bundling all the services into a single monolith (i.e. aggregating all load into N nodes), then you should also be able to fix the problem by using a cluster scheduler like k8s with equivalently sized nodes. As long as you don't have bursts that are an integer factor of your baseline system load, both approaches should work equivalently (to the first order). It sounds like they were running individual instance(s) per microservice, which isn't a good fit for very bursty services.<p>And as a bonus, with a cluster scheduler you get a number of primitives to do resource reservation, which you don't get for free if you're merging all of your services back inside a single monolith. This means the problem of back-pressure from a single misbehaving endpoint -- which was one of the reasons they moved to microservices in the first place -- will probably come up in some form down the road.<p>&gt; Recall that the original motivation for separating each destination codebase into its own repo was to isolate test failures. However, it turned out this was a false advantage. Tests that made HTTP requests were still failing with some frequency. With destinations separated into their own repos, there was little motivation to clean up failing tests. This poor hygiene led to a constant source of frustrating technical debt.<p>I don't have anything to say here except... don't do this? If your tests are failing you should be fixing your tests (or removing them if they aren't adding value), not adding new integrations. Consistently-failing tests are a big warning sign that your CI/CD process is not in good shape, and a healthy CI/CD process is a strict precursor to doing microservices successfully.<p>&gt; The outbound HTTP requests to destination endpoints during the test run was the primary cause of failing tests. Unrelated issues like expired credentials shouldn’t fail tests.<p>Your UTs shouldn't be hitting your external dependencies; the correct solution here is the one that they eventually landed on, i.e. to either record/replay real HTTP requests, or to mock out the HTTP responses manually. You still need real integration and smoke tests in a production-like environment to make sure that you've not missed a change in the remote API schema. IME this is one of the biggest challenges of working with external APIs, and I don't envy the task of maintaining 150 integrations, however this issue seems unrelated to microservices.";0
17501431;HackerNews;2018-07-10;"<p><pre><code>  +
  -js
  -java
    -monolith.jar {guava, netty, etc }
      -svc1
      -svc2
  -python
  -go
</code></pre>
For biz code, I've seen this kind of lib-ifying architecture provide a nice microservices workflow. The unifying heuristic is: Any dependency goes into the lib project. Anything unique to the service that requires no dep should usually go into the service project. The nice thing about this is it modular with fast compiles and preserves optionality. Since everyone is using the same deps, the code can live in the svc project or the lib project. A bit of namespacing convention makes it's trivial to shuttle the code between the two projects to wherever it's most natural to have it.";0
17501375;HackerNews;2018-07-10;"the model in GP is useful, but it doesn't account for the unequal distribution of information.  e.g.  some people are perceiving the cycle at a different phase and can be convinced  to work &quot;harder, not smart&quot; for an interval to shift their position within the cycle.<p>E.g. &quot;Let's get really good at microservices so we don't need a monolith.&quot;  IOW there's no consensus about when to dig in and go deeper.  And even if there was, some people would leverage that information and try to pull ahead of the others digging in.";0
17501322;HackerNews;2018-07-10;Easy until you have 100,000 of them anyway, in which case it's expensive and slow to run it for every dev. (At that point you have enough devs that microservices 100% make sense, though);0
17501319;HackerNews;2018-07-10;"The whole article reads like BS to me.<p>So the initial problem was a single queue? Well, then split the queue, no need to go all crazy splitting all the code.<p>Switching to 100+ microservices? There is no need to switch to 100+ repos too, runtime services don't need to have one repo per service, just use a modular approach, or even feature flags.<p>100+ microservices, some of them with much lower load than others? Then consolidate the lower load ones, no need to consolidate &quot;all&quot; of the microservices at once.<p>Library inconsistencies between services? No, just no, always use the same library version for all services. Automate importing/updating the libraries if you need to.<p>A single change breaks tests in a way you need to fix unrelated code? WTF, don't you have unit tests to ensure service boundary consistency and API contracts?<p>Little motivation to clean up failing tests? Yeah... you're doing it wrong.<p>Only then you figure out to record traffic for the tests? HUGE FACEPALM, that's the FIRST thing you should do when dealing with remote services!";0
17501314;HackerNews;2018-07-10;I think what was missed, in the article, is that the fundamental problem was centered around a shared architecture of destinations and shared code.<p>You cannot possibly have every destination be a separate repo and then have the development lifecycle of your shared code be so active that it ultimately puts at risk the architecture of your entire organization.<p>What makes shared code so perfect is having stability such that you extract your variant code into your non-shared code. Shared code should evolve at a much slower pace than your non-shared code or you risk this very outcome.<p>Microservices are not dead, nor are they the solution to everything. We need better architects.;0
17501308;HackerNews;2018-07-10;"I'm not sure how worthwhile it is writing any more &quot;microservices are dumb&quot; articles - all the people who have spent the last 5 years leaving microservice messes in their wake appear to have moved on to creating &quot;serverless&quot; messes of lambda functions which people like you and me are going to be going around tidying up in about 5 years from now.";0
17501296;HackerNews;2018-07-10;"(a) a prototype
(b) a set of applications that share a database.<p>You would have to have an oddly disconnected schema if modifications to the program don't result in programs accessing parts of the database that other programs are already accessing. If this isn't a problem it means you're using your database as nature intended and letting it provide a language-neutral, shared repository with transactional and consistency guarantees.<p>so maybe not microservices, but fine nonetheless.<p>EDIT: two more comments:<p>- this is exactly what relational databases were designed for. If people can't do this with their micro-services, maybe their choice of database is the issue.<p>- &quot;micro-service&quot; as the original post suggests, is not synonymous with good. &quot;monolith&quot; is only synonymous with bad because it got run-over by the hype-train. If you have something that works well, be happy. Most people don't.";0
17501225;HackerNews;2018-07-10;"One example I'm familiar with that <i>sounds</i> like microservices is the Robot Operating System (ROS). At its heart it's just a framework for pub/sub over IP, it just happens to be targeted towards robotics. A ROS system comprises of 'nodes' for each logical operation e.g. image acquisition -&gt; camera calibration -&gt; analysis -&gt; output.<p>The system is defined by a graph of these nodes, since you can pipe messages wherever they're needed; all nodes can be many-many. Each node is a self-contained application which communicates to a master node via tcp/ip (like most pub/sub systems a master node is requried to tell nodes where to send messages). So you can do cool stuff like have lots of seprate networked computers all talking to each other (fairly) easily.<p>It works pretty well and once you've got a particular node stable - e.g. the node that acquires images - you don't need to touch it. If you need to refactor or bugfix, you only edit that code. If you need to test new things, you can just drop them into an existing system because there's separation between the code (e.g. you just tell the system what your new node will publish/subscribe and it'll do the rest).<p>There is definitely a feeling of duct tape and glue, since you're often using nodes made by lots of different people, some of which are maintainted, others aren't, different naming conventions, etc. However, I think that's just because ROS is designed to be as generic as possible, rather than a side effect of it running like a microservice.";0
17501224;HackerNews;2018-07-10;In my experience, there's a lot of cargo culting around microservices. The benefits are conferred by having a strong team that pays attention to architecture and good engineering practices.<p>Regardless of whether you are a monolith or a large zoo of services, it works when the team is rigorous about separation of concerns and carefully testing both the happy path and the failure modes.<p>Where I've seen monoliths fail, it was developers not being rigorous/conscientious/intentional enough at the module boundaries. With microservices... same thing.;0
17501208;HackerNews;2018-07-10;I'm thinking it's more of a software business life cycle. Microservices are somewhat easier to deploy, which makes it easy for developers to add capabilities to be used by UX developers. Once the eye candy and rough functionality is in place, the sales team signs up new clients. The company draws down investment in developers and puts more money into infrastructure. The product moves from developer land to system land which is monolithic and can handle heavier loads.;0
17501175;HackerNews;2018-07-10;<i>I'd wager that microservices, a lot of the time, are basically used as a management structure rather than for their benefits as pure tech</i><p>A lot of software is used to control or impose someone's will on others organizationally.;0
17501150;HackerNews;2018-07-10;How on earth did you end up with 100s of services?<p>I can't imagine you have applied Conway's law.<p>I think there's some serious confusion between FaaS and microservice architecture.;0
17501138;HackerNews;2018-07-10;"Microservice based architecture will continue to be used quite a bit as many weaker engineering leads have latched onto it as a way to solve interpersonal problems on their team and letting everyone have their own little fiefdoms.  It creates a terrible scenario and a very dysfunctional team, but the hype cycle and a few high scale orgs mentioning it means its part of the &quot;mainstream&quot; now and something that ill-informed leads will be pushing for the next decade, whether it makes sense for the specific context (it doesn't) or not.";0
17501108;HackerNews;2018-07-10;"I have had thinkings on how efficient Micro services are compared to a Monolith.<p>In a micro service architecture, the request bounces through different service layers, json serialization, network transfers.<p>In a monthlith normal application, especially if its on one machine, the request touches main memory and then CPU cache.<p>Latency Numbers Every Programmer Should Know, I especially think of the Send 1K bytes over 1 Gbps network vs cache latency of microservices/monolith.
<a href=""https://gist.github.com/jboner/2841832"" rel=""nofollow"">https://gist.github.com/jboner/2841832</a><p>Ie instead of micro services 10000 ns for a network transfer you could fetch within 10ns from CPU cache in a monolith, that is 1000 times more efficient.<p>Are we beyond the peak of inflated expectations on Micro services and towards the Plateau of productivity?
<a href=""https://en.wikipedia.org/wiki/Hype_cycle"" rel=""nofollow"">https://en.wikipedia.org/wiki/Hype_cycle</a>";0
17501088;HackerNews;2018-07-10;"&gt;  Just as bad, just in a different way.<p>Actually worse in many ways:<p>- Harder to test<p>- Harder to debug<p>- Harder to deploy<p>- Harder to monitor<p>- Harder to reason about, refactor, change/add functionality<p>- You've (basically) turned a lot of the operations your services need to perform into RPCs, probably killing performance on top of everything else, leading to<p>- More complex/demanding (or just MOAR) infrastructure requirements<p>- Higher dev, maintenance and infrastructure costs<p>- Slower delivery of value to the business and customers<p>- Potentially crippling opportunity costs<p>It surprises me how often people don't see this coming. Seriously: keep your systems as simple as you possibly can. Unless you're Netflix, dozens or hundreds of microservices probably isn't as simple as you possibly can.";0
17501083;HackerNews;2018-07-10;"&gt; Our initial microservice architecture worked for a time, solving the immediate performance issues in our pipeline by isolating the destinations from each other. However, we weren’t set up to scale. We lacked the proper tooling for testing and deploying the microservices when bulk updates were needed. As a result, our developer productivity quickly declined.<p>Well, I'm not surprised. Microservices are made possible by advances in automated testing, CI, and deployment. You should have those things anyway, even if you have a monolith -- but to go to microservices without them is a pretty bad decision.";0
17500967;HackerNews;2018-07-10;"&gt;I think that with the rise in popularity of functions as a service (lambda, gcf, azure), we are heading more and more towards nanoservices.<p>There are some pretty big asterisks next to running &quot;nanoservices&quot;. Mostly how expensive they actually are to run at large scale and the weird caveats that can happen due to them not always being up.<p>And I wouldn't advocate for &quot;always-up nanoservices&quot;.<p>The basic answer to both &quot;nanoservices&quot; and microservices is do what you think is right but don't go too far. There are good reasons to make a nanoservice and good reasons not to, same with microservices.";0
17500945;HackerNews;2018-07-10;"This coincides with my own experiences in the financial sector. Distributed computing is undoubtedly the way to scale, but the trick is making the distributed nature of the system completely invisible (or as much as possible) to the developers, the applications and the supporting staff.<p>I have seen this phenomena of thinking that a large system, broken down into tiny parts, is somehow easier to manage time and time again over 30 years of development. In every case, the one thing the central thinkers fail to realize is that complexity is like conservation of energy - it can be transformed, but it cannot be destroyed.<p>Also, when it comes to large teams I have seen one thing work when it comes to sharing a resource(s) critical to a larger system - shared pain.  If the central/reusable  code/service breaks everyone's stuff, then everyone forms a team to immediately address the problem before continuing on. The solution is almost never &quot;find a way to let the other teams continue while something important is on fire.&quot; It seems like a major motivation for a microservices architecture seeks to avoid the pain - which perhaps is not the best reason to use microservices.<p>I like the idea of unseen, but indispensable, complexity. For instance, the human brain is probably the most complex thing in the world, but the interface is fairly simple :)";0
17500931;HackerNews;2018-07-10;If you have 140 somewhat similar entities that all share common code, then they don't fullfill the very important microservice criterion of being independent. In your case, I would recommend to use a plugin based system. Do it the other way around, have 1 application that contains the common code (previously shared library code) and create 140 plugins. This way you can update the single application, load all plugins, execute the tests, check if everything is fine and deploy the application. Every plugin can live in its own repository and can be versioned separately, but a new version can only be deployed if it works with the latest version of the application.;0
17500902;HackerNews;2018-07-10;"&gt;It's worse than that; it's my observation that most microservice architectures just ignore consistency altogether (&quot;we don't need no stinking transactions!&quot;) and blindly follow the happy path.<p>If two microservices have to share databases, they shouldn't be microservices.<p>One microservice should have write access to one database and preferably, all read requests run through that microservice for exactly the reason you mentioned.<p>&gt;I've never quite understood why people think that taking software modules and separating them by a slow, unreliable network connection with tedious hand-wired REST processing should somehow make an architecture better.<p>If you're running microservices between regions and communicating with each other outside of the network it is living in, you're probably doing it wrong.<p>Microservices shouldn't have to incur the cost of going from SF to China and back. If one lives in SF, all should and you can co-locate the entire ecosystem (+1 for &quot;only huge companies with big requirements should do microservices&quot;)<p>&gt;ustomers couldn't care less, except that it now takes far longer to implement features that cross multiple services - which, if you've decomposed your services zealously enough, is pretty much all of them.<p>Again, that is an example of microservices gone wrong. You'll have the same amount of changes even in a monolith and I'd argue adding new features is safer in microservices (No worries of causing side effects, etc).<p>I will give you +1 on that anyway because I designed a &quot;microservice&quot; that ended up being 3 microservices because of dumb requirements. It probably could've been a monolith quite happily.";0
17500887;HackerNews;2018-07-10;It’s basically a feature flag. I don’t like feature flags but it is a thing.<p>But if you are testing an artifact, why isn’t the artifact testing part of your CI process? What you want to do is no more or less an anti pattern than swapping out mock services to test a microservice.<p>I’m assuming the use of a service discovery tool to determine what gets run. Either way, you could screw it up by it being misconfigured.;0
17500884;HackerNews;2018-07-10;"I'd wager that microservices, a lot of the time, are basically used as a management structure rather than for their benefits as pure tech, so less mature teams can silo themselves off and avoid communication (e.g. &quot;I can work just on my backend image processing bit without dealing with the React guys now&quot;, &quot;now the CTO won't be on my back so much,&quot; or whatever).<p>The irony being that anything approaching SOA (or microservices) requires exactly the same amount of communication. More likely they require more since it's almost certain that such a decision introduced chaos.";0
17500880;HackerNews;2018-07-10;"Look at the package dependency tree of an average linux program. They are absolutely examples of &quot;microservices&quot; talking to each other.";0
17500872;HackerNews;2018-07-10;"&gt;The first item on the list was to consolidate the now over 140 services into a single service.<p>software engineering walks in circles. NoSQL people seem to be growing up and learning about consistency and transactionality.  I'm waiting for somebody to discover threads and shared variables back as a revolutionary way to improve performance and greatly simplify the implementation of a system of &quot;actors&quot;.<p>Joking aside,  i think greatest reason for Segment's microservices failure was that they didn't use Kubernetes.";0
17500870;HackerNews;2018-07-10;Sometimes, I've seen a lack of regard for data consistency within a monolith.<p>That is not completely on the developer, either. Pre 4.0 Mongodb, for example, does not do transactions. On the other hand, I've seen some pretty flagrant disregard for it just because there are not atomicity guarantees.<p>Microservices makes reasoning on that harder.;0
17500869;HackerNews;2018-07-10;"Surely this is &quot;no true scotsman&quot;? They've tried microservices and it didn't work for them, you're saying that if only they'd &quot;fixed&quot; their micro-services architecture it would have worked?";0
17500861;HackerNews;2018-07-10;"A number of comments touched on this, that microservices are also an organizational strategy: you have a team that manages that particular service.<p>My company is probably typical in that we implicitly use microservices because we have consume dozens of microservices <i>provided by other companies</i>.<p>We only have a handful of services we maintain, each with dedicated engineers.<p>And what becomes a service? You want to answer two questions:<p>1. Does it have a concrete business justification?
2. Does it have clear functional requirements?<p>If it has a business justification, it will get people assigned to keeping it running. If it has clear functional requirements, it will make sense to the people working on it which service does what.<p>That's still pretty vague, so you want to look at who has done it well, and why it worked for them.<p>Companies like AWS have been extremely successful in using microservices because every last one service has a business rationale, it's either directly making money (EC2, S3, SQS) <i>or</i> it supports the needs of customers who are using a service that makes money (VPC, Cloudformation, all their internal auth, billing, provisioning, security and such).<p>The caveat there is that the big B2B service providers are not a great model for companies that have a lot of business logic.";0
17500857;HackerNews;2018-07-10;If you're going to go microservices, you want service A to use service B's public API (MQ or RPC or whatever), not to quietly depend on the schema B happened to choose. And sharing a database server instance turns overloads into cascading failures, unless the stack is <i>very</i> good at enforcing resource limits on noisy neighbors.;0
17500851;HackerNews;2018-07-10;"To be blunt, its news to a lot of people, but it also isn't wrong. Microservices really shouldn't share a database, and if they do then they aren't &quot;microservices&quot;.";0
17500817;HackerNews;2018-07-10;Running E2E blackbox test is equally simple for all kinds of architectures, especially today, when it's so easy to create a clean test environment with multiple containers even on developer's machine. It may be harder to automate this process for a distributed system, but, frankly speaking, I don't see a big difference between a docker-compose file or a launch script for monolith - I've been writing such tests for distributed systems casually for several years and from my personal experience it's much easier to process the test output and debug the microservices than monolithic applications.;0
17500814;HackerNews;2018-07-10;Reading this post-mortem was very useful, and I appreciate the segment engineering team sharing it.<p>It seems like the primary problem causes were flaky/unreliable tests, and difficulty making coordinated changes across many small repositories.<p>Having worked on similar projects before (and currently), with a small team driving microservices oriented projects, I would probably recommend:<p>1) single repository to allow easy coordinated changes.<p>2) a build system that only runs tests that are downstream of the change you made (Bazel is my favorite here, but others exist). This means all services use the HEAD version of libraries, and you find out if a library change broke one of the services you didn't think about. This also allows for faster performance.<p>3) Emphasis on making tests reliable. Mock out external services, or if you must reach out to dependencies use  conditional test execution, like golang's Skip or junit's Assume if you can't verify a working connection.<p>If you still can't build a reliable service with those choices, then it's time to think about changing the architecture.;0
17500794;HackerNews;2018-07-10;They used microservices to paper over poor testing hygiene and software architecture. Forking one service into a bunch of different flavors of the same service multiplied the maintenance complexity. Services/microservices that are tightly coupled to shared libraries with lots of surface area  are an anti-pattern. Instead, I would make the shared libraries into a service, and make a standard interface for the destinations as plugins.;0
17500755;HackerNews;2018-07-10;OP can correct me if I'm wrong, but I believe the comment around Segements engineering team was most likely referencing the original decision to create hundreds of microservices in the first place.;0
17500735;HackerNews;2018-07-10;The fix, then, is to put that commonality into a new microservice the other microservices call.<p>The more I read about the problems people have with microservices, the more I'm convinced they've never read about flow-based programming.;0
17500668;HackerNews;2018-07-10;"&quot;packages&quot; and &quot;ports&quot; are both collections of &quot;microservices&quot; in your analogy, so you can't really assert the whole thing is a monolith.";0
17500661;HackerNews;2018-07-10;"The important part here, that the parent addressed, is the statement &quot;small team&quot;, as an organizational structure distinct from, say, a &quot;large team&quot; which has the opportunity to experience Conway's law.<p>It sounds like Segment very much prematurely optimized. Microservices is probably not a great approach below 20 developers, and becomes necessary past 100, at least in my experience.";0
17500657;HackerNews;2018-07-10;This is a classic case of not understanding micro services and trying to fit a problem around a tool.<p>At work, we have close to ~50 services(no one calls them microservices), but they do not suffer from this brittleness. We segregate our services based on languages. So, all C services go under coco/ , all Java services go under jumanji/ , all go services go under goat/ , all JS services go under js/. This means, everytime you touch something under a repo, it affects everyone. You are forced to use existing code or improve it, or you risk breaking code for everyone else. What does this solve? This solves the fundamental problem a lot of leetcode/hackerrank monkeys miss, programming is a Social activity it is not a go into a cave and come out with a perfect solution in a month activity. More interaction among developers means Engineers are forced to account for trade offs. Software Engineering in its entirety is all about trade offs, unlike theoretical Comp Science.<p>Anyway, this helps because as Engineers we must respect and account for other Engineers decisions. This methods helps tremendously to do this. No one complains, everyone who wants 1000 more microservices usually turns out to be a code monkey entangled in new fad, or who doesn't want to work with other Engineers.<p>You want to use rust? There is a repo named fe2O3/, go on. Accountability and responsibility is on your shoulders now.<p>If you think about it, an Engineer is tied to his tools, why not segregate repos at language level instead of some arbitrary boundary no one knows about in a dynamic ecosystem?;0
17500651;HackerNews;2018-07-10;Working at a large tech company, we tend to approach microservice division as information authority division.<p>If the data doesn't need to exist in the same database, put it in a new service with a separate database (or at least completely independent tables).<p>Ideally there is no shared code between services (and there shouldn't need to be, because each service owns completely disparate data), so the only coupling is the API definitions.<p>Each service is free to use whatever internal architecture they see fit as long as they honor the API definitions they provide to their dependent services.<p>In the case outlined in the article, the fact that each microservice had similar enough concerns to all use some common libraries and the same database makes me doubt that this should ever have been built as microservices to begin with.;0
17500639;HackerNews;2018-07-10;iamleppert does address that. As they said, microservices are supposed to mirror the structure of your team(s).<p>A single team should never be maintaining 140 different microservices. That's not a failing of the microservices architecture, that is a failing of massively abusing the microservices architecture. Arguably, a small team should never be managing more than 10 services (totally arbitrary number, but it seems like the upper limit to what a human can focus on).;0
17500620;HackerNews;2018-07-10;"That doesn't really say anything though: &quot;Things were bad, and it felt out of control, because of microservices. They didn't work, and actually made things worse and more complex. The things that were supposed to be good were bad. We didn't do good work, we did bad work, and slowly.&quot;<p>How do you address something like that coherently? I sort of agree with the OP, it kinda sounds like they just didn't know what they were doing. Or maybe that quote wasn't really the meat of their complaint?";0
17500613;HackerNews;2018-07-10;I'm hardly a microservice apologist, but sharing code or data  across services is a major smell. It means these things are related, and should likely be bundled together. Don't just break a service apart because it's de rigueur.;0
17500605;HackerNews;2018-07-10;"&gt; Usually microservices have their own databases<p>That's news to me, and seems insane.<p>Unless you mean &quot;their own database tables&quot;, not &quot;database servers&quot;. But that's just the same as having multiple directories and files in a Unix filesystem.";0
17500600;HackerNews;2018-07-10;"&quot;microservices is the architecture du jour&quot;<p>? There is nothing new about microservices. They've been a hot topic for far longer than Segment has been an idea.";0
17500592;HackerNews;2018-07-10;When I was designing a microservices architecture for a former employee, the justification for it was security. We needed separation of the components in the system because we were going to be handling money. It used ZeroMQ for communication, because I wanted something more lightweight and falut-resistant for message passing (in such a sensitive application, we didn't want to trust the network). Although it was of course fun to design, microservices weren't my first choice. It just made sense to us to use it in that particular scenario.;0
17500588;HackerNews;2018-07-10;"Nothing you said addresses the issues they mentioned:<p>&gt; In early 2017 we reached a tipping point with a core piece of Segment’s product. It seemed as if we were falling from the microservices tree, hitting every branch on the way down. Instead of enabling us to move faster, the small team found themselves mired in exploding complexity. Essential benefits of this architecture became burdens. As our velocity plummeted, our defect rate exploded.";0
17500578;HackerNews;2018-07-10;"But there is a big difference. These small targeted programs are invoked in user land, usually by the user. Microservices get invoked directly by the user when debugging is going on. Otherwise they are expected to automagically talk to each other and depending on the abstraction even discovery each other automatically.<p>Also I can pipe these tools together from the same terminal session, like<p><pre><code>  tail -f foo | grep something | awk ...
</code></pre>
You don't have that in general with Microservices. Unix tools <i>are</i> Lego, Microservices aren't. They are Domino at best.<p>Probably one could come up with an abstraction to do Lego with Microservices but we're not there yet.";0
17500577;HackerNews;2018-07-10;Before you go down the path of splitting your app up into microservices you should grok erlang/BEAM/OTP. Lots of thinking went into its creation that leads to highly reliable real time systems and at the very least some of the ideas can be lifted in informing how to best design things. (But really, you should probably just use it instead.);0
17500549;HackerNews;2018-07-10;Some people call this... microservices!  Common advice is that a microservice should align with a bounded context in domain driven design, which can involve a LOT of code.<p>Many large companies have millions of LOC behind their microservices.  Your average startup probably doesn't.;0
17500536;HackerNews;2018-07-10;"With web apps the main concern is data consistency between relations. On the OS level you have these same concerns with memory and disk, and there's database-like systems in the kernel and drivers to handle it. Essentially all these utilities are running within the same &quot;database&quot; which is disk and memory management handled by the kernel. Usually microservices have their own databases, which is where consistency hell begins";0
17500525;HackerNews;2018-07-10;In this case it sounds like they started with a microservice architecture, but CI/CD automation necessary for robust testing and auto-scaling was not in place. The problem of queues getting backed up might have been addressed by adding a circuit breaker, but instead they chose to introduce shared libraries (again, without necessary testing and deployment), which resulted in very tight coupling of the so-called microservices.;0
17500519;HackerNews;2018-07-10;"Yes, but &quot;monolithic&quot; web applications can be built in the same way. It might not be a 100% accurate usage of the term, but microservice/SOA advocates love to call modular applications monliths anyways, to the point that it's something that most people seem to do.";0
17500517;HackerNews;2018-07-10;I don’t see any reason you shouldn’t always design a system as domain specific micro services.<p>Now those micro services shouldn’t always be out of process modules that communicate over HTTP/queues, etc. A microservice can just as easily be separately compiled modules within a monolithic solution with different namespaces, public versus private classes, and communicate with each other in process.<p>Then if you see that you need to share a “service” across teams/projects, or a module needs to be separately, deployed, scaled, it’s quite easy to separate out the service into versioned packages or a separate out of process service.;0
17500484;HackerNews;2018-07-10;I think this is probably true for larger or more distributed corporate environments, but I think a modular monolith is going to be a more productive and flexible architecture for most teams, and should be the default option for most startups (many of whom are doing microservices from day 1 it seems).<p>1. Is auto-enforced modularity, separation of concerns, etc actually better than enforcing these things through development practices like code review? Why are you paying people a 6 figure salary if they can't write modular software?<p>2. Is the flexibility you gain from this loose coupling worth the additional costs and overhead you incur? And is it really more flexible than a modular system in the first place? And how does their flexibility differ? With an API boundary breaking changes are often not an option. In a modular codebase they can easily be made in a single commit as requirements change.<p>3. Is bring-your-own-language actually a good idea for most businesses? Is there a net benefit for most people beyond attracting and retaining talent? What about the ability to move developers across teams and between different business functions? Having many different tech stacks is going to increase the cost of doing this.<p>I do see the appeal of some of these things, but IMO the pros outweigh the cons for a smaller number of businesses than you've mentioned. And the above is only a small sample of that. Most things are just more difficult with a distributed system. It's going to depend on the problem space of course, but most backend web software could easily be written in a single language in a single codebase, and beyond that modularization via libraries can solve a lot of the same problems as microservices. I'm <i>very</i> skeptical of the idea that microservices are somehow going to improve reliability or development speed unless you have a large team.;0
17500481;HackerNews;2018-07-10;Teams have a lot to do with microservice use. If you have a small team a monolith can work well. If you have a large team, microservices have some significant advantages. And in some cases, you might have a microservice in Swift when you need high performance, but other less intensive services might be in Ruby or some other language, etc.<p>Right took for the job should be the goal as opposed to chasing fashion. Microservices are definitely overused, but they do have many legitimate use cases. Your CRUD web application probably doesn’t need microservices, but complex build systems might.<p>Imagine the complexity of Amazon.com or Netflix were a monolith. But something like Basecamp is probably better as a monolith.<p>There is also the issue of scale. An ML processor might need more (and different) hardware than a user with system.<p>Right tool (and architecture) for the job.;0
17500462;HackerNews;2018-07-10;Seems like an application of sturgeon's law, there is a tendency to do what is new because it is new. A well designed microservice architecture can be a tremendous boon for a product. A well designed monolith can be a tremendous boon for a product. About 90% of both are poorly done. It's interesting how often people favor ANY motion vs motion in the correct direction.;0
17500454;HackerNews;2018-07-10;It's quite possible to do a monolithic app well and correctly.  It's also quite possible to do a big pile of microservices as a cohesive app well and correctly.<p>In my professional experience, where I've run into numerous examples of good and bad examples of both, microservices tend to win because it's a lot easier to unwind the badly implemented microservices compared to the badly implemented monolithic service.<p>Of course, this is just another small set of anecdata.;0
17500412;HackerNews;2018-07-10;"People might read this article as an argument against microservices. It's not, in my opinion. It's an argument against impractical design. Their system should never have been 50 separate repos in the first place; in terms of design, it's all one app that would benefit hugely from being a single coordinated system.<p>Microservices work when they are separate, independent, single-concern systems that coordinate using APIs. People often go overboard in splitting apps up into small pieces, even when those pieces logically belong to a single system. Start with figuring out the subsystems, then considering whether they are worth splitting in the first place.<p>It's worth pointing out that microservices don't mean separate repos or even codebase separation. What matters the most is encapsulation. Monoliths grow horrible because they end up being balls of spaghetti, and forcing modularization at the service level is a way to avoid such messes by reducing the individual parts manageable sizes, allowing a part to be replaced without being concerned about its tendrils having grown through the whole system.<p>For me, the biggest value of microservices is composition, of thinking abou modules as off-the-shelf components that you use as parts to build something bigger. Using a complete enough set of microservices, I can build a frontend or client that has zero app-specific backend code. For example, if I have a generic data layer (think Firebase), a user database layer with OAuth/OIDC, and a way to store images, then I can build Instagram from scratch with no backend development at all. That's very powerful.<p>But once I need some specialized, app-specific stuff (&quot;business rules&quot;), such as rating of photos, commenting, moderation, etc., then those probably wouldn't be microservices! The concerns are unified there for the most part, and disentangling them would just lead to annoying fragmentation. A single use-case specific service (&quot;monolith&quot;) that would exist at the center of it all.<p>On the other hand, composition is mostly useful if your pieces are going to be reusable. If I intend to build more than one Instagram, or maybe a Facebook (which also needs data storage, and logins, and photos, etc.), then the individual pieces would be reusable and could just be shared between the apps. But if I'm just building Instagram for 5 years and I'm not building a series of apps for different use cases, reusability has zero importance, and I might as well just move everything into a single monorepo and forget about making anything general-purpose. (Each piece should be general-purpose <i>enough</i>, but they usually don't need to be so generic that you could open-source it for everyone.)<p>I never liked the word &quot;microservice&quot;, and I think we'd be better off if we called them, say, modules or subsystems.";0
17500411;HackerNews;2018-07-10;Use Erlang/Elixir. You get the benefits of the monolith <i>and</i> the benefits of the microservices;0
17500398;HackerNews;2018-07-10;Microservices are an organizational and design choice that is intended to mirror the structure of the teams or engineers, the actual human beings doing this stuff.<p>It seems like Segment didn’t really understand this at all, and instead decided to have seemingly arbitrary and rediculous service boundaries that had no relationship to the real world.<p>See also people who create poor abstractions in their code and other sources of technical debt.<p>This isn’t an article about how microservices architecture is somehow bad, it’s an article about how bad Segment’s engineering team is. You can make the same argument for anything. At the end of the day some architecture or process or programming language or tech can’t replace real thinking about your problem and correct application.;0
17500393;HackerNews;2018-07-10;"Microservices aren't some magic bullet for scaling. If anything, they conform to Conway's Law [1]. I'd agree, though, if a single engineer is singularly responsible for 2+ microservices that are only supporting a single product...you're doing it wrong.<p>[1] <a href=""https://en.wikipedia.org/wiki/Conway%27s_law"" rel=""nofollow"">https://en.wikipedia.org/wiki/Conway%27s_law</a>";0
17500385;HackerNews;2018-07-10;Right, but the thing that makes Linux actually useful isn't really the kernel is it? I would say what makes it useful is all the various small, targeted programs (some might call them microservices) it lets you interact with to solve real world problems.<p>If Linux tried to be an entire computing system all in one code base, (sed, vim, grep, top, etc., etc.) what do you think that would look like code base/maintainability wise? Sounds like a nightmare to me.;0
17500381;HackerNews;2018-07-10;Not really. For example, it's easier to mock a microservice, than a module, for testing purposes. Let's say you have component A and component B, A depends on B (dependency implemented via runtime sync or async call), B is computationally intensive or has certain requirements on resources that make it harder or impossible to test on developer's machine. You may want to test only A: with monolithic architecture you'll have to produce another build of the application, that contains mock of B (or you need something like OSGi for runtime module discovery). When both components are implemented as microservices, you can start a container with mock of B instead of real B.;0
17500363;HackerNews;2018-07-10;"&gt; along with not following the basic rule of services &quot;put together first, split later&quot;.<p>Agreed. I treat services like an amoeba. Let your monolith grow until you see the obvious split points. The first one I typically see is authentication, but YMMV.<p>Notice I also do not say 'microservices'. I don't care about micro as much as functional grouping.";0
17500360;HackerNews;2018-07-10;It's refreshing to read an article which challenges common wisdom.<p>I've endured a lot of suffering at the hands of the microservices fan club. It's good to see reason finally prevail over rhetoric.<p>It would have been nice if people had written articles like this 2 years ago but unfortunately, people with such good reasoning abilities would probably not have been able to find work back then.<p>Software development rhetoric is like religion. If you're not on board you will be burned at the stake.<p>So many times during technical discussions, I had to keep my mouth shut in the name of self-preservation.;0
17500352;HackerNews;2018-07-10;I liked the rundown and agree there are tactical benefits for building monolithic API's, but one of the core tenants of microservice architecture coming out of the domain driven design space is that monolithic systems hide business logic. I'm pretty sure by moving everything back to a monolith, you've abandon DDD entirely. From a strategic perspective, that's bad for your business.<p>If you get your DevOps ducks in order, a lot of the issues you have with standing up and maintaining individual microservices should be manageable. I certainly understand the pain of dependency management, but that's also a part of good architecture.<p>I'm willing to hear more of these stories though. We can always learn about edge cases or even new paradigms that come out of current thinking.;0
17500347;HackerNews;2018-07-10;Microservices aren't a magic bullet and won't save you from a poorly designed application. You already need to have very good separation of concerns within your application to make it out of microservices, and at that point an API that communicates over HTTP isn't much different from one that communicates over the application stack.;0
17500335;HackerNews;2018-07-10;That's exactly it. But when you start doing 'microservices' you get the architecture astronauts who go and see into how many silly little services a monolith can be broken up. The end results are as predictable as the original monolith, both end up as an unmaintainable mess in a couple of years.<p>I predict the same will happen to the 'superstar', it just isn't old enough yet (and at least it was built with some badly needed domain knowledge).;0
17500323;HackerNews;2018-07-10;I probably don't understand their architecture well enough since they don't try and explain it in detail. The blame is also put on microservices whereas it's the architecture that can be fixed to improve performance and the cost of deployment.<p>A shared library works up until a certain point. If every service uses a shared library then you are already getting to a world of a monorepo. A monorepo for different services should work fine if the overall architecture is feasible.;0
17500320;HackerNews;2018-07-10;"+1<p>I felt this article is more about how to use microservices right way vs butchering the idea. It is not right to characterize this as microservices vs monolith service.
Initial version of their attempt went too far by spinning up a service for each destination. This is taking microservices to extreme which caused organizational and maintenance issue once number of destinations increased. I am surprised they did not foresee this.<p>The final solution is also microservice architecture with a better separation of concerns/functionalities. One service for managing in bound queue of events and other service for interacting with all destinations.";0
17500276;HackerNews;2018-07-10;As with J2EE EJBs, microservices conflates two things:<p>- a strong API between components<p>- network calls<p>The former is a very good idea that should be implemented widely in most code bases, especially as they mature, using techniques like modules and interfaces.<p>The latter is incredibly powerful in some cases but comes at a huge cost in system complexity, performance and comprehensibility.  It should be used sparingly.;0
17500233;HackerNews;2018-07-10;"People look at microservices as a solution to the Big Ball of Mud, and then confuse &quot;a solution&quot; with &quot;the solution&quot;.<p>You really do have to modularize.  In some languages, you can even use separate compilation units for separate modules to enforce the separation.<p>You can do all of that but get simultaneous deployment, which cuts out whole classes of integration nightmares.";0
17500230;HackerNews;2018-07-10;"It's worse than that; it's my observation that most microservice architectures just ignore consistency altogether (&quot;we don't need no stinking transactions!&quot;) and blindly follow the happy path.<p>I've never quite understood why people think that taking software modules and separating them by a slow, unreliable network connection with tedious hand-wired REST processing should somehow make an architecture better. I think it's one of those things that gives the illusion of productivity - &quot;I did all this work, and now I have left-pad-as-a-service running! Look at the little green status light on the cool dashboard we spent the last couple months building!&quot;<p>Programmers get excited about little happily running services, these are &quot;real&quot; to them. Customers couldn't care less, except that it now takes far longer to implement features that cross multiple services - which, if you've decomposed your services zealously enough, is pretty much all of them.";0
17500209;HackerNews;2018-07-10;"100's of problem children sounds like a step too far.<p>A few services &gt; monolith<p>monolith &gt; 100's of services.<p>The big trick with any technology is to apply it properly rather than dogmatically and if you are breaking up your monolith into a 100's(!) of microservices you are clearly not in control of your domain. That's a spaghetti of processes and connections between them rather than a spaghetti of code. Just as bad, just in a different way.";0
17500149;HackerNews;2018-07-10;Too many smart folks that I've worked with, for some reason, just stop thinking critically when it comes to certain ideas.<p>I was a product manager on a team of really rockstar developers. They all earn at least $200k a year.<p>Instead of demanding more ambitious projects, you could keep about 99% of them happy by just letting them use the new framework of the week to build their next web app. Their excitement when they were green-lighted to use react was mind-boggling. Building another dumb website with the new framework, yay.<p>Mindlessly applying microservice architecture is the same issue at heart.;0
17500147;HackerNews;2018-07-10;I feel that when it comes to making any decision about separating things out, you really do have to consider the size of the team you've got before you commit to it. If you've got three devs and they all work across all of the microservices, then what on earth do they add except a hell of a lot of overhead?<p>If you had ten teams of seven and they managed two services each... it's easier to see how that architecture could actually help.<p>Same as if you have a two person team building a web-app and they go for client-server architecture rather than a basic full stack web framework. If you're both working the frontend and backend at the same time, save yourself the extra ops effort.;0
17500140;HackerNews;2018-07-10;Not that I disagree with microservices easily going awry, the problem here seems to be traced to shared library code.  Each microservice should be as standalone as possible.  Your contract with that service is the service contract.  Not some shared library.<p>As soon as you have shared library, you now have coordinated deployments.  And that is just not fun and will cause problems.<p>The trick here is that this does mean you will duplicate things in different spots.  But that duplication is there for a reason.  It is literally done in two places.  When you update the service, you have to do it in a backwards compatible way.  And then you can follow with updates to the callers.  This makes it obvious you will have a split fleet at some point, but it also means you can easily control it.;0
17500131;HackerNews;2018-07-10;"Microservices typically have two goals, performance and modularity. However, porting a typical webapp to a fast, modular compiled language will typically achieve at least one (often two) orders of magnitude performance improvement over a typical interpreted language. We are seeing this to be true more often than not, and a large performance gain off the bat like that may even obviate a lot of the desire to move to microservices.<p>Furthermore, if one uses modules (as one should), one can arbitrarily and somewhat trivially run those modules either in-process (compiled in) or out-of-process (via REST, gRPC, Cap’n Proto, or another RPC system), e.g., in a separate service/microservice/whatever you want to call it. This gives you a best of both worlds approach where code can be arbitrarily run in a monolith or a separate service as-needed. This changes the thought dynamics from a rigid &quot;monolith vs. microservice&quot; decision to a more fluid process where things can be rather easily changed on a whim. When modularity is the goal, then services become something of a secondary concern.<p>Microservices are used as something of a sledgehammer to force modularity and performance in languages that lack proper modularity and/or are innately slow or otherwise inefficient, while suffering orchestration costs and the performance penalties of copying data across multiple processes and networks as well as making it harder to derive a single-source of truth in some cases.<p>Probably a good approach for a typical webappp looking to improve performance would be to first port core logic to a modern, fast, compiled language with modules, then evaluate the performance from there, and then determine if any modules should be split out into separate processes or services.<p>Like NoSQL, microservices can be (but not always are) a case of the cure being worse than the disease; however, they can also be useful in certain situations or architectures. Like anything in engineering, there are tradeoffs and it depends on your situation.";0
17500111;HackerNews;2018-07-10;Personally, I don’t like the terms ‘microservices’ or ‘nanoservices.’ What’s the value add in describing the relative size of the service? The _domain_ should drive what becomes its own service. Every service should handle the business logic within a particular domain. It’s definitely a goldilocks problem, though, in that there’s a too-small and too-large, and we’re looking for the just-right fit!;0
17500096;HackerNews;2018-07-10;"Am I understanding correctly that they had 3 engineers and &gt;140 microservices?  Microservices definitely have their own costs and tradeoffs, but 140 services and 3 engineers sounds like just a terrible engineering choice.";0
17500086;HackerNews;2018-07-10;"I agree, and their conclusion even features this salient bit:<p>&gt;However, we weren’t set up to scale. We lacked the proper tooling for testing and deploying the microservices when bulk updates were needed. As a result, our developer productivity quickly declined.<p>My impression after reading this post was that microservices were symptoms of problems in how their organization wasn't set up to implement them effectively, rather than the actual cause of those problems.";0
17499999;HackerNews;2018-07-10;So they split everything apart because their tests were failing and they didn't want to spend time fixing them, and they they merged it back together by spending time fixing and improving their tests?<p>It seems like the problem here was bad testing and micro repos, not microservices.;0
17499928;HackerNews;2018-07-10;What kind of issues you faced while working with microserviceS?;0
17499842;HackerNews;2018-07-10;Agreed.<p>Reading about their setup and comparing with some truly large scale services I work with, I'm left with the idea that Segment's service is roughly the size of one microservice on our end.<p>Perhaps the takeaway is don't go overboard with fragmenting services when they conceptually fulfill the same business role. And regardless of the architecture of the system, there are hard state problems to deal with in association with service availability.;0
17499807;HackerNews;2018-07-10;I've always said if the Linux kernel can be a giant monolith, in C no less, than there's maybe 100 web applications in the world that need to be split into multiple services.<p>I've worked with microservices a lot. It's a never-ending nightmare. You push data consistency concerns out of the database and between service boundaries.<p>Fanning out one big service in parallel with a matching scalable DB is by far the most sane way to build things.;0
